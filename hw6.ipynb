{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled23.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMEiV4oOmgqXneksl831dqH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitryKutsev/hse_compling_homework/blob/master/hw6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEfNUCBFBGjx",
        "colab_type": "code",
        "outputId": "0e72fd81-17d7-4513-b097-12cc445e7ae3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.1/lenta-ru-news.csv.bz2"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-19 19:25:06--  https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.1/lenta-ru-news.csv.bz2\n",
            "Resolving github.com (github.com)... 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/87156914/619f9f00-1e96-11ea-946e-dac89df8aced?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200219%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200219T192506Z&X-Amz-Expires=300&X-Amz-Signature=82caffb8f44756eb0fa838c94c84b85e997bd6486b6fbdf55053e0332e5801aa&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.bz2&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-02-19 19:25:06--  https://github-production-release-asset-2e65be.s3.amazonaws.com/87156914/619f9f00-1e96-11ea-946e-dac89df8aced?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200219%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200219T192506Z&X-Amz-Expires=300&X-Amz-Signature=82caffb8f44756eb0fa838c94c84b85e997bd6486b6fbdf55053e0332e5801aa&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.bz2&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.177.51\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.177.51|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 346031300 (330M) [application/octet-stream]\n",
            "Saving to: ‘lenta-ru-news.csv.bz2’\n",
            "\n",
            "lenta-ru-news.csv.b 100%[===================>] 330.00M  50.2MB/s    in 6.6s    \n",
            "\n",
            "2020-02-19 19:25:13 (49.9 MB/s) - ‘lenta-ru-news.csv.bz2’ saved [346031300/346031300]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjFg8bpAssV7",
        "colab_type": "code",
        "outputId": "18f313bb-d8fe-47c2-df5d-4206e6e4f51c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!wget https://rusvectores.org/static/models/rusvectores2/news_mystem_skipgram_1000_20_2015.bin.gz\n",
        "!ls"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-19 19:25:14--  https://rusvectores.org/static/models/rusvectores2/news_mystem_skipgram_1000_20_2015.bin.gz\n",
            "Resolving rusvectores.org (rusvectores.org)... 116.203.104.23\n",
            "Connecting to rusvectores.org (rusvectores.org)|116.203.104.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 549952184 (524M) [application/x-gzip]\n",
            "Saving to: ‘news_mystem_skipgram_1000_20_2015.bin.gz.1’\n",
            "\n",
            "news_mystem_skipgra 100%[===================>] 524.47M  27.4MB/s    in 20s     \n",
            "\n",
            "2020-02-19 19:25:34 (26.8 MB/s) - ‘news_mystem_skipgram_1000_20_2015.bin.gz.1’ saved [549952184/549952184]\n",
            "\n",
            "data_paraphraser_norm.csv\n",
            "lenta-ru-news.csv\n",
            "lenta-ru-news.csv.bz2\n",
            "news_mystem_skipgram_1000_20_2015.bin.gz\n",
            "news_mystem_skipgram_1000_20_2015.bin.gz.1\n",
            "paraphraser_gold.zip\n",
            "paraphrases_gold.xml\n",
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WPvPEgeb98S",
        "colab_type": "code",
        "outputId": "ada33893-8c9b-4c09-d361-5b91a2e10fa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "#!rm model.bin model.txt \n",
        "# !wget http://vectors.nlpl.eu/repository/20/186.zip\n",
        "!wget http://vectors.nlpl.eu/repository/20/185.zip\n",
        "!unzip 185.zip\n",
        "!ls"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-19 19:40:31--  http://vectors.nlpl.eu/repository/20/185.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 639268566 (610M) [application/zip]\n",
            "Saving to: ‘185.zip’\n",
            "\n",
            "185.zip             100%[===================>] 609.65M  22.7MB/s    in 31s     \n",
            "\n",
            "2020-02-19 19:41:03 (19.4 MB/s) - ‘185.zip’ saved [639268566/639268566]\n",
            "\n",
            "Archive:  185.zip\n",
            "  inflating: meta.json               \n",
            "  inflating: model.bin               \n",
            "  inflating: model.txt               \n",
            "  inflating: README                  \n",
            "185.zip\t\t\t   news_mystem_skipgram_1000_20_2015.bin.gz\n",
            "data_paraphraser_norm.csv  news_mystem_skipgram_1000_20_2015.bin.gz.1\n",
            "lenta-ru-news.csv\t   paraphraser_gold.zip\n",
            "lenta-ru-news.csv.bz2\t   paraphrases_gold.xml\n",
            "meta.json\t\t   README\n",
            "model.bin\t\t   sample_data\n",
            "model.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrGWKppNE97u",
        "colab_type": "code",
        "outputId": "f14aedf8-e5e2-4ab0-8282-2f742c2e94d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "!wget https://github.com/DmitryKutsev/hse_compling_homework/blob/master/paraphraser_gold.zip?raw=true\n",
        "!wget https://github.com/DmitryKutsev/hse_compling_homework/blob/master/data_paraphraser_norm.csv?raw=true\n",
        "!mv paraphraser_gold.zip?raw=true paraphraser_gold.zip\n",
        "!mv data_paraphraser_norm.csv?raw=true data_paraphraser_norm.csv\n",
        "!unzip paraphraser_gold.zip\n",
        "!ls"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-19 19:25:38--  https://github.com/DmitryKutsev/hse_compling_homework/blob/master/paraphraser_gold.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/DmitryKutsev/hse_compling_homework/raw/master/paraphraser_gold.zip [following]\n",
            "--2020-02-19 19:25:38--  https://github.com/DmitryKutsev/hse_compling_homework/raw/master/paraphraser_gold.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/DmitryKutsev/hse_compling_homework/master/paraphraser_gold.zip [following]\n",
            "--2020-02-19 19:25:38--  https://raw.githubusercontent.com/DmitryKutsev/hse_compling_homework/master/paraphraser_gold.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 118720 (116K) [application/zip]\n",
            "Saving to: ‘paraphraser_gold.zip?raw=true’\n",
            "\n",
            "\r          paraphras   0%[                    ]       0  --.-KB/s               \rparaphraser_gold.zi 100%[===================>] 115.94K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-02-19 19:25:38 (2.95 MB/s) - ‘paraphraser_gold.zip?raw=true’ saved [118720/118720]\n",
            "\n",
            "--2020-02-19 19:25:40--  https://github.com/DmitryKutsev/hse_compling_homework/blob/master/data_paraphraser_norm.csv?raw=true\n",
            "Resolving github.com (github.com)... 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/DmitryKutsev/hse_compling_homework/raw/master/data_paraphraser_norm.csv [following]\n",
            "--2020-02-19 19:25:40--  https://github.com/DmitryKutsev/hse_compling_homework/raw/master/data_paraphraser_norm.csv\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/DmitryKutsev/hse_compling_homework/master/data_paraphraser_norm.csv [following]\n",
            "--2020-02-19 19:25:40--  https://raw.githubusercontent.com/DmitryKutsev/hse_compling_homework/master/data_paraphraser_norm.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3575140 (3.4M) [text/plain]\n",
            "Saving to: ‘data_paraphraser_norm.csv?raw=true’\n",
            "\n",
            "data_paraphraser_no 100%[===================>]   3.41M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-02-19 19:25:41 (33.6 MB/s) - ‘data_paraphraser_norm.csv?raw=true’ saved [3575140/3575140]\n",
            "\n",
            "Archive:  paraphraser_gold.zip\n",
            "replace paraphrases_gold.xml? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "data_paraphraser_norm.csv\n",
            "lenta-ru-news.csv\n",
            "lenta-ru-news.csv.bz2\n",
            "news_mystem_skipgram_1000_20_2015.bin.gz\n",
            "news_mystem_skipgram_1000_20_2015.bin.gz.1\n",
            "paraphraser_gold.zip\n",
            "paraphrases_gold.xml\n",
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMeuIrD5if_O",
        "colab_type": "code",
        "outputId": "525cd665-da78-4f53-abd0-bed80bdc0567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!bzip2 -d lenta-ru-news.csv.bz2"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWZRDAhDimQk",
        "colab_type": "code",
        "outputId": "d1272cdb-73b6-4ff1-d714-ce3ee62af5e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "!ls\n",
        "!pip install pymorphy2[fast]\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data_paraphraser_norm.csv\t\t  paraphraser_gold.zip\n",
            "lenta-ru-news.csv\t\t\t  paraphrases_gold.xml\n",
            "news_mystem_skipgram_1000_20_2015.bin.gz  sample_data\n",
            "Collecting pymorphy2[fast]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.7MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2[fast]) (0.6.2)\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 28kB/s \n",
            "\u001b[?25hCollecting DAWG>=0.7.3; extra == \"fast\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/ef/91b619a399685f7a0a95a03628006ba814d96293bbbbed234ee66fbdefd9/DAWG-0.8.0.tar.gz (371kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 54.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: DAWG\n",
            "  Building wheel for DAWG (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for DAWG: filename=DAWG-0.8.0-cp36-cp36m-linux_x86_64.whl size=853073 sha256=ff770f786f1f35a24d46fc8487d947c284750c44ded47819d09d077653b26807\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/1f/f0/a5b1f9d02e193c997d252c33d215f24dfd7a448bc0166b2a12\n",
            "Successfully built DAWG\n",
            "Installing collected packages: dawg-python, pymorphy2-dicts, DAWG, pymorphy2\n",
            "Successfully installed DAWG-0.8.0 dawg-python-0.7.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zX7M6MAkoQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from collections import Counter,defaultdict\n",
        "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
        "from string import punctuation\n",
        "import os\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from lxml import html\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import gensim\n",
        "import numpy as np\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "morph = MorphAnalyzer()\n",
        "punct = punctuation+'«»—…“”*№–,'\n",
        "stops = set(stopwords.words('russian'))\n",
        "\n",
        "def normalize(text):\n",
        "    \n",
        "    words = [word.strip(punct) for word in text.lower().split()]\n",
        "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "def tokenize(text):\n",
        "    \n",
        "    words = [word.strip(punct) for word in text.lower().split()]\n",
        "\n",
        "    return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHRVBKC9niKF",
        "colab_type": "code",
        "outputId": "86674fce-4ee6-46ad-e333-bfbad4dda752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_handler = open('lenta-ru-news.csv', 'r')\n",
        "data = data_handler.read()\n",
        "print(len(data))\n",
        "data = data[:6000000]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1172327461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qQ8d2jirWll",
        "colab_type": "code",
        "outputId": "7706da16-e678-4114-ed11-817095a2c715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print(data[:1000])\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "url,title,text,topic,tags,date\n",
            "https://lenta.ru/news/1914/09/16/hungarnn/,1914. Русские войска вступили в пределы Венгрии  ,\"Бои у Сопоцкина и Друскеник закончились отступлением германцев. Неприятель, приблизившись с севера к Осовцу начал артиллерийскую борьбу с крепостью. В артиллерийском бою принимают участие тяжелые калибры. С раннего утра 14 сентября огонь достиг значительного напряжения. Попытка германской пехоты пробиться ближе к крепости отражена. В Галиции мы заняли Дембицу. Большая колонна, отступавшая по шоссе от Перемышля к Саноку, обстреливалась с высот нашей батареей и бежала, бросив парки, обоз и автомобили. Вылазки гарнизона Перемышля остаются безуспешными. При продолжающемся отступлении австрийцев обнаруживается полное перемешивание их частей, захватываются новые партии пленных, орудия и прочая материальная часть. На перевале Ужок мы разбили неприятельский отряд, взяли его артиллерию и много пленных и, продолжая преследовать, вступили в пределы Венгрии. \n",
            "«Русский инвали\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWmUFssTRnoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_norm = [normalize(text) for text in data.split('.')]\n",
        "#data_norm = \".\".join(data_norm)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5J3zqnP8Rnmh",
        "colab_type": "code",
        "outputId": "4174bd46-f3d3-4eb9-f901-0e4090544ff3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "data_norm = [text for text in data_norm if text]\n",
        "print(len(data_norm))\n",
        "data_norm[:6]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['url,title,text,topic,tags,date https://lenta',\n",
              " 'ru/news/1914/09/16/hungarnn/,1914',\n",
              " 'русский войско вступить предел венгрия бой сопоцкина друскеник закончиться отступление германец',\n",
              " 'неприятель приблизиться север осовца начать артиллерийский борьба крепость',\n",
              " 'артиллерийский бой принимать участие тяжёлый калибр',\n",
              " 'ранний утро 14 сентябрь огонь достигнуть значительный напряжение']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqanL3ybRnjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_w2v = gensim.models.Word2Vec([text.split() for text in data_norm], size=50, sg=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNTHoPBj4OD3",
        "colab_type": "code",
        "outputId": "17e0ffab-995a-478e-deb5-47ea3469e841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "my_w2v.most_similar('чечня')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('дагестан', 0.8086865544319153),\n",
              " ('чеченский', 0.7842105627059937),\n",
              " ('ингушетия', 0.7530069351196289),\n",
              " ('республика', 0.7528858780860901),\n",
              " ('гудермес', 0.7520864605903625),\n",
              " ('грозный', 0.7513536810874939),\n",
              " ('аслан', 0.7377203106880188),\n",
              " ('чеченец', 0.7351552248001099),\n",
              " ('наурский', 0.7228381037712097),\n",
              " ('боевик', 0.7208527326583862)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UjOP9Mq4b54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dim = 50\n",
        "#get_embedding(data_norm, my_w2v, dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nepsHLlxfyW7",
        "colab_type": "code",
        "outputId": "b308df91-7d1f-4444-ec7c-ee7f82d4670c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#rusv_v2w = gensim.models.KeyedVectors.load_word2vec_format('news_mystem_skipgram_1000_20_2015.bin.gz', binary=True)\n",
        "# rusv_v2w = gensim.models.KeyedVectors.load_word2vec_format('model.bin', binary=True)\n",
        "rusv_v2w = gensim.models.KeyedVectors.load_word2vec_format('model.bin', binary=True)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K80JkCtYfy0H",
        "colab_type": "code",
        "outputId": "910d40ee-dd80-427d-b365-f9b1d3493f5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "rusv_v2w.most_similar('путин_NOUN')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('путина_NOUN', 0.7462903261184692),\n",
              " ('россия_NOUN', 0.725328266620636),\n",
              " ('медведев_NOUN', 0.6876657605171204),\n",
              " ('зюган_NOUN', 0.6616120338439941),\n",
              " ('жириновский_ADJ', 0.6568332314491272),\n",
              " ('путин_PROPN', 0.6374367475509644),\n",
              " ('лдпр_NOUN', 0.6191665530204773),\n",
              " ('жирик_NOUN', 0.6120600700378418),\n",
              " ('кпрфа_PROPN', 0.6116076707839966),\n",
              " ('россие_NOUN', 0.6096259355545044)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-7ficwpVPDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_df_embedding(text, model, dim):\n",
        "    text = text.split()\n",
        "    words = Counter(text)\n",
        "    total = len(text)\n",
        "    vectors = np.zeros((len(words), dim))   \n",
        "    for i, word in enumerate(words):\n",
        "        #print(i)\n",
        "        try:\n",
        "            #print(model.most_similar(word))\n",
        "            v = model[word]\n",
        "        except (KeyError, ValueError) as errs:\n",
        "            #print(errs)\n",
        "            continue\n",
        "            #print(v*(words[word]/total))\n",
        "        vectors[i] = v*(words[word]/total)\n",
        "            #print(i)\n",
        "            #print(vectors[i])\n",
        "\n",
        "    #print(vectors)\n",
        "    if vectors.any():\n",
        "        #print(vector)\n",
        "        vector = np.average(vectors, axis=0)\n",
        "    else:\n",
        "        vector = np.zeros((dim))\n",
        "    \n",
        "    return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwMRV4iJXrxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_xml = html.fromstring(open('paraphrases_gold.xml', 'rb').read())\n",
        "texts_1 = []\n",
        "texts_2 = []\n",
        "classes = []\n",
        "\n",
        "for p in corpus_xml.xpath('//paraphrase'):\n",
        "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
        "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
        "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
        "    \n",
        "df_data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xemqbs8R6R0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_data['text_1_norm'] = df_data['text_1'].apply(normalize)\n",
        "df_data['text_2_norm'] = df_data['text_2'].apply(normalize)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foeHzuCyYbeN",
        "colab_type": "code",
        "outputId": "dfbd5801-ac57-4311-efef-91f9bcba6700",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "\n",
        "dim = 50\n",
        "df_data['text_1_notnorm'] = df_data['text_1'].apply(tokenize)\n",
        "df_data['text_2_notnorm'] = df_data['text_2'].apply(tokenize)\n",
        "\n",
        "X_text_1_ft = np.zeros((len(df_data['text_1_notnorm']), dim))\n",
        "X_text_2_ft = np.zeros((len(df_data['text_2_notnorm']), dim))\n",
        "\n",
        "for i, text in enumerate(df_data['text_1_notnorm'].values):\n",
        "    X_text_1_ft[i] = get_df_embedding(text, my_w2v, dim)\n",
        "    \n",
        "for i, text in enumerate(df_data['text_2_notnorm'].values):\n",
        "    X_text_2_ft[i] = get_df_embedding(text, my_w2v, dim)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rb8a07_TK9B",
        "colab_type": "code",
        "outputId": "a31b9dd4-cd92-42da-8cf0-6e4c094a2dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "df_data.head(5)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_1</th>\n",
              "      <th>text_2</th>\n",
              "      <th>label</th>\n",
              "      <th>text_1_norm</th>\n",
              "      <th>text_2_norm</th>\n",
              "      <th>text_1_notnorm</th>\n",
              "      <th>text_2_notnorm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Цены на нефть восстанавливаются</td>\n",
              "      <td>Парламент Словакии поблагодарил народы бывшего...</td>\n",
              "      <td>-1</td>\n",
              "      <td>цена нефть восстанавливаться</td>\n",
              "      <td>парламент словакия поблагодарить народ бывший ...</td>\n",
              "      <td>цены на нефть восстанавливаются</td>\n",
              "      <td>парламент словакии поблагодарил народы бывшего...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"Гоголь-центр\" покажет видеозапись скандальног...</td>\n",
              "      <td>Кехман запретил «Гоголь-центру» показывать вид...</td>\n",
              "      <td>-1</td>\n",
              "      <td>гоголь-центр показать видеозапись скандальный ...</td>\n",
              "      <td>кехман запретить гоголь-центр показывать видео...</td>\n",
              "      <td>гоголь-центр покажет видеозапись скандального ...</td>\n",
              "      <td>кехман запретил гоголь-центру показывать видео...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Агент: РФС вновь задерживает зарплату Фабио Ка...</td>\n",
              "      <td>СМИ: Агент Фабио Капелло грозится подать в суд...</td>\n",
              "      <td>-1</td>\n",
              "      <td>агент рфс вновь задерживать зарплата фабио кап...</td>\n",
              "      <td>сми агент фабио капелло грозиться подать суд рфс</td>\n",
              "      <td>агент рфс вновь задерживает зарплату фабио кап...</td>\n",
              "      <td>сми агент фабио капелло грозится подать в суд ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>День Победы в Москве обещает выдаться облачным</td>\n",
              "      <td>Любляна отпразднует День Победы вместе с Москвой</td>\n",
              "      <td>-1</td>\n",
              "      <td>день победа москва обещать выдаться облачный</td>\n",
              "      <td>люблян отпраздновать день победа вместе москва</td>\n",
              "      <td>день победы в москве обещает выдаться облачным</td>\n",
              "      <td>любляна отпразднует день победы вместе с москвой</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Посол РФ в США: Россия будет бороться с попытк...</td>\n",
              "      <td>Правительство запланировало заработать на лоте...</td>\n",
              "      <td>-1</td>\n",
              "      <td>посол рф сша россия бороться попытка переписат...</td>\n",
              "      <td>правительство запланировать заработать лотерея...</td>\n",
              "      <td>посол рф в сша россия будет бороться с попытка...</td>\n",
              "      <td>правительство запланировало заработать на лоте...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              text_1  ...                                     text_2_notnorm\n",
              "0                    Цены на нефть восстанавливаются  ...  парламент словакии поблагодарил народы бывшего...\n",
              "1  \"Гоголь-центр\" покажет видеозапись скандальног...  ...  кехман запретил гоголь-центру показывать видео...\n",
              "2  Агент: РФС вновь задерживает зарплату Фабио Ка...  ...  сми агент фабио капелло грозится подать в суд ...\n",
              "3     День Победы в Москве обещает выдаться облачным  ...   любляна отпразднует день победы вместе с москвой\n",
              "4  Посол РФ в США: Россия будет бороться с попытк...  ...  правительство запланировало заработать на лоте...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf1GHgTxi4_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_text_ft = np.concatenate([X_text_1_ft, X_text_2_ft], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ar1wE4jjJT6",
        "colab_type": "code",
        "outputId": "8dbdc552-b831-4275-c8e0-fd4c4634e92d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#Это получается предобученная модель\n",
        "y = df_data['label'].values\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(X_text_ft, y,random_state=1)\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\n",
        "                             class_weight='balanced')\n",
        "clf.fit(train_X, train_y)\n",
        "preds = clf.predict(valid_X)\n",
        "print(classification_report(valid_y, preds))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.51      0.54      0.52       181\n",
            "           0       0.53      0.50      0.51       207\n",
            "           1       0.28      0.28      0.28        93\n",
            "\n",
            "    accuracy                           0.47       481\n",
            "   macro avg       0.44      0.44      0.44       481\n",
            "weighted avg       0.47      0.47      0.47       481\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2zSaWSFv_TU",
        "colab_type": "code",
        "outputId": "41efdc58-7eeb-4561-96f0-670297f0c160",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(clf, X_text_ft, y, cv=5, scoring = 'f1_micro')\n",
        "scores.mean()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4069642857142857"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXr_4FbN2fDQ",
        "colab_type": "code",
        "outputId": "e1da6c05-269f-4c78-95f4-bbea1a256563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(X_text_ft, y,random_state=1)\n",
        "clf = LogisticRegression(C=1000)\n",
        "clf.fit(train_X, train_y)\n",
        "preds = clf.predict(valid_X)\n",
        "print(classification_report(valid_y, preds))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.46      0.55      0.50       181\n",
            "           0       0.48      0.57      0.52       207\n",
            "           1       0.32      0.09      0.14        93\n",
            "\n",
            "    accuracy                           0.47       481\n",
            "   macro avg       0.42      0.40      0.39       481\n",
            "weighted avg       0.44      0.47      0.44       481\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgqVsdwZr1Kf",
        "colab_type": "code",
        "outputId": "20b63344-dee3-4fc3-c192-58451284bbe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(clf, X_text_ft, y, cv=5, scoring = 'f1_micro')\n",
        "scores.mean()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.45477949134199136"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1SnTUYfNlcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_csv = pd.read_csv('data_paraphraser_norm.csv', error_bad_lines=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYi5woRlQRs9",
        "colab_type": "code",
        "outputId": "f57a99c6-eead-47cd-9f0d-5ccf9c728ac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "corpus_csv['text_1_norm'].apply"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Series.apply of 0       полицейский_NOUN разрешать_VERB стрелять_VERB ...\n",
              "1       право_ADV полицейский_NOUN на_ADP проникновени...\n",
              "2       президент_NOUN египет_NOUN вводить_VERB чрезвы...\n",
              "3       вернуться_VERB из_ADP сирия_NOUN россиянин_NOU...\n",
              "4       в_ADP москва_NOUN из_ADP сирия_NOUN вернуться_...\n",
              "                              ...                        \n",
              "7222    путин_NOUN освобождать_VERB от_ADP должность_N...\n",
              "7223    облако_NOUN над_ADP москва_NOUN в_ADP день_NOU...\n",
              "7224    любляна_NOUN отпраздновать_VERB день_NOUN побе...\n",
              "7225    девять_NUM самолет_NOUN ввс_NOUN разгонять_VER...\n",
              "7226    май_NOUN метрополитен_NOUN петербург_NOUN быть...\n",
              "Name: text_1_norm, Length: 7227, dtype: object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my1_c2cNtWDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "dim = 50\n",
        "\n",
        "# corpus_csv['text_1_norm'] = corpus_csv['text_1_norm'].apply()\n",
        "# corpus_csv['text_2_norm'] = corpus_csv['text_2_norm'].apply()\n",
        "\n",
        "X_text_1_ft = np.zeros((len(corpus_csv['text_1_norm']), 300))\n",
        "X_text_2_ft = np.zeros((len(corpus_csv['text_2_norm']), 300))\n",
        "\n",
        "for i, text in enumerate(corpus_csv['text_1_norm'].values):\n",
        "    X_text_1_ft[i] = get_df_embedding(text, rusv_v2w, 300)\n",
        "    \n",
        "for i, text in enumerate(corpus_csv['text_2_norm'].values):\n",
        "    X_text_2_ft[i] = get_df_embedding(text, rusv_v2w, 300)\n",
        "    #print(X_text_2_ft[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL4XpW1UNIsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_text_ft = np.concatenate([X_text_1_ft, X_text_2_ft], axis=1)\n",
        "#X_text_ft"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPQvZdG0NIuY",
        "colab_type": "code",
        "outputId": "8adeca47-bc15-4d4e-92b4-e5d9bedb7489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "y = corpus_csv['label'].values\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(X_text_ft, y,random_state=1)\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\n",
        "                             class_weight='balanced')\n",
        "clf.fit(train_X, train_y)\n",
        "preds = clf.predict(valid_X)\n",
        "print(classification_report(valid_y, preds))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.54      0.59      0.56       629\n",
            "           0       0.50      0.51      0.50       737\n",
            "           1       0.39      0.33      0.36       441\n",
            "\n",
            "    accuracy                           0.49      1807\n",
            "   macro avg       0.48      0.48      0.48      1807\n",
            "weighted avg       0.49      0.49      0.49      1807\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s52-nFsouZSY",
        "colab_type": "code",
        "outputId": "23ad7543-dfe4-4018-b811-ad1698f84a0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(clf, X_text_ft, y, cv=5, scoring = 'f1_micro')\n",
        "scores.mean()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4311530675243004"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd1sn0sFiU2S",
        "colab_type": "code",
        "outputId": "21e96bb8-50fb-4144-b42f-756548d03419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "y = corpus_csv['label'].values\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(X_text_ft, y,random_state=1)\n",
        "clf = RandomForestClassifier(n_estimators=200, max_depth=7, min_samples_leaf=15,\n",
        "                             class_weight='balanced')\n",
        "clf.fit(train_X, train_y)\n",
        "preds = clf.predict(valid_X)\n",
        "print(classification_report(valid_y, preds))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.55      0.58      0.56       629\n",
            "           0       0.49      0.51      0.50       737\n",
            "           1       0.38      0.33      0.35       441\n",
            "\n",
            "    accuracy                           0.49      1807\n",
            "   macro avg       0.47      0.47      0.47      1807\n",
            "weighted avg       0.48      0.49      0.48      1807\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxNJd005SPmX",
        "colab_type": "code",
        "outputId": "841d671f-40dc-4146-a982-980700bfab6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(clf, X_text_ft, y, cv=5, scoring = 'f1_micro')\n",
        "scores.mean()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4364094243994888"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAVJAdyeSPo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEioWjNzSPrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# per = open('data_paraphraser_norm.csv', 'r')\n",
        "# per = per.read().splitlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiDrMo38SPuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# corpus_csv = pd.read_csv('data_paraphraser_norm.csv', error_bad_lines=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeLu52_BSPwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# per_lines_norm = [normalize(text) for text in per]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuM2A85CuRPW",
        "colab_type": "code",
        "outputId": "2bca862f-e369-4bec-df61-5db767e3e56f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        }
      },
      "source": [
        "vectors_df = corpus_csv\n",
        "# vectors_df[texts_1]\n",
        "\n",
        "vectors_df['text_1_nor'] = vectors_df['text_1'].apply(normalize)\n",
        "vectors_df['text_2_nor'] = vectors_df['text_2'].apply(normalize)\n",
        "vectors_df.head(2)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text_1</th>\n",
              "      <th>text_2</th>\n",
              "      <th>text_1_norm</th>\n",
              "      <th>text_2_norm</th>\n",
              "      <th>text_1_nor</th>\n",
              "      <th>text_2_nor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
              "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
              "      <td>полицейский_NOUN разрешать_VERB стрелять_VERB ...</td>\n",
              "      <td>полиция_NOUN мочь_VERB разрешать_VERB стрелять...</td>\n",
              "      <td>полицейский разрешить стрелять поражение гражд...</td>\n",
              "      <td>полиция мочь разрешить стрелять хулиган травма...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
              "      <td>Правила внесудебного проникновения полицейских...</td>\n",
              "      <td>право_ADV полицейский_NOUN на_ADP проникновени...</td>\n",
              "      <td>правило_NOUN внесудебный_ADJ проникновение_NOU...</td>\n",
              "      <td>право полицейский проникновение жилища решить ...</td>\n",
              "      <td>правило внесудебный проникновение полицейский ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
              "      <td>Власти Египта угрожают ввести в стране чрезвыч...</td>\n",
              "      <td>президент_NOUN египет_NOUN вводить_VERB чрезвы...</td>\n",
              "      <td>власть_NOUN египет_NOUN угрожать_VERB вводить_...</td>\n",
              "      <td>президент египет ввести чрезвычайный положение...</td>\n",
              "      <td>власть египет угрожать ввести страна чрезвычай...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1</td>\n",
              "      <td>Вернувшихся из Сирии россиян волнует вопрос тр...</td>\n",
              "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
              "      <td>вернуться_VERB из_ADP сирия_NOUN россиянин_NOU...</td>\n",
              "      <td>самолет_NOUN мчс_NOUN вывозить_VERB россиянин_...</td>\n",
              "      <td>вернуться сирия россиянин волновать вопрос тру...</td>\n",
              "      <td>самолёт мчс вывезти россиянин разрушить сирия</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>В Москву из Сирии вернулись 2 самолета МЧС с р...</td>\n",
              "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
              "      <td>в_ADP москва_NOUN из_ADP сирия_NOUN вернуться_...</td>\n",
              "      <td>самолет_NOUN мчс_NOUN вывозить_VERB россиянин_...</td>\n",
              "      <td>москва сирия вернуться 2 самолёт мчс россиянин...</td>\n",
              "      <td>самолёт мчс вывезти россиянин разрушить сирия</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7222</th>\n",
              "      <td>0</td>\n",
              "      <td>Путин освободил от должности ряд генералов</td>\n",
              "      <td>Путин снял с должностей более 20 руководителей...</td>\n",
              "      <td>путин_NOUN освобождать_VERB от_ADP должность_N...</td>\n",
              "      <td>путин_NOUN снимать_VERB с_ADP должность_NOUN м...</td>\n",
              "      <td>путин освободить должность ряд генералов</td>\n",
              "      <td>путин снять должность 20 руководитель-силовик</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7223</th>\n",
              "      <td>-1</td>\n",
              "      <td>Облака над Москвой в День Победы разгонят девя...</td>\n",
              "      <td>Путеводитель по Дню Победы: как провести 9 мая...</td>\n",
              "      <td>облако_NOUN над_ADP москва_NOUN в_ADP день_NOU...</td>\n",
              "      <td>путеводитель_NOUN по_ADP день_NOUN победа_NOUN...</td>\n",
              "      <td>облако москва день победа разогнать девять сам...</td>\n",
              "      <td>путеводитель день победа провести 9 май москва</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7224</th>\n",
              "      <td>-1</td>\n",
              "      <td>Любляна отпразднует День Победы вместе с Москвой</td>\n",
              "      <td>В Москве ограничат движение в связи с Днем Победы</td>\n",
              "      <td>любляна_NOUN отпраздновать_VERB день_NOUN побе...</td>\n",
              "      <td>в_ADP москва_NOUN ограничивать_VERB движение_N...</td>\n",
              "      <td>люблян отпраздновать день победа вместе москва</td>\n",
              "      <td>москва ограничить движение связь днём победа</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7225</th>\n",
              "      <td>-1</td>\n",
              "      <td>Девять самолетов ВВС разгонят облака над Москв...</td>\n",
              "      <td>В Москве ограничат движение в связи с Днем Победы</td>\n",
              "      <td>девять_NUM самолет_NOUN ввс_NOUN разгонять_VER...</td>\n",
              "      <td>в_ADP москва_NOUN ограничивать_VERB движение_N...</td>\n",
              "      <td>девять самолёт ввс разогнать облако москва ден...</td>\n",
              "      <td>москва ограничить движение связь днём победа</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7226</th>\n",
              "      <td>-1</td>\n",
              "      <td>9 мая метрополитен Петербурга будет работать к...</td>\n",
              "      <td>Мартынов: комендантский час в Донецке 9 мая бу...</td>\n",
              "      <td>май_NOUN метрополитен_NOUN петербург_NOUN быть...</td>\n",
              "      <td>мартынов_NOUN комендантский_ADJ час_NOUN в_ADP...</td>\n",
              "      <td>9 май метрополитен петербург работать круглосу...</td>\n",
              "      <td>мартынов комендантский час донецк 9 май отменный</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7227 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      label  ...                                         text_2_nor\n",
              "0         0  ...  полиция мочь разрешить стрелять хулиган травма...\n",
              "1         0  ...  правило внесудебный проникновение полицейский ...\n",
              "2         0  ...  власть египет угрожать ввести страна чрезвычай...\n",
              "3        -1  ...      самолёт мчс вывезти россиянин разрушить сирия\n",
              "4         0  ...      самолёт мчс вывезти россиянин разрушить сирия\n",
              "...     ...  ...                                                ...\n",
              "7222      0  ...      путин снять должность 20 руководитель-силовик\n",
              "7223     -1  ...     путеводитель день победа провести 9 май москва\n",
              "7224     -1  ...       москва ограничить движение связь днём победа\n",
              "7225     -1  ...       москва ограничить движение связь днём победа\n",
              "7226     -1  ...   мартынов комендантский час донецк 9 май отменный\n",
              "\n",
              "[7227 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuVWrnotHDaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_lines_norm = [normalize(text) for text in data.splitlines()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giKPmA2rHDfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv = CountVectorizer(min_df=3, max_df=0.4, max_features=1000)\n",
        "X = cv.fit_transform(data_lines_norm)\n",
        "X_per = cv.fit_transform(per_lines_norm )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw1gN-gan_HV",
        "colab_type": "code",
        "outputId": "c6433aa4-be26-497e-f1fa-86d7af6a4970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nmf = NMF(50)\n",
        "nmf.fit(X_per)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
              "    n_components=50, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
              "    verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKexZbOxNvi4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "2c73b455-6005-4051-ea17-9c4f6ecaf323"
      },
      "source": [
        "tfidf = TfidfVectorizer(min_df=3, max_df=0.4, max_features=1000)\n",
        "tfidf.fit(pd.concat([vectors_df['text_1_nor'], vectors_df['text_2_nor']]))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=0.4, max_features=1000,\n",
              "                min_df=3, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVitMIL-obdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svd = TruncatedSVD(200)\n",
        "\n",
        "X_text_1 = svd.fit_transform(tfidf.transform(vectors_df['text_1_nor']))\n",
        "X_text_2 = svd.fit_transform(tfidf.transform(vectors_df['text_2_nor']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twk5qgdPpInF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vect_dict = []\n",
        "for i, text in enumerate(X_text_1):\n",
        "    distance = cosine_distances(X_text_2[i].reshape(-1, 1), X_text_1[i].reshape(-1, 1))\n",
        "    vect_dict.append(distance)\n",
        "vectors_df['distance_svd'] = vect_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zavre8MHRSJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIGinOfDRSFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRpucRjpRSDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDhAxRG1OlYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nmf = NMF(50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9WN8D6lOlVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_text_1_nmf = nmf.fit_transform(tfidf.transform(vectors_df['text_1_nor']))\n",
        "X_text_2_nmf = nmf.fit_transform(tfidf.transform(vectors_df['text_2_nor']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5dmmmsfOlSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vect_dict = []\n",
        "for i, text in enumerate(X_text_1):\n",
        "    distance = cosine_distances(X_text_2_nmf[i].reshape(-1, 1), X_text_1_nmf[i].reshape(-1, 1))\n",
        "    vect_dict.append(distance)\n",
        "vectors_df['distance_nmf'] = vect_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKqt7BInQzZj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "3ba52a0e-9cfd-4cbb-beea-9deb74fd2fc3"
      },
      "source": [
        "vectors_df.head()"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text_1</th>\n",
              "      <th>text_2</th>\n",
              "      <th>text_1_norm</th>\n",
              "      <th>text_2_norm</th>\n",
              "      <th>text_1_nor</th>\n",
              "      <th>text_2_nor</th>\n",
              "      <th>distance_Word2Vec(vocab=10162, size=50, alpha=0.025)</th>\n",
              "      <th>distance_FastText(vocab=9811, size=50, alpha=0.025)</th>\n",
              "      <th>distance_&lt;gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f01cb8c9c88&gt;</th>\n",
              "      <th>distance_svd</th>\n",
              "      <th>distance_nmf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
              "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
              "      <td>полицейский_NOUN разрешать_VERB стрелять_VERB ...</td>\n",
              "      <td>полиция_NOUN мочь_VERB разрешать_VERB стрелять...</td>\n",
              "      <td>полицейский разрешить стрелять поражение гражд...</td>\n",
              "      <td>полиция мочь разрешить стрелять хулиган травма...</td>\n",
              "      <td>[[0.0, 2.0, 0.0, 2.0, 2.0, 2.0, 0.0, 0.0, 2.0,...</td>\n",
              "      <td>[[0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 2.0, 2.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 0.0,...</td>\n",
              "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
              "      <td>Правила внесудебного проникновения полицейских...</td>\n",
              "      <td>право_ADV полицейский_NOUN на_ADP проникновени...</td>\n",
              "      <td>правило_NOUN внесудебный_ADJ проникновение_NOU...</td>\n",
              "      <td>право полицейский проникновение жилища решить ...</td>\n",
              "      <td>правило внесудебный проникновение полицейский ...</td>\n",
              "      <td>[[0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0,...</td>\n",
              "      <td>[[0.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0,...</td>\n",
              "      <td>[[0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 2.0, 2.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 0.0,...</td>\n",
              "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
              "      <td>Власти Египта угрожают ввести в стране чрезвыч...</td>\n",
              "      <td>президент_NOUN египет_NOUN вводить_VERB чрезвы...</td>\n",
              "      <td>власть_NOUN египет_NOUN угрожать_VERB вводить_...</td>\n",
              "      <td>президент египет ввести чрезвычайный положение...</td>\n",
              "      <td>власть египет угрожать ввести страна чрезвычай...</td>\n",
              "      <td>[[0.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0,...</td>\n",
              "      <td>[[0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0,...</td>\n",
              "      <td>[[0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 2.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 0.0,...</td>\n",
              "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1</td>\n",
              "      <td>Вернувшихся из Сирии россиян волнует вопрос тр...</td>\n",
              "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
              "      <td>вернуться_VERB из_ADP сирия_NOUN россиянин_NOU...</td>\n",
              "      <td>самолет_NOUN мчс_NOUN вывозить_VERB россиянин_...</td>\n",
              "      <td>вернуться сирия россиянин волновать вопрос тру...</td>\n",
              "      <td>самолёт мчс вывезти россиянин разрушить сирия</td>\n",
              "      <td>[[0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0,...</td>\n",
              "      <td>[[0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0,...</td>\n",
              "      <td>[[0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 0.0,...</td>\n",
              "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>В Москву из Сирии вернулись 2 самолета МЧС с р...</td>\n",
              "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
              "      <td>в_ADP москва_NOUN из_ADP сирия_NOUN вернуться_...</td>\n",
              "      <td>самолет_NOUN мчс_NOUN вывозить_VERB россиянин_...</td>\n",
              "      <td>москва сирия вернуться 2 самолёт мчс россиянин...</td>\n",
              "      <td>самолёт мчс вывезти россиянин разрушить сирия</td>\n",
              "      <td>[[0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0,...</td>\n",
              "      <td>[[0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 2.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0,...</td>\n",
              "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  ...                                       distance_nmf\n",
              "0      0  ...  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...\n",
              "1      0  ...  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...\n",
              "2      0  ...  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...\n",
              "3     -1  ...  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...\n",
              "4      0  ...  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eOh4r5hQzWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX_-8JWqQzRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUXwlLYNow3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def most_similar(word, id2vec):\n",
        "#     similar = [id2word[i] for i in cosine_distances(id2vec[word2id[word]].reshape(1, -1), id2vec).argsort()[0][:10]]\n",
        "#     return similar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHqd1GjopIhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fast_text = gensim.models.FastText([text.split() for text in data_lines_norm], size=50, \n",
        "                                   min_n=4, max_n=8) \n",
        "w2v = gensim.models.Word2Vec([text.split() for text in data_lines_norm], size=50, sg=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piKXPD_D_y6x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "fe9d909b-59cd-4598-c5e1-8f2c43960c18"
      },
      "source": [
        "def cosine_sim(model, text_1, text_2, res_df, dim=50):\n",
        "\n",
        "  X_text_1 = np.zeros((len(text_1), dim))\n",
        "  X_text_2 = np.zeros((len(text_2), dim))\n",
        "  vect_dict = []\n",
        "  for i, text in enumerate(text_1.values):\n",
        "      X_text_1[i] = get_df_embedding(text, model, dim)\n",
        "      \n",
        "  for i, text in enumerate(text_2.values):\n",
        "      X_text_2[i] = get_df_embedding(text, model, dim)\n",
        "  for i, text in enumerate(X_text_1):\n",
        "      distance = cosine_distances(X_text_2[i].reshape(-1, 1), X_text_1[i].reshape(-1, 1))\n",
        "      vect_dict.append(distance)\n",
        "  res_df['distance_' + str(model)] = vect_dict\n",
        "\n",
        "\n",
        "cosine_sim(my_w2v, vectors_df['text_1_nor'], vectors_df['text_2_nor'], vectors_df, dim=50)\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0-ppnxiGTfJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a912aa9c-5bca-4057-9a95-fc3f22264ac9"
      },
      "source": [
        "cosine_sim(fast_text, vectors_df['text_1_nor'], vectors_df['text_2_nor'], vectors_df, dim=50)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OutEfmKAIlgS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8ad9f765-39dd-4f9c-e016-3cce523c2669"
      },
      "source": [
        "cosine_sim(fast_text, vectors_df['text_1_nor'], vectors_df['text_2_nor'], vectors_df, dim=50)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1ixJUAwIlcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cosine_sim(rusv_v2w, vectors_df['text_1_norm'], vectors_df['text_2_norm'], vectors_df, dim=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXpLcriWIlZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matr_cosine_sim(model, text_1, text_2, res_df, dim=50)\n",
        "\n",
        "    X_text_1 = svd.fit_transform(tfidf.transform(data['text_1_norm']))\n",
        "    X_text_2 = svd.fit_transform(tfidf.transform(data['text_2_norm']))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "298b0zSZ_y39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#vectors_df = vectors_df.drop(columns='distance_w2v')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxw0epZ6HbKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors_only = vectors_df.drop(['text_1','text_2', 'text_1_norm', 'text_2_norm', 'text_1_nor', 'text_2_nor'], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EshTvYl1AeJw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "efba4b17-3be2-4dae-82a2-97ca3e1fba26"
      },
      "source": [
        "vectors_only.columns = ['label','my_w2v', 'fasttext', 'rusv_w2v', 'svd', 'nmf']\n",
        "vectors_only.head(3)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>my_w2v</th>\n",
              "      <th>fasttext</th>\n",
              "      <th>rusv_w2v</th>\n",
              "      <th>svd</th>\n",
              "      <th>nmf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.0, 2.0, 0.0, 2.0, 2.0, 2.0, 0.0, 0.0, 2.0,...</td>\n",
              "      <td>[[0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 2.0, 2.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 0.0,...</td>\n",
              "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0,...</td>\n",
              "      <td>[[0.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0,...</td>\n",
              "      <td>[[0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 2.0, 2.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 0.0,...</td>\n",
              "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0,...</td>\n",
              "      <td>[[0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0,...</td>\n",
              "      <td>[[0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 2.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 0.0,...</td>\n",
              "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  ...                                                nmf\n",
              "0      0  ...  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...\n",
              "1      0  ...  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...\n",
              "2      0  ...  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...\n",
              "\n",
              "[3 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGBEphTvAeVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_full = np.concatenate([vectors_only['my_w2v'], vectors_only['fasttext']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhditrm4WPZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLkjjK5zVqEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = vectors_only['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Psd6BR4aAeSz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "56f6814d-b5f1-4dc9-be8f-09d15c4bd039"
      },
      "source": [
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\n",
        "                             class_weight='balanced')\n",
        "clf.fit(X_full, train_y)\n",
        "preds = clf.predict(valid_X)\n",
        "print(classification_report(valid_y, preds))"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-154-aecebb8dd369>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\n\u001b[1;32m      3\u001b[0m                              class_weight='balanced')\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \"\"\"\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8FCB5r2AeQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyS2Y1WZ54p3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}