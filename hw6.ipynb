{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled23.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN8D6DOnJpz2Ffmts61cqXg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitryKutsev/hse_compling_homework/blob/master/hw6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEfNUCBFBGjx",
        "colab_type": "code",
        "outputId": "46c9256e-0cbd-4d41-dcbb-1eea8f972e3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.1/lenta-ru-news.csv.bz2"
      ],
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-19 15:58:23--  https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.1/lenta-ru-news.csv.bz2\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/87156914/619f9f00-1e96-11ea-946e-dac89df8aced?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200219%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200219T155824Z&X-Amz-Expires=300&X-Amz-Signature=8a7cba87fdb8f5772df346f08fc0a91658cf2cd211ec471abe1745bbab8f3522&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.bz2&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-02-19 15:58:24--  https://github-production-release-asset-2e65be.s3.amazonaws.com/87156914/619f9f00-1e96-11ea-946e-dac89df8aced?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200219%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200219T155824Z&X-Amz-Expires=300&X-Amz-Signature=8a7cba87fdb8f5772df346f08fc0a91658cf2cd211ec471abe1745bbab8f3522&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.bz2&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.162.171\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.162.171|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 346031300 (330M) [application/octet-stream]\n",
            "Saving to: ‘lenta-ru-news.csv.bz2.1’\n",
            "\n",
            "lenta-ru-news.csv.b 100%[===================>] 330.00M  46.8MB/s    in 7.5s    \n",
            "\n",
            "2020-02-19 15:58:31 (43.8 MB/s) - ‘lenta-ru-news.csv.bz2.1’ saved [346031300/346031300]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjFg8bpAssV7",
        "colab_type": "code",
        "outputId": "347ed599-6e80-4143-f14c-a9630acec06a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!wget https://rusvectores.org/static/models/rusvectores2/news_mystem_skipgram_1000_20_2015.bin.gz\n",
        "!ls"
      ],
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-19 15:58:34--  https://rusvectores.org/static/models/rusvectores2/news_mystem_skipgram_1000_20_2015.bin.gz\n",
            "Resolving rusvectores.org (rusvectores.org)... 116.203.104.23\n",
            "Connecting to rusvectores.org (rusvectores.org)|116.203.104.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 549952184 (524M) [application/x-gzip]\n",
            "Saving to: ‘news_mystem_skipgram_1000_20_2015.bin.gz.2’\n",
            "\n",
            "news_mystem_skipgra 100%[===================>] 524.47M  25.8MB/s    in 22s     \n",
            "\n",
            "2020-02-19 15:58:57 (23.7 MB/s) - ‘news_mystem_skipgram_1000_20_2015.bin.gz.2’ saved [549952184/549952184]\n",
            "\n",
            "185.zip\t\t\t   model.txt\n",
            "186.zip\t\t\t   news_mystem_skipgram_1000_20_2015.bin.gz\n",
            "data_paraphraser_norm.csv  news_mystem_skipgram_1000_20_2015.bin.gz.1\n",
            "lenta-ru-news.csv\t   news_mystem_skipgram_1000_20_2015.bin.gz.2\n",
            "lenta-ru-news.csv.bz2\t   paraphraser_gold.zip\n",
            "lenta-ru-news.csv.bz2.1    paraphrases_gold.xml\n",
            "meta.json\t\t   README\n",
            "model.bin\t\t   sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrGWKppNE97u",
        "colab_type": "code",
        "outputId": "c0216cbc-26c1-4ef4-dbdc-45136876f332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "!wget https://github.com/DmitryKutsev/hse_compling_homework/blob/master/paraphraser_gold.zip?raw=true\n",
        "!wget https://github.com/DmitryKutsev/hse_compling_homework/blob/master/data_paraphraser_norm.csv?raw=true\n",
        "!mv paraphraser_gold.zip?raw=true paraphraser_gold.zip\n",
        "!mv data_paraphraser_norm.csv?raw=true data_paraphraser_norm.csv\n",
        "!unzip paraphraser_gold.zip\n",
        "!ls"
      ],
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-19 15:59:00--  https://github.com/DmitryKutsev/hse_compling_homework/blob/master/paraphraser_gold.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/DmitryKutsev/hse_compling_homework/raw/master/paraphraser_gold.zip [following]\n",
            "--2020-02-19 15:59:00--  https://github.com/DmitryKutsev/hse_compling_homework/raw/master/paraphraser_gold.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/DmitryKutsev/hse_compling_homework/master/paraphraser_gold.zip [following]\n",
            "--2020-02-19 15:59:01--  https://raw.githubusercontent.com/DmitryKutsev/hse_compling_homework/master/paraphraser_gold.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 118720 (116K) [application/zip]\n",
            "Saving to: ‘paraphraser_gold.zip?raw=true’\n",
            "\n",
            "\r          paraphras   0%[                    ]       0  --.-KB/s               \rparaphraser_gold.zi 100%[===================>] 115.94K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-02-19 15:59:01 (3.71 MB/s) - ‘paraphraser_gold.zip?raw=true’ saved [118720/118720]\n",
            "\n",
            "--2020-02-19 15:59:03--  https://github.com/DmitryKutsev/hse_compling_homework/blob/master/data_paraphraser_norm.csv?raw=true\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/DmitryKutsev/hse_compling_homework/raw/master/data_paraphraser_norm.csv [following]\n",
            "--2020-02-19 15:59:03--  https://github.com/DmitryKutsev/hse_compling_homework/raw/master/data_paraphraser_norm.csv\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/DmitryKutsev/hse_compling_homework/master/data_paraphraser_norm.csv [following]\n",
            "--2020-02-19 15:59:03--  https://raw.githubusercontent.com/DmitryKutsev/hse_compling_homework/master/data_paraphraser_norm.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3575140 (3.4M) [text/plain]\n",
            "Saving to: ‘data_paraphraser_norm.csv?raw=true’\n",
            "\n",
            "data_paraphraser_no 100%[===================>]   3.41M  20.9MB/s    in 0.2s    \n",
            "\n",
            "2020-02-19 15:59:04 (20.9 MB/s) - ‘data_paraphraser_norm.csv?raw=true’ saved [3575140/3575140]\n",
            "\n",
            "Archive:  paraphraser_gold.zip\n",
            "replace paraphrases_gold.xml? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "185.zip\t\t\t   model.txt\n",
            "186.zip\t\t\t   news_mystem_skipgram_1000_20_2015.bin.gz\n",
            "data_paraphraser_norm.csv  news_mystem_skipgram_1000_20_2015.bin.gz.1\n",
            "lenta-ru-news.csv\t   news_mystem_skipgram_1000_20_2015.bin.gz.2\n",
            "lenta-ru-news.csv.bz2\t   paraphraser_gold.zip\n",
            "lenta-ru-news.csv.bz2.1    paraphrases_gold.xml\n",
            "meta.json\t\t   README\n",
            "model.bin\t\t   sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMeuIrD5if_O",
        "colab_type": "code",
        "outputId": "5f99aaa7-5acc-4b25-dfae-3997bb354418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!bzip2 -d lenta-ru-news.csv.bz2"
      ],
      "execution_count": 315,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bzip2: Output file lenta-ru-news.csv already exists.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWZRDAhDimQk",
        "colab_type": "code",
        "outputId": "00b88172-0246-4f59-b509-9be5cea38718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "!ls\n",
        "!pip install pymorphy2[fast]\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "185.zip\t\t\t   model.txt\n",
            "186.zip\t\t\t   news_mystem_skipgram_1000_20_2015.bin.gz\n",
            "data_paraphraser_norm.csv  news_mystem_skipgram_1000_20_2015.bin.gz.1\n",
            "lenta-ru-news.csv\t   news_mystem_skipgram_1000_20_2015.bin.gz.2\n",
            "lenta-ru-news.csv.bz2\t   paraphraser_gold.zip\n",
            "lenta-ru-news.csv.bz2.1    paraphrases_gold.xml\n",
            "meta.json\t\t   README\n",
            "model.bin\t\t   sample_data\n",
            "Requirement already satisfied: pymorphy2[fast] in /usr/local/lib/python3.6/dist-packages (0.8)\n",
            "Requirement already satisfied: dawg-python>=0.7 in /usr/local/lib/python3.6/dist-packages (from pymorphy2[fast]) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from pymorphy2[fast]) (2.4.393442.3710985)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2[fast]) (0.6.2)\n",
            "Requirement already satisfied: DAWG>=0.7.3; extra == \"fast\" in /usr/local/lib/python3.6/dist-packages (from pymorphy2[fast]) (0.7.8)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 316
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zX7M6MAkoQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from collections import Counter,defaultdict\n",
        "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
        "from string import punctuation\n",
        "import os\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from lxml import html\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import gensim\n",
        "import numpy as np\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "morph = MorphAnalyzer()\n",
        "punct = punctuation+'«»—…“”*№–,'\n",
        "stops = set(stopwords.words('russian'))\n",
        "\n",
        "def normalize(text):\n",
        "    \n",
        "    words = [word.strip(punct) for word in text.lower().split()]\n",
        "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "def tokenize(text):\n",
        "    \n",
        "    words = [word.strip(punct) for word in text.lower().split()]\n",
        "\n",
        "    return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHRVBKC9niKF",
        "colab_type": "code",
        "outputId": "566dcee7-1437-4106-dcb9-cc2d4be8555f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_handler = open('lenta-ru-news.csv', 'r')\n",
        "data = data_handler.read()\n",
        "print(len(data))\n",
        "data = data[:6000000]"
      ],
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1172327461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qQ8d2jirWll",
        "colab_type": "code",
        "outputId": "f246afc7-fc45-4984-ebc0-002e39e63223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print(data[:1000])\n"
      ],
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "url,title,text,topic,tags,date\n",
            "https://lenta.ru/news/1914/09/16/hungarnn/,1914. Русские войска вступили в пределы Венгрии  ,\"Бои у Сопоцкина и Друскеник закончились отступлением германцев. Неприятель, приблизившись с севера к Осовцу начал артиллерийскую борьбу с крепостью. В артиллерийском бою принимают участие тяжелые калибры. С раннего утра 14 сентября огонь достиг значительного напряжения. Попытка германской пехоты пробиться ближе к крепости отражена. В Галиции мы заняли Дембицу. Большая колонна, отступавшая по шоссе от Перемышля к Саноку, обстреливалась с высот нашей батареей и бежала, бросив парки, обоз и автомобили. Вылазки гарнизона Перемышля остаются безуспешными. При продолжающемся отступлении австрийцев обнаруживается полное перемешивание их частей, захватываются новые партии пленных, орудия и прочая материальная часть. На перевале Ужок мы разбили неприятельский отряд, взяли его артиллерию и много пленных и, продолжая преследовать, вступили в пределы Венгрии. \n",
            "«Русский инвали\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWmUFssTRnoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_norm = [normalize(text) for text in data.split('.')]\n",
        "#data_norm = \".\".join(data_norm)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5J3zqnP8Rnmh",
        "colab_type": "code",
        "outputId": "9b165b9c-33fe-459e-ab64-2f5b9962518f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "data_norm = [text for text in data_norm if text]\n",
        "print(len(data_norm))\n",
        "data_norm[:6]"
      ],
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['url,title,text,topic,tags,date https://lenta',\n",
              " 'ru/news/1914/09/16/hungarnn/,1914',\n",
              " 'русский войско вступить предел венгрия бой сопоцкина друскеник закончиться отступление германец',\n",
              " 'неприятель приблизиться север осовца начать артиллерийский борьба крепость',\n",
              " 'артиллерийский бой принимать участие тяжёлый калибр',\n",
              " 'ранний утро 14 сентябрь огонь достигнуть значительный напряжение']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 321
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqanL3ybRnjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_w2v = gensim.models.Word2Vec([text.split() for text in data_norm], size=50, sg=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNTHoPBj4OD3",
        "colab_type": "code",
        "outputId": "cf51a228-95db-4b0e-87c4-1f9fe5ce00f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "my_w2v.most_similar('чечня')"
      ],
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('чеченский', 0.8202345371246338),\n",
              " ('дагестан', 0.7921947240829468),\n",
              " ('республика', 0.7872808575630188),\n",
              " ('грозный', 0.767946183681488),\n",
              " ('гудермес', 0.740414023399353),\n",
              " ('масхадов', 0.7178921103477478),\n",
              " ('боевик', 0.716596782207489),\n",
              " ('ингушетия', 0.7139213681221008),\n",
              " ('бандформирование', 0.7128461599349976),\n",
              " ('абхазия', 0.7110559940338135)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 323
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UjOP9Mq4b54",
        "colab_type": "code",
        "outputId": "a5b1ceb7-189c-4144-ddb8-7093cff84669",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "dim = 50\n",
        "get_embedding(data_norm, my_w2v, dim)"
      ],
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-3.09441571e-08, -7.87149774e-08,  1.07313980e-07, -1.82417891e-09,\n",
              "       -1.18236060e-08,  1.53033524e-08, -6.33423944e-08, -7.30311697e-08,\n",
              "       -2.97573896e-08,  4.86181928e-08,  3.23832244e-08, -6.27493334e-08,\n",
              "       -2.83825310e-08, -1.98730085e-08, -7.54928560e-08, -4.17951986e-08,\n",
              "       -1.94083206e-08, -8.03071622e-08,  6.24000620e-08, -2.65043554e-08,\n",
              "        9.88499923e-09,  3.47791597e-08,  3.62603998e-08, -1.57076282e-08,\n",
              "       -4.77165309e-08, -3.64112081e-08,  8.53117702e-09,  4.07912892e-08,\n",
              "       -1.41890170e-08,  2.38664419e-08, -9.32649154e-09,  5.27152831e-08,\n",
              "        5.28227593e-09,  4.05267618e-08, -3.34719930e-08, -1.22331574e-10,\n",
              "        2.21215860e-09, -4.87119762e-08, -5.98922699e-09, -4.13303014e-08,\n",
              "        5.31587312e-08, -4.81894553e-08, -6.23652987e-08, -1.30173372e-08,\n",
              "       -6.65622118e-08, -1.72031093e-08, -2.03094333e-08,  1.32566529e-08,\n",
              "       -4.18184569e-08,  2.28526826e-08])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 324
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WPvPEgeb98S",
        "colab_type": "code",
        "outputId": "5ebedbe5-53fc-4a6e-b544-7cdf24470a30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "!rm model.bin model.txt \n",
        "# !wget http://vectors.nlpl.eu/repository/20/186.zip\n",
        "# !wget http://vectors.nlpl.eu/repository/20/185.zip\n",
        "!unzip 185.zip\n",
        "!ls"
      ],
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  185.zip\n",
            "replace meta.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "  inflating: model.bin               \n",
            "  inflating: model.txt               n\n",
            "n\n",
            "n\n",
            "\n",
            "replace README? [y]es, [n]o, [A]ll, [N]one, [r]ename: 185.zip\t\t\t   model.txt\n",
            "186.zip\t\t\t   news_mystem_skipgram_1000_20_2015.bin.gz\n",
            "data_paraphraser_norm.csv  news_mystem_skipgram_1000_20_2015.bin.gz.1\n",
            "lenta-ru-news.csv\t   news_mystem_skipgram_1000_20_2015.bin.gz.2\n",
            "lenta-ru-news.csv.bz2\t   paraphraser_gold.zip\n",
            "lenta-ru-news.csv.bz2.1    paraphrases_gold.xml\n",
            "meta.json\t\t   README\n",
            "model.bin\t\t   sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nepsHLlxfyW7",
        "colab_type": "code",
        "outputId": "8a968616-01fe-466c-bdbb-a11e6e8809b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#rusv_v2w = gensim.models.KeyedVectors.load_word2vec_format('news_mystem_skipgram_1000_20_2015.bin.gz', binary=True)\n",
        "# rusv_v2w = gensim.models.KeyedVectors.load_word2vec_format('model.bin', binary=True)\n",
        "rusv_v2w = gensim.models.KeyedVectors.load_word2vec_format('model.bin', binary=True)\n"
      ],
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K80JkCtYfy0H",
        "colab_type": "code",
        "outputId": "a09284c0-44e4-4137-b122-977648fa280c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "rusv_v2w.most_similar('путин_NOUN')"
      ],
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('путина_NOUN', 0.7462903261184692),\n",
              " ('россия_NOUN', 0.725328266620636),\n",
              " ('медведев_NOUN', 0.6876657605171204),\n",
              " ('зюган_NOUN', 0.6616120338439941),\n",
              " ('жириновский_ADJ', 0.6568332314491272),\n",
              " ('путин_PROPN', 0.6374367475509644),\n",
              " ('лдпр_NOUN', 0.6191665530204773),\n",
              " ('жирик_NOUN', 0.6120600700378418),\n",
              " ('кпрфа_PROPN', 0.6116076707839966),\n",
              " ('россие_NOUN', 0.6096259355545044)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 327
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-7ficwpVPDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_df_embedding(text, model, dim):\n",
        "    text = text.split()\n",
        "    words = Counter(text)\n",
        "    total = len(text)\n",
        "    vectors = np.zeros((len(words), dim))   \n",
        "    for i, word in enumerate(words):\n",
        "        #print(i)\n",
        "        try:\n",
        "            #print(model.most_similar(word))\n",
        "            v = model[word]\n",
        "        except (KeyError, ValueError) as errs:\n",
        "            #print(errs)\n",
        "            continue\n",
        "            #print(v*(words[word]/total))\n",
        "        vectors[i] = v*(words[word]/total)\n",
        "            #print(i)\n",
        "            #print(vectors[i])\n",
        "\n",
        "    #print(vectors)\n",
        "    if vectors.any():\n",
        "        #print(vector)\n",
        "        vector = np.average(vectors, axis=0)\n",
        "    else:\n",
        "        vector = np.zeros((dim))\n",
        "    \n",
        "    return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwMRV4iJXrxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_xml = html.fromstring(open('paraphrases_gold.xml', 'rb').read())\n",
        "texts_1 = []\n",
        "texts_2 = []\n",
        "classes = []\n",
        "\n",
        "for p in corpus_xml.xpath('//paraphrase'):\n",
        "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
        "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
        "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
        "    \n",
        "df_data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xemqbs8R6R0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_data['text_1_norm'] = df_data['text_1'].apply(normalize)\n",
        "df_data['text_2_norm'] = df_data['text_2'].apply(normalize)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foeHzuCyYbeN",
        "colab_type": "code",
        "outputId": "487db808-d0de-4a7c-c572-5cb347c7e912",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "\n",
        "dim = 50\n",
        "df_data['text_1_notnorm'] = df_data['text_1'].apply(tokenize)\n",
        "df_data['text_2_notnorm'] = df_data['text_2'].apply(tokenize)\n",
        "\n",
        "X_text_1_ft = np.zeros((len(df_data['text_1_notnorm']), dim))\n",
        "X_text_2_ft = np.zeros((len(df_data['text_2_notnorm']), dim))\n",
        "\n",
        "for i, text in enumerate(df_data['text_1_notnorm'].values):\n",
        "    X_text_1_ft[i] = get_df_embedding(text, my_w2v, dim)\n",
        "    \n",
        "for i, text in enumerate(df_data['text_2_notnorm'].values):\n",
        "    X_text_2_ft[i] = get_df_embedding(text, my_w2v, dim)"
      ],
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rb8a07_TK9B",
        "colab_type": "code",
        "outputId": "b7ae9bf8-65cc-41dc-9d21-e996d3832ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "df_data.head(5)"
      ],
      "execution_count": 332,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_1</th>\n",
              "      <th>text_2</th>\n",
              "      <th>label</th>\n",
              "      <th>text_1_norm</th>\n",
              "      <th>text_2_norm</th>\n",
              "      <th>text_1_notnorm</th>\n",
              "      <th>text_2_notnorm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Цены на нефть восстанавливаются</td>\n",
              "      <td>Парламент Словакии поблагодарил народы бывшего...</td>\n",
              "      <td>-1</td>\n",
              "      <td>цена нефть восстанавливаться</td>\n",
              "      <td>парламент словакия поблагодарить народ бывший ...</td>\n",
              "      <td>цены на нефть восстанавливаются</td>\n",
              "      <td>парламент словакии поблагодарил народы бывшего...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"Гоголь-центр\" покажет видеозапись скандальног...</td>\n",
              "      <td>Кехман запретил «Гоголь-центру» показывать вид...</td>\n",
              "      <td>-1</td>\n",
              "      <td>гоголь-центр показать видеозапись скандальный ...</td>\n",
              "      <td>кехман запретить гоголь-центр показывать видео...</td>\n",
              "      <td>гоголь-центр покажет видеозапись скандального ...</td>\n",
              "      <td>кехман запретил гоголь-центру показывать видео...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Агент: РФС вновь задерживает зарплату Фабио Ка...</td>\n",
              "      <td>СМИ: Агент Фабио Капелло грозится подать в суд...</td>\n",
              "      <td>-1</td>\n",
              "      <td>агент рфс вновь задерживать зарплата фабио кап...</td>\n",
              "      <td>сми агент фабио капелло грозиться подать суд рфс</td>\n",
              "      <td>агент рфс вновь задерживает зарплату фабио кап...</td>\n",
              "      <td>сми агент фабио капелло грозится подать в суд ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>День Победы в Москве обещает выдаться облачным</td>\n",
              "      <td>Любляна отпразднует День Победы вместе с Москвой</td>\n",
              "      <td>-1</td>\n",
              "      <td>день победа москва обещать выдаться облачный</td>\n",
              "      <td>люблян отпраздновать день победа вместе москва</td>\n",
              "      <td>день победы в москве обещает выдаться облачным</td>\n",
              "      <td>любляна отпразднует день победы вместе с москвой</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Посол РФ в США: Россия будет бороться с попытк...</td>\n",
              "      <td>Правительство запланировало заработать на лоте...</td>\n",
              "      <td>-1</td>\n",
              "      <td>посол рф сша россия бороться попытка переписат...</td>\n",
              "      <td>правительство запланировать заработать лотерея...</td>\n",
              "      <td>посол рф в сша россия будет бороться с попытка...</td>\n",
              "      <td>правительство запланировало заработать на лоте...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              text_1  ...                                     text_2_notnorm\n",
              "0                    Цены на нефть восстанавливаются  ...  парламент словакии поблагодарил народы бывшего...\n",
              "1  \"Гоголь-центр\" покажет видеозапись скандальног...  ...  кехман запретил гоголь-центру показывать видео...\n",
              "2  Агент: РФС вновь задерживает зарплату Фабио Ка...  ...  сми агент фабио капелло грозится подать в суд ...\n",
              "3     День Победы в Москве обещает выдаться облачным  ...   любляна отпразднует день победы вместе с москвой\n",
              "4  Посол РФ в США: Россия будет бороться с попытк...  ...  правительство запланировало заработать на лоте...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 332
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf1GHgTxi4_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_text_ft = np.concatenate([X_text_1_ft, X_text_2_ft], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ar1wE4jjJT6",
        "colab_type": "code",
        "outputId": "f16d574d-569e-430f-f5ad-f689ae2623c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#Это получается предобученная модель\n",
        "y = df_data['label'].values\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(X_text_ft, y,random_state=1)\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\n",
        "                             class_weight='balanced')\n",
        "clf.fit(train_X, train_y)\n",
        "preds = clf.predict(valid_X)\n",
        "print(classification_report(valid_y, preds))"
      ],
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.52      0.52      0.52       181\n",
            "           0       0.54      0.53      0.53       207\n",
            "           1       0.26      0.27      0.27        93\n",
            "\n",
            "    accuracy                           0.48       481\n",
            "   macro avg       0.44      0.44      0.44       481\n",
            "weighted avg       0.48      0.48      0.48       481\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2zSaWSFv_TU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0bdcd9af-68b9-43da-dde7-f6e26760bc3e"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(clf, X_text_ft, y, cv=5, scoring = 'f1_micro')\n",
        "scores.mean()"
      ],
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3815442828511343"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 335
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXr_4FbN2fDQ",
        "colab_type": "code",
        "outputId": "3436f732-3261-4c6e-b2e6-620966e27c11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(X_text_ft, y,random_state=1)\n",
        "clf = LogisticRegression(C=1000)\n",
        "clf.fit(train_X, train_y)\n",
        "preds = clf.predict(valid_X)\n",
        "print(classification_report(valid_y, preds))"
      ],
      "execution_count": 336,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.44      0.51      0.47       181\n",
            "           0       0.47      0.57      0.51       207\n",
            "           1       0.26      0.05      0.09        93\n",
            "\n",
            "    accuracy                           0.45       481\n",
            "   macro avg       0.39      0.38      0.36       481\n",
            "weighted avg       0.42      0.45      0.42       481\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgqVsdwZr1Kf",
        "colab_type": "code",
        "outputId": "419f701c-f9bb-4f64-a89d-b38a96a8bfd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(clf, X_text_ft, y, cv=5, scoring = 'f1_micro')\n",
        "scores.mean()"
      ],
      "execution_count": 337,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3520997876321187"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 337
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1SnTUYfNlcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_csv = pd.read_csv('data_paraphraser_norm.csv', error_bad_lines=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYi5woRlQRs9",
        "colab_type": "code",
        "outputId": "fd4ac3b3-a52d-4c10-fa3a-002ba6280df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "corpus_csv['text_1_norm'].apply"
      ],
      "execution_count": 365,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Полицейским разрешат стрелять на поражение по ...\n",
              "1       Право полицейских на проникновение в жилище ре...\n",
              "2       Президент Египта ввел чрезвычайное положение в...\n",
              "3       Вернувшихся из Сирии россиян волнует вопрос тр...\n",
              "4       В Москву из Сирии вернулись 2 самолета МЧС с р...\n",
              "                              ...                        \n",
              "7222           Путин освободил от должности ряд генералов\n",
              "7223    Облака над Москвой в День Победы разгонят девя...\n",
              "7224     Любляна отпразднует День Победы вместе с Москвой\n",
              "7225    Девять самолетов ВВС разгонят облака над Москв...\n",
              "7226    9 мая метрополитен Петербурга будет работать к...\n",
              "Name: text_1, Length: 7227, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 365
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my1_c2cNtWDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "dim = 50\n",
        "\n",
        "# corpus_csv['text_1_norm'] = corpus_csv['text_1_norm'].apply()\n",
        "# corpus_csv['text_2_norm'] = corpus_csv['text_2_norm'].apply()\n",
        "\n",
        "X_text_1_ft = np.zeros((len(corpus_csv['text_1_norm']), 300))\n",
        "X_text_2_ft = np.zeros((len(corpus_csv['text_2_norm']), 300))\n",
        "\n",
        "for i, text in enumerate(corpus_csv['text_1_norm'].values):\n",
        "    X_text_1_ft[i] = get_df_embedding(text, rusv_v2w, 300)\n",
        "    \n",
        "for i, text in enumerate(corpus_csv['text_2_norm'].values):\n",
        "    X_text_2_ft[i] = get_df_embedding(text, rusv_v2w, 300)\n",
        "    #print(X_text_2_ft[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL4XpW1UNIsB",
        "colab_type": "code",
        "outputId": "7dc88d61-9eba-47cd-db40-aa092d275b92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "X_text_ft = np.concatenate([X_text_1_ft, X_text_2_ft], axis=1)\n",
        "X_text_ft"
      ],
      "execution_count": 341,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.00795916, -0.01174852, -0.01734468, ...,  0.00814719,\n",
              "         0.01839041, -0.01368497],\n",
              "       [-0.00072169,  0.00059484, -0.01141266, ...,  0.0163677 ,\n",
              "         0.02709406, -0.02394614],\n",
              "       [-0.01263203,  0.00081496,  0.00657211, ...,  0.01804899,\n",
              "         0.02425772, -0.01231225],\n",
              "       ...,\n",
              "       [-0.00961985, -0.02270706, -0.00570588, ...,  0.00414114,\n",
              "         0.01269401, -0.0114705 ],\n",
              "       [-0.00658599, -0.00920898,  0.00010471, ...,  0.00414114,\n",
              "         0.01269401, -0.0114705 ],\n",
              "       [-0.01109063, -0.01903416, -0.01624854, ...,  0.00117148,\n",
              "         0.01932425,  0.00015063]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 341
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPQvZdG0NIuY",
        "colab_type": "code",
        "outputId": "1e5f3adb-b1ce-4a64-de30-23e67a65486e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "y = corpus_csv['label'].values\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(X_text_ft, y,random_state=1)\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\n",
        "                             class_weight='balanced')\n",
        "clf.fit(train_X, train_y)\n",
        "preds = clf.predict(valid_X)\n",
        "print(classification_report(valid_y, preds))"
      ],
      "execution_count": 342,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.55      0.60      0.57       629\n",
            "           0       0.50      0.50      0.50       737\n",
            "           1       0.37      0.32      0.35       441\n",
            "\n",
            "    accuracy                           0.49      1807\n",
            "   macro avg       0.47      0.47      0.47      1807\n",
            "weighted avg       0.49      0.49      0.49      1807\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s52-nFsouZSY",
        "colab_type": "code",
        "outputId": "552d9ed9-a410-4e39-eaee-8f7449f299d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(clf, X_text_ft, y, cv=5, scoring = 'f1_micro')\n",
        "scores.mean()"
      ],
      "execution_count": 343,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4358578969786596"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 343
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd1sn0sFiU2S",
        "colab_type": "code",
        "outputId": "efa1dd4b-8d72-42d6-aaa0-6cada9bd577b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "y = corpus_csv['label'].values\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(X_text_ft, y,random_state=1)\n",
        "clf = RandomForestClassifier(n_estimators=200, max_depth=7, min_samples_leaf=15,\n",
        "                             class_weight='balanced')\n",
        "clf.fit(train_X, train_y)\n",
        "preds = clf.predict(valid_X)\n",
        "print(classification_report(valid_y, preds))"
      ],
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.54      0.59      0.56       629\n",
            "           0       0.48      0.51      0.49       737\n",
            "           1       0.37      0.30      0.33       441\n",
            "\n",
            "    accuracy                           0.48      1807\n",
            "   macro avg       0.47      0.46      0.46      1807\n",
            "weighted avg       0.48      0.48      0.48      1807\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxNJd005SPmX",
        "colab_type": "code",
        "outputId": "cb00cf9f-996f-4952-c897-d2aa814ce1ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(clf, X_text_ft, y, cv=5, scoring = 'f1_micro')\n",
        "scores.mean()"
      ],
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4375153507827344"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 345
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAVJAdyeSPo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEioWjNzSPrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "per = open('data_paraphraser_norm.csv', 'r')\n",
        "per = per.read().splitlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiDrMo38SPuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_csv = pd.read_csv('data_paraphraser_norm.csv', error_bad_lines=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeLu52_BSPwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "per_lines_norm = [normalize(text) for text in per]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuVWrnotHDaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_lines_norm = [normalize(text) for text in data.splitlines()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUvjc4p2HDdF",
        "colab_type": "code",
        "outputId": "475363f1-d564-4656-a50a-b54085e86676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "print(len(data_lines_norm))\n",
        "data_lines_norm[:4]"
      ],
      "execution_count": 347,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4251\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['url,title,text,topic,tags,date',\n",
              " 'https://lenta.ru/news/1914/09/16/hungarnn/,1914 русский войско вступить предел венгрия бой сопоцкина друскеник закончиться отступление германец неприятель приблизиться север осовца начать артиллерийский борьба крепость артиллерийский бой принимать участие тяжёлый калибр ранний утро 14 сентябрь огонь достигнуть значительный напряжение попытка германский пехота пробиться близкий крепость отразить галиция занять дембица большой колонна отступать шоссе перемышль санок обстреливаться высота наш батарея бежать бросить парка обоз автомобиль вылазка гарнизон перемышль оставаться безуспешный продолжаться отступление австриец обнаруживаться полный перемешивание часть захватываться новое партия пленный орудие прочий материальный часть перевал ужок разбить неприятельский отряд взять артиллерия пленный продолжать преследовать вступить предел венгрия',\n",
              " 'русский инвалид 16 сентябрь 1914 года.\",библиотека,первать мировая,1914/09/16',\n",
              " 'https://lenta.ru/news/1914/09/16/lermontov/,1914 празднование столетие м.ю лермонтов отложить министерство народный просвещение вид происходить чрезвычайный событие признать соответственный день годовщина день рождение м.ю лермонтов 2-го октябрь 1914 год ограничиться совершение учебный заведение панихида поэт отложить празднование юбилей благоприятный время']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 347
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giKPmA2rHDfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv = CountVectorizer(min_df=3, max_df=0.4, max_features=1000)\n",
        "X = cv.fit_transform(data_lines_norm)\n",
        "X_per = cv.fit_transform(per_lines_norm )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw1gN-gan_HV",
        "colab_type": "code",
        "outputId": "f42a68b5-544f-48fa-c816-f38b725d116b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nmf = NMF(50)\n",
        "nmf.fit(X_per)"
      ],
      "execution_count": 360,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
              "    n_components=50, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
              "    verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 360
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVitMIL-obdG",
        "colab_type": "code",
        "outputId": "2a5792a2-e13b-4454-dbc1-a200ff90f7ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "svd = TruncatedSVD(50)\n",
        "svd.fit(X_per)"
      ],
      "execution_count": 361,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TruncatedSVD(algorithm='randomized', n_components=50, n_iter=5,\n",
              "             random_state=None, tol=0.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 361
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twk5qgdPpInF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QpbaMaNodEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id2word = {i:w for i,w in enumerate(cv.get_feature_names())}\n",
        "word2id = {w:i for i,w in id2word.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFo-zTqaotLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id2vec_svd = nmf.components_.T\n",
        "id2vec_nmf = svd.components_.T\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUXwlLYNow3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def most_similar(word, id2vec):\n",
        "    similar = [id2word[i] for i in cosine_distances(id2vec[word2id[word]].reshape(1, -1), id2vec).argsort()[0][:10]]\n",
        "    return similar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHqd1GjopIhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fast_text = gensim.models.FastText([text.split() for text in data_lines_norm], size=50, \n",
        "                                   min_n=4, max_n=8) \n",
        "w2v = gensim.models.Word2Vec([text.split() for text in data_lines_norm], size=50, sg=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuM2A85CuRPW",
        "colab_type": "code",
        "outputId": "1ab88c87-dbb0-41c6-d0f3-afe784a45dd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        }
      },
      "source": [
        "vectors_df = corpus_csv\n",
        "# vectors_df[texts_1]\n",
        "\n",
        "vectors_df['text_1_nor'] = vectors_df['text_1'].apply(normalize)\n",
        "vectors_df['text_2_nor'] = vectors_df['text_2'].apply(normalize)\n",
        "vectors_df"
      ],
      "execution_count": 373,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text_1</th>\n",
              "      <th>text_2</th>\n",
              "      <th>text_1_norm</th>\n",
              "      <th>text_2_norm</th>\n",
              "      <th>text_1_nor</th>\n",
              "      <th>text_2_nor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
              "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
              "      <td>полицейский_NOUN разрешать_VERB стрелять_VERB ...</td>\n",
              "      <td>полиция_NOUN мочь_VERB разрешать_VERB стрелять...</td>\n",
              "      <td>полицейский разрешить стрелять поражение гражд...</td>\n",
              "      <td>полиция мочь разрешить стрелять хулиган травма...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
              "      <td>Правила внесудебного проникновения полицейских...</td>\n",
              "      <td>право_ADV полицейский_NOUN на_ADP проникновени...</td>\n",
              "      <td>правило_NOUN внесудебный_ADJ проникновение_NOU...</td>\n",
              "      <td>право полицейский проникновение жилища решить ...</td>\n",
              "      <td>правило внесудебный проникновение полицейский ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
              "      <td>Власти Египта угрожают ввести в стране чрезвыч...</td>\n",
              "      <td>президент_NOUN египет_NOUN вводить_VERB чрезвы...</td>\n",
              "      <td>власть_NOUN египет_NOUN угрожать_VERB вводить_...</td>\n",
              "      <td>президент египет ввести чрезвычайный положение...</td>\n",
              "      <td>власть египет угрожать ввести страна чрезвычай...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1</td>\n",
              "      <td>Вернувшихся из Сирии россиян волнует вопрос тр...</td>\n",
              "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
              "      <td>вернуться_VERB из_ADP сирия_NOUN россиянин_NOU...</td>\n",
              "      <td>самолет_NOUN мчс_NOUN вывозить_VERB россиянин_...</td>\n",
              "      <td>вернуться сирия россиянин волновать вопрос тру...</td>\n",
              "      <td>самолёт мчс вывезти россиянин разрушить сирия</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>В Москву из Сирии вернулись 2 самолета МЧС с р...</td>\n",
              "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
              "      <td>в_ADP москва_NOUN из_ADP сирия_NOUN вернуться_...</td>\n",
              "      <td>самолет_NOUN мчс_NOUN вывозить_VERB россиянин_...</td>\n",
              "      <td>москва сирия вернуться 2 самолёт мчс россиянин...</td>\n",
              "      <td>самолёт мчс вывезти россиянин разрушить сирия</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7222</th>\n",
              "      <td>0</td>\n",
              "      <td>Путин освободил от должности ряд генералов</td>\n",
              "      <td>Путин снял с должностей более 20 руководителей...</td>\n",
              "      <td>путин_NOUN освобождать_VERB от_ADP должность_N...</td>\n",
              "      <td>путин_NOUN снимать_VERB с_ADP должность_NOUN м...</td>\n",
              "      <td>путин освободить должность ряд генералов</td>\n",
              "      <td>путин снять должность 20 руководитель-силовик</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7223</th>\n",
              "      <td>-1</td>\n",
              "      <td>Облака над Москвой в День Победы разгонят девя...</td>\n",
              "      <td>Путеводитель по Дню Победы: как провести 9 мая...</td>\n",
              "      <td>облако_NOUN над_ADP москва_NOUN в_ADP день_NOU...</td>\n",
              "      <td>путеводитель_NOUN по_ADP день_NOUN победа_NOUN...</td>\n",
              "      <td>облако москва день победа разогнать девять сам...</td>\n",
              "      <td>путеводитель день победа провести 9 май москва</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7224</th>\n",
              "      <td>-1</td>\n",
              "      <td>Любляна отпразднует День Победы вместе с Москвой</td>\n",
              "      <td>В Москве ограничат движение в связи с Днем Победы</td>\n",
              "      <td>любляна_NOUN отпраздновать_VERB день_NOUN побе...</td>\n",
              "      <td>в_ADP москва_NOUN ограничивать_VERB движение_N...</td>\n",
              "      <td>люблян отпраздновать день победа вместе москва</td>\n",
              "      <td>москва ограничить движение связь днём победа</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7225</th>\n",
              "      <td>-1</td>\n",
              "      <td>Девять самолетов ВВС разгонят облака над Москв...</td>\n",
              "      <td>В Москве ограничат движение в связи с Днем Победы</td>\n",
              "      <td>девять_NUM самолет_NOUN ввс_NOUN разгонять_VER...</td>\n",
              "      <td>в_ADP москва_NOUN ограничивать_VERB движение_N...</td>\n",
              "      <td>девять самолёт ввс разогнать облако москва ден...</td>\n",
              "      <td>москва ограничить движение связь днём победа</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7226</th>\n",
              "      <td>-1</td>\n",
              "      <td>9 мая метрополитен Петербурга будет работать к...</td>\n",
              "      <td>Мартынов: комендантский час в Донецке 9 мая бу...</td>\n",
              "      <td>май_NOUN метрополитен_NOUN петербург_NOUN быть...</td>\n",
              "      <td>мартынов_NOUN комендантский_ADJ час_NOUN в_ADP...</td>\n",
              "      <td>9 май метрополитен петербург работать круглосу...</td>\n",
              "      <td>мартынов комендантский час донецк 9 май отменный</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7227 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      label  ...                                         text_2_nor\n",
              "0         0  ...  полиция мочь разрешить стрелять хулиган травма...\n",
              "1         0  ...  правило внесудебный проникновение полицейский ...\n",
              "2         0  ...  власть египет угрожать ввести страна чрезвычай...\n",
              "3        -1  ...      самолёт мчс вывезти россиянин разрушить сирия\n",
              "4         0  ...      самолёт мчс вывезти россиянин разрушить сирия\n",
              "...     ...  ...                                                ...\n",
              "7222      0  ...      путин снять должность 20 руководитель-силовик\n",
              "7223     -1  ...     путеводитель день победа провести 9 май москва\n",
              "7224     -1  ...       москва ограничить движение связь днём победа\n",
              "7225     -1  ...       москва ограничить движение связь днём победа\n",
              "7226     -1  ...   мартынов комендантский час донецк 9 май отменный\n",
              "\n",
              "[7227 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 373
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgjk06F_rWUA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "d651aa1f-d6a3-41ac-a58f-87d97c6ca43c"
      },
      "source": [
        "#test\n",
        "\n",
        "X1_w2v = np.zeros((len(vectors_df['text_1_nor']), dim))\n",
        "X1_fasttext = np.zeros((len(vectors_df['text_1_nor']), dim))\n",
        "X1_nmf = np.zeros((len(vectors_df['text_1_nor']), dim))\n",
        "X1_svd = np.zeros((len(vectors_df['text_1_nor']), dim))\n",
        "X1_rusvect = np.zeros((len(vectors_df['text_1_nor']), dim))\n",
        "\n",
        "X2_w2v = np.zeros((len(vectors_df['text_2_nor']), dim))\n",
        "X2_fasttext = np.zeros((len(vectors_df['text_2_nor']), dim))\n",
        "X2_nmf = np.zeros((len(vectors_df['text_2_nor']), dim))\n",
        "X2_svd = np.zeros((len(vectors_df['text_2_nor']), dim))\n",
        "X2_rusvect = np.zeros((len(vectors_df['text_2_nor']), dim))\n",
        "\n",
        "for i, text in enumerate(vectors_df['text_1_nor'].values):\n",
        "    X1_w2v[i] = get_df_embedding(text, my_w2v, dim)\n",
        "    X1_fasttext[i] = get_df_embedding(text, fast_text, dim)\n",
        "    X1_nmf[i] = get_df_embedding(text, fast_text, dim)\n",
        "    X1_svd[i] = get_df_embedding(text, fast_text, dim)\n",
        "    X1_rusvect[i] = get_df_embedding(text, rusv_v2w, dim)\n",
        "#for i, text in enumerate(data['text_2_notnorm'][i].values):\n",
        "    X2_w2v[i] = get_df_embedding(vectors_df['text_2_nor'][i], my_w2v, dim)\n",
        "    X2_fasttext[i] = get_df_embedding(vectors_df['text_2_nor'][i], fast_text, dim)\n",
        "    X2_nmf[i] = get_df_embedding(vectors_df['text_2_nor'][i], fast_text, dim)\n",
        "    X2_svd[i] = get_df_embedding(vectors_df['text_2_nor'][i], fast_text, dim)\n",
        "    X2_rusvect[i] = get_df_embedding(text, rusv_v2w, dim)\n",
        "    distance_w2v = cosine_distances(X1_w2v[i], X2_w2v[i])\n",
        "    vectors_df['distance_fasttext']= cosine_distances(X1_fast_text[i], X2_fast_text[i])\n",
        "    vectors_df['distance_w2v'] = cosine_distances(X1_w2v[i], X2_w2v[i])\n",
        "    vectors_df['distance_nmf'] = cosine_distances(X1_nmf[i], X2_nmf[i])\n",
        "    vectors_df['distance_svd'] = cosine_distances(X1_svd[i], X2_svd[i])\n",
        "    vectors_df['rusvect_w2v'] = cosine_distances(X1_rusvect[i], X2_rusvect[i])\n",
        "\n",
        "# dist_dict.append(distance)\n",
        "# return dist_dict\n"
      ],
      "execution_count": 379,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-379-18df20ed4976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mX2_svd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_df_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_2_nor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfast_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mX2_rusvect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_df_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrusv_v2w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdistance_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_w2v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2_w2v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mvectors_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'distance_fasttext'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcosine_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_fast_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2_fast_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mvectors_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'distance_w2v'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_w2v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2_w2v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_distances\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m    819\u001b[0m     \"\"\"\n\u001b[1;32m    820\u001b[0m     \u001b[0;31m# 1.0 - cosine_similarity(X, Y) without copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m     \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m     \u001b[0mS\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0mS\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    139\u001b[0m         X = check_array(X, accept_sparse=accept_sparse, dtype=dtype,\n\u001b[1;32m    140\u001b[0m                         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                         estimator=estimator)\n\u001b[0m\u001b[1;32m    142\u001b[0m         Y = check_array(Y, accept_sparse=accept_sparse, dtype=dtype,\n\u001b[1;32m    143\u001b[0m                         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    554\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[-0.05650598 -0.06286707  0.06896559 -0.01425251 -0.01034439  0.00731768\n -0.00897308 -0.02476926 -0.02278018  0.04905628  0.04233099 -0.01686845\n -0.02122421  0.00586369 -0.03738403 -0.03233989 -0.01083631 -0.0572606\n  0.01871627  0.01241988  0.01558112 -0.00391045  0.00868186  0.02543399\n -0.04383875 -0.02671832 -0.03554334  0.04712019 -0.04386362  0.02496025\n -0.01480816  0.02625768 -0.02053426  0.0305965  -0.02147113 -0.01864921\n  0.01304697  0.01314984  0.01723633 -0.04267887  0.04977615 -0.03169131\n -0.01109961 -0.00446008 -0.04502319  0.01661364 -0.039266   -0.01358543\n -0.03958381  0.01270581].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20L4yo5ExNgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dim = 50\n",
        "cosine_dist = []\n",
        "vectors_df['text_1_notnorm'] = vectors_df['text_1'].apply(tokenize)\n",
        "vectors_df['text_2_notnorm'] = vectors_df['text_2'].apply(tokenize)\n",
        "\n",
        "X_text_1_ft = np.zeros((len(vectors_df['text_1_norm']), 300))\n",
        "X_text_2_ft = np.zeros((len(vectors_df['text_2_norm']), 300))\n",
        "\n",
        "for i, text in enumerate(vectors_df['text_1_norm'].values):\n",
        "    X_text_1_ft[i] = get_df_embedding(text, rusv_v2w, 300)\n",
        "    \n",
        "for i, text in enumerate(vectors_df['text_2_norm'].values):\n",
        "    X_text_2_ft[i] = get_df_embedding(text, rusv_v2w, 300)\n",
        "    #print(X_text_2_ft[i])\n",
        "cosine_distances()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5gFg4XG0BES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_text_ft = np.concatenate([X_text_1_ft, X_text_2_ft], axis=1)\n",
        "X_text_ft\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}