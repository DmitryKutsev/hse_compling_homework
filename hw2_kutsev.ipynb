{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Задание 1. (5 баллов) \n",
    "# # В тетрадке реализована биграмная языковая модель (при генерации учитывается информация только о 1\n",
    "# предыдущем слове). Реализуйте триграмную модель и сгенерируйте несколько текстов. Сравните их с текстами,\n",
    "# сгенерированными биграмной моделью. \n",
    "# # Можно использовать те же тексты, что в семинаре, или взять какой-то другой (на английском или русском языке)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dostoevsky = open('besy_dostoevsky.txt', encoding='cp1251').read()\n",
    "tolstoy = open('anna_karenina.txt', encoding='utf-8').read()\n",
    "dostoevsky = dostoevsky[:10000]\n",
    "tolstoy = tolstoy[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import numpy as np\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = [word.strip(punctuation) for word in text.lower().split()]\n",
    "    normalized_text = [word for word in normalized_text if word]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dostoevsky = normalize(dostoevsky)\n",
    "norm_tolstoy = normalize(tolstoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['спасибо', 'что', 'скачали', 'книгу', 'в', 'бесплатной', 'электронной', 'библиотеке', 'royallib.ru', 'http://royallib.ru']\n",
      "1507\n",
      "['annotation', '«анна', 'каренина»', '–', 'это', 'сложное', 'психологически', 'утонченное', 'остропроблемное', 'произведение']\n",
      "1587\n"
     ]
    }
   ],
   "source": [
    "print(norm_dostoevsky[:10])\n",
    "print(len(norm_dostoevsky))\n",
    "print(norm_tolstoy[:10])\n",
    "print(len(norm_tolstoy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уникальных лемм в срезе Бесов - 836\n",
      "Уникальный лемм в срезе Анны Карениной -  817\n"
     ]
    }
   ],
   "source": [
    "print(\"Уникальных лемм в срезе Бесов -\", len(set(norm_dostoevsky)))\n",
    "print(\"Уникальный лемм в срезе Анны Карениной - \", len(set(norm_tolstoy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dostoevsky = Counter(norm_dostoevsky)\n",
    "vocab_tolstoy = Counter(norm_tolstoy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 83),\n",
       " ('в', 57),\n",
       " ('что', 30),\n",
       " ('не', 28),\n",
       " ('его', 19),\n",
       " ('но', 19),\n",
       " ('он', 16),\n",
       " ('же', 15),\n",
       " ('с', 15),\n",
       " ('а', 11)]"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dostoevsky.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 70),\n",
       " ('–', 52),\n",
       " ('в', 47),\n",
       " ('не', 36),\n",
       " ('он', 36),\n",
       " ('что', 27),\n",
       " ('на', 25),\n",
       " ('с', 25),\n",
       " ('как', 22),\n",
       " ('матвей', 13)]"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_tolstoy.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 0.055076310550763105),\n",
       " ('в', 0.0378234903782349),\n",
       " ('что', 0.019907100199071003),\n",
       " ('не', 0.0185799601857996),\n",
       " ('его', 0.012607830126078301),\n",
       " ('но', 0.012607830126078301),\n",
       " ('он', 0.010617120106171201),\n",
       " ('же', 0.009953550099535502),\n",
       " ('с', 0.009953550099535502),\n",
       " ('а', 0.0072992700729927005)]"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas_dosoevsky = Counter({word:c/len(norm_dostoevsky) for word, c in vocab_dostoevsky.items()})\n",
    "probas_dosoevsky.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 0.04410838059231254),\n",
       " ('–', 0.03276622558286074),\n",
       " ('в', 0.029615626969124134),\n",
       " ('не', 0.022684310018903593),\n",
       " ('он', 0.022684310018903593),\n",
       " ('что', 0.017013232514177693),\n",
       " ('на', 0.01575299306868305),\n",
       " ('с', 0.01575299306868305),\n",
       " ('как', 0.013862633900441084),\n",
       " ('матвей', 0.008191556395715185)]"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas_tolstoy = Counter({word:c/len(norm_tolstoy) for word, c in vocab_tolstoy.items()})\n",
    "probas_tolstoy.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tolstoy', -28.392685814933895), ('dostoevsky', -43.52203961189503)]"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = 'Все смешалось в доме облонских'\n",
    "\n",
    "prob = Counter({'tolstoy':0, 'dostoevsky':0})\n",
    "\n",
    "for word in normalize(phrase):\n",
    "    prob['dostoevsky'] += np.log(probas_dosoevsky.get(word, 0.00001))\n",
    "    prob['tolstoy'] += np.log(probas_tolstoy.get(word, 0.00001))\n",
    "\n",
    "prob.most_common()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dostoevsky = [['<start>'] + ['<start>'] + normalize(text) + \\\n",
    "                        ['<end>'] for text in sent_tokenize(dostoevsky)]\n",
    "sentences_tolstoy = [['<start>'] + ['<start>'] + normalize(text) + \\\n",
    "                     ['<end>'] for text in sent_tokenize(tolstoy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_dostoevsky = Counter()\n",
    "bigrams_dostoevsky = Counter()\n",
    "trigrams_dostoevsky = Counter()\n",
    "\n",
    "for sentence in sentences_dostoevsky:\n",
    "    unigrams_dostoevsky.update(sentence)\n",
    "    bigrams_dostoevsky.update(ngrammer(sentence))\n",
    "for sentence in sentences_dostoevsky:\n",
    "    trigrams_dostoevsky.update(ngrammer(sentence, n=3))\n",
    "\n",
    "\n",
    "\n",
    "unigrams_tolstoy = Counter()\n",
    "bigrams_tolstoy = Counter()\n",
    "trigrams_tolstoy = Counter()\n",
    "\n",
    "for sentence in sentences_tolstoy:\n",
    "    unigrams_tolstoy.update(sentence)\n",
    "    bigrams_tolstoy.update(ngrammer(sentence))\n",
    "for sentence in sentences_tolstoy:\n",
    "    trigrams_tolstoy.update(ngrammer(sentence, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_matrix_dostoevsky = np.zeros((len(unigrams_dostoevsky), \n",
    "                   len(unigrams_dostoevsky)))\n",
    "id2word_dostoevsky = list(unigrams_dostoevsky)\n",
    "word2id_dostoevsky = {word:i for i, word in enumerate(id2word_dostoevsky)}\n",
    "\n",
    "\n",
    "for ngram in bigrams_dostoevsky:\n",
    "    word1, word2 = ngram.split()\n",
    "    bi_matrix_dostoevsky[word2id_dostoevsky[word1]][word2id_dostoevsky[word2]] =  (bigrams_dostoevsky[ngram]/\n",
    "                                                                     unigrams_dostoevsky[word1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tri_matrix_dostoevsky = np.zeros((len(bigrams_dostoevsky), \n",
    "                   len(unigrams_dostoevsky)))\n",
    "bi_id2word_dostoevsky = list(bigrams_dostoevsky)\n",
    "bi_word2id_dostoevsky = {word:i for i, word in enumerate(bi_id2word_dostoevsky)}\n",
    "\n",
    "\n",
    "for ngram in trigrams_dostoevsky:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    tri_matrix_dostoevsky[bi_word2id_dostoevsky[word1 + ' ' + word2]][word2id_dostoevsky[word3]] \\\n",
    "        = (trigrams_dostoevsky[ngram]/ bigrams_dostoevsky[word1 + ' ' + word2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_matrix_tolstoy = np.zeros((len(unigrams_tolstoy), \n",
    "                   len(unigrams_tolstoy)))\n",
    "id2word_tolstoy = list(unigrams_tolstoy)\n",
    "word2id_tolstoy = {word:i for i, word in enumerate(id2word_tolstoy)}\n",
    "\n",
    "\n",
    "for ngram in bigrams_tolstoy:\n",
    "    word1, word2 = ngram.split()\n",
    "    bi_matrix_tolstoy[word2id_tolstoy[word1]][word2id_tolstoy[word2]] =  (bigrams_tolstoy[ngram]/\n",
    "                                                                     unigrams_tolstoy[word1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_matrix_tolstoy = np.zeros((len(bigrams_tolstoy), \n",
    "                   len(unigrams_tolstoy)))\n",
    "bi_id2word_tolstoy = list(bigrams_tolstoy)\n",
    "bi_word2id_tolstoy = {word:i for i, word in enumerate(bi_id2word_tolstoy)}\n",
    "\n",
    "\n",
    "for ngram in trigrams_tolstoy:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    tri_matrix_tolstoy[bi_word2id_tolstoy[word1 + ' ' + word2]][word2id_tolstoy[word3]] = \\\n",
    "    (trigrams_tolstoy[ngram]/bigrams_tolstoy[word1 + ' ' + word2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word, word2id, n=100, start='<start>'):\n",
    "    text = []\n",
    "    current_idx = word2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx])\n",
    "        text.append(id2word[chosen])\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            chosen = word2id['<start>']\n",
    "        current_idx = chosen\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tri_generate(matrix, uni_id2word, uni_word2id, bi_id2word, bi_word2id,  n=100):\n",
    "    text = []\n",
    "    start_bi = \"<start> <start>\"\n",
    "    current_idx = bi_word2id[start_bi]\n",
    "    text.append(start_bi)\n",
    "    for i in range(n):\n",
    "        try:\n",
    "            chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx])\n",
    "            text.append(uni_id2word[chosen])        \n",
    "            if uni_id2word[chosen] == '<end>':\n",
    "                chosen = np.random.choice(matrix.shape[1],\n",
    "                            p=matrix[bi_word2id[start_bi]])\n",
    "                current_idx = chosen\n",
    "            else:\n",
    "                shift = bi_id2word[current_idx].split(\" \")[-1] + \" \" + uni_id2word[chosen]\n",
    "                current_idx = bi_word2id[shift]\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    все эти хоры поют о чем-то один минерал то есть вернее в тридцатых годах в этом роде часто пописывали \n",
      " так даже что мне кажется без нее и прожить не мог \n",
      " верховенского i приступая к описанию недавних и столь странных событий происшедших в нашем доселе ничем не отличавшемся городе я принужден по неумению моему начать несколько издалека а именно некоторыми биографическими подробностями о талантливом и многочтимом степане трофимовиче верховенском \n",
      " \n",
      " что скачали книгу в бесплатной электронной библиотеке royallib.ru http://royallib.ru все книги автора http://royallib.ru/author/dostoevskiy_fedor.html эта же книга в других форматах http://royallib.ru/book/dostoevskiy_fedor/besi.html приятного чтения \n",
      " никак ведь нельзя сказать что\n"
     ]
    }
   ],
   "source": [
    "print(tri_generate(tri_matrix_dostoevsky, id2word_dostoevsky, word2id_dostoevsky, bi_id2word_dostoevsky, \\\n",
    "                   bi_word2id_dostoevsky).replace('<end>', '\\n').replace('<start>', ' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "уверь кто-нибудь тогда честнейшего степана трофимовича с детских лет довел его местом тотчас же после потери кафедры — так думаю что продолжение столь странных событий происшедших в великолепном красном сафьянном переплете \n",
      "   но даже науки у одного студента \n",
      " в нем не в последние двадцать лет довел его уважаю \n",
      "     но не оказалось по крайней мере в некоторых сферах его постоянно опасаются что начинавшего тогда опасною \n",
      " ну вот эту-то поэму там то есть предмет уже вовсе не знали \n",
      "   видевшие же \n",
      "   я предлагал напечатать за совершенною ее напечатать здесь —\n"
     ]
    }
   ],
   "source": [
    "print(generate(bi_matrix_dostoevsky, id2word_dostoevsky, word2id_dostoevsky).\n",
    "      replace('<end>', '\\n').replace('<start> <start>', ' ').replace('<start>', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    потом добрая и несколько жалкая улыбка показалась на его красивом лице \n",
      " кучер просили расчета \n",
      " третий день не было кроме того общего ответа который дает жизнь на все самые сложные и неразрешимые вопросы \n",
      " в россии под натиском буржуазного прогресса как падают нравы ослабевают семейные устои вырождается аристократия \n",
      " librs.net благодарим вас за использование нашей библиотеки librs.net \n",
      " из своих комнат мужа третий день после ссоры князь степан аркадьич был человек правдивый в отношении к себе самому \n",
      " это сложное психологически утонченное остропроблемное произведение насыщенное приметами времени \n",
      " не il mio tesoro и не может простить \n",
      " всему дому\n"
     ]
    }
   ],
   "source": [
    "print(tri_generate(tri_matrix_tolstoy, id2word_tolstoy, word2id_tolstoy, bi_id2word_tolstoy,\\\n",
    "                   bi_word2id_tolstoy).replace('<end>', '\\n').replace('<start>', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "в спальне у нас в обычный час то воскресенье а что-то лучше скрыть от жены в том что он прочел ее и объявила мужу что они встретились в карманы и ее и подождав немного прибавил с отчаянием вспоминая самые сложные и только в зеркале видно будет – что это \n",
      " ааа!..» – с ним толстой часть седьмая часть первая минута когда матвей \n",
      " но вдруг вскочил сел на стеклянных столах да но ведь пока она уже… надо же как хорошо жили \n",
      "    «да хорошо все запиской в америке \n",
      "   – спрашивала она хотела \n",
      " <start>\n"
     ]
    }
   ],
   "source": [
    "print(generate(bi_matrix_tolstoy, id2word_tolstoy, word2id_tolstoy).\n",
    "      replace('<end>', '\\n').replace('<start> <start>', ' ').replace('<start> ', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Задание 2. (5 баллов) \n",
    "# При помощи gensim.models.Phrases реализуйте byte-pair-encoding,\n",
    "# про который говорилось на первом семинаре \n",
    "# (https://github.com/mannefedov/compling_nlp_hse_course/blob/master/notebooks/Preprocessing.ipynb) \n",
    "# А именно 1) возьмите любой текст; разбейте его на предложения, а каждое предложение разбейте на \n",
    "# отдельные символы (не потеряйте пробелы) 2) обучите gensim.models.Phrases на полученных\n",
    "# символьных предложениях 3) примените полученный нграммер к этим символьным предложениям \n",
    "# 4) повторите 2 и 3 N количество раз, чтобы начали получаться целые слова\n",
    "# Параметры в gensim.models.Phrases влияют на количество получаемых нграммов после каждого \n",
    "# прохода, поэтому не забудьте их настроить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.summarization.textcleaner import split_sentences\n",
    "from gensim.models.phrases import Phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['С', 'п', 'а', 'с', 'и', 'б', 'о', ',', ' ', 'ч']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens  = sent_tokenize(dostoevsky, 'russian')[:10]\n",
    "symbol_tokens = [list(i) for i in sent_tokens]\n",
    "symbol_tokens[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = Phrases(symbol_tokens, scoring='npmi', min_count=1, threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Phrases(p[symbol_tokens], scoring='npmi', min_count=1, threshold=0.1 )\n",
    "p2 = Phrases(p1[p[symbol_tokens]], scoring='npmi', min_count=1, threshold=-1 )\n",
    "p3 = Phrases(p2[p1[p[symbol_tokens]]], scoring='npmi', min_count=1, threshold=-1 )\n",
    "p4 = Phrases(p3[p2[p1[p[symbol_tokens]]]], scoring='npmi', min_count=1, threshold=-1 )\n",
    "p5 = Phrases(p4[p3[p2[p1[p[symbol_tokens]]]]], scoring='npmi', min_count=1, threshold=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['С_п', 'а_с', 'и_б', 'о_,', ' _ч', 'т_о', ' _с', 'к_а', 'ч_а', 'л_и', ' _к', 'н_и', 'г_у', ' _в', ' _б', 'е_с', 'п_л', 'а_т', 'н_о', 'й_ ', 'э_л', 'е_к', 'т_р', 'о_н', 'н_о', 'й_ ', 'б_и', 'б_л', 'и', 'о_т', 'е_к', 'е_ ', 'R_o', 'y_a', 'l_l', 'i_b', '._r', 'u_:', ' _h', 't_t', 'p_:', '/_/', 'r_o', 'y_a', 'l_l', 'i_b', '._r', 'u_\\n', '\\n_В', 'с', 'е_ ', 'к_н', 'и_г', 'и_ ', 'а_в', 'т_о', 'р_а', ':_ ', 'h_t', 't_p', ':_/', '/_r', 'o_y', 'a_l', 'l_i', 'b_.', 'r_u', '/_a', 'u_t', 'h_o', 'r_/', 'd_o', 's_t', 'o_e', 'v_s', 'k_i', 'y__', 'f_e', 'd_o', 'r_.', 'h_t', 'm_l', '\\n_\\n', 'Э_т', 'а_ ', 'ж_е', ' _к', 'н_и', 'г_а', ' _в', ' _д', 'р_у', 'г_и', 'х_ ', 'ф_о', 'р_м', 'а_т', 'а_х', ':_ ', 'h_t', 't_p', ':_/', '/_r', 'o_y', 'a_l', 'l_i', 'b_.', 'r_u', '/_b', 'o_o', 'k_/', 'd_o', 's_t', 'o_e', 'v_s', 'k_i', 'y__', 'f_e', 'd_o', 'r_/', 'b_e', 's_i', '._h', 't_m', 'l_\\n', '\\n_П', 'р_и', 'я_т', 'н_о', 'г_о', ' _ч', 'т_е', 'н_и', 'я_!']] \n",
      "\n",
      "\n",
      "\n",
      "[['С_п_а_с', 'и_б_о_,', ' _ч_т_о', ' _с_к_а', 'ч_а_л_и', ' _к_н_и', 'г_у_ _в', ' _б_е_с', 'п_л_а_т', 'н_о_й_ ', 'э_л_е_к', 'т_р_о_н', 'н_о_й_ ', 'б_и_б_л', 'и_о_т', 'е_к_е_ ', 'R_o_y_a', 'l_l_i_b', '._r_u_:', ' _h_t_t', 'p_:_/_/', 'r_o_y_a', 'l_l_i_b', '._r_u_\\n', '\\n_В_с', 'е_ _к_н', 'и_г_и_ ', 'а_в_т_о', 'р_а_:_ ', 'h_t_t_p', ':_/_/_r', 'o_y_a_l', 'l_i_b_.', 'r_u_/_a', 'u_t_h_o', 'r_/_d_o', 's_t_o_e', 'v_s_k_i', 'y___f_e', 'd_o_r_.', 'h_t_m_l', '\\n_\\n_Э_т', 'а_ _ж_е', ' _к_н_и', 'г_а_ _в', ' _д_р_у', 'г_и_х_ ', 'ф_о_р_м', 'а_т_а_х', ':_ _h_t', 't_p_:_/', '/_r_o_y', 'a_l_l_i', 'b_._r_u', '/_b_o_o', 'k_/_d_o', 's_t_o_e', 'v_s_k_i', 'y___f_e', 'd_o_r_/', 'b_e_s_i', '._h_t_m', 'l_\\n_\\n_П', 'р_и_я_т', 'н_о_г_о', ' _ч_т_е', 'н_и_я_!']] \n",
      "\n",
      "\n",
      "\n",
      "[['С_п_а_с_и_б_о_,_ _ч_т_о_ _с_к_а', 'ч_а_л_и_ _к_н_и_г_у_ _в_ _б_е_с', 'п_л_а_т_н_о_й_ _э_л_е_к_т_р_о_н', 'н_о_й_ _б_и_б_л_и_о_т_е_к_е_ ', 'R_o_y_a_l_l_i_b_._r_u_:_ _h_t_t', 'p_:_/_/_r_o_y_a_l_l_i_b_._r_u_\\n', '\\n_В_с_е_ _к_н_и_г_и_ _а_в_т_о', 'р_а_:_ _h_t_t_p_:_/_/_r_o_y_a_l', 'l_i_b_._r_u_/_a_u_t_h_o_r_/_d_o', 's_t_o_e_v_s_k_i_y___f_e_d_o_r_.', 'h_t_m_l_\\n_\\n_Э_т_а_ _ж_е_ _к_н_и', 'г_а_ _в_ _д_р_у_г_и_х_ _ф_о_р_м', 'а_т_а_х_:_ _h_t_t_p_:_/_/_r_o_y', 'a_l_l_i_b_._r_u_/_b_o_o_k_/_d_o', 's_t_o_e_v_s_k_i_y___f_e_d_o_r_/', 'b_e_s_i_._h_t_m_l_\\n_\\n_П_р_и_я_т', 'н_о_г_о_ _ч_т_е_н_и_я_!']] \n",
      "\n",
      "\n",
      "\n",
      "[['С_п_а_с_и_б_о_,_ _ч_т_о_ _с_к_а', 'ч_а_л_и_ _к_н_и_г_у_ _в_ _б_е_с', 'п_л_а_т_н_о_й_ _э_л_е_к_т_р_о_н', 'н_о_й_ _б_и_б_л_и_о_т_е_к_е_ ', 'R_o_y_a_l_l_i_b_._r_u_:_ _h_t_t', 'p_:_/_/_r_o_y_a_l_l_i_b_._r_u_\\n', '\\n_В_с_е_ _к_н_и_г_и_ _а_в_т_о', 'р_а_:_ _h_t_t_p_:_/_/_r_o_y_a_l', 'l_i_b_._r_u_/_a_u_t_h_o_r_/_d_o', 's_t_o_e_v_s_k_i_y___f_e_d_o_r_.', 'h_t_m_l_\\n_\\n_Э_т_а_ _ж_е_ _к_н_и', 'г_а_ _в_ _д_р_у_г_и_х_ _ф_о_р_м', 'а_т_а_х_:_ _h_t_t_p_:_/_/_r_o_y', 'a_l_l_i_b_._r_u_/_b_o_o_k_/_d_o', 's_t_o_e_v_s_k_i_y___f_e_d_o_r_/', 'b_e_s_i_._h_t_m_l_\\n_\\n_П_р_и_я_т', 'н_о_г_о_ _ч_т_е_н_и_я_!']] \n",
      "\n",
      "\n",
      "\n",
      "[['С_п_а_с_и_б_о_,_ _ч_т_о_ _с_к_а_ч_а_л_и_ _к_н_и_г_у_ _в_ _б_е_с', 'п_л_а_т_н_о_й_ _э_л_е_к_т_р_о_н_н_о_й_ _б_и_б_л_и_о_т_е_к_е_ ', 'R_o_y_a_l_l_i_b_._r_u_:_ _h_t_t_p_:_/_/_r_o_y_a_l_l_i_b_._r_u_\\n', '\\n_В_с_е_ _к_н_и_г_и_ _а_в_т_о_р_а_:_ _h_t_t_p_:_/_/_r_o_y_a_l', 'l_i_b_._r_u_/_a_u_t_h_o_r_/_d_o_s_t_o_e_v_s_k_i_y___f_e_d_o_r_.', 'h_t_m_l_\\n_\\n_Э_т_а_ _ж_е_ _к_н_и_г_а_ _в_ _д_р_у_г_и_х_ _ф_о_р_м', 'а_т_а_х_:_ _h_t_t_p_:_/_/_r_o_y_a_l_l_i_b_._r_u_/_b_o_o_k_/_d_o', 's_t_o_e_v_s_k_i_y___f_e_d_o_r_/_b_e_s_i_._h_t_m_l_\\n_\\n_П_р_и_я_т', 'н_о_г_о_ _ч_т_е_н_и_я_!']] \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(list(p[symbol_tokens][:1]), '\\n\\n\\n')\n",
    "print(list(p1[p[symbol_tokens]][:1]), '\\n\\n\\n')\n",
    "print(list(p3[p2[p1[p[symbol_tokens]]]][:1]), '\\n\\n\\n')\n",
    "print(list(p3[p2[p1[p[symbol_tokens]]]][:1]), '\\n\\n\\n' )\n",
    "print(list(p4[p3[p2[p1[p[symbol_tokens]]]]][:1]),'\\n\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
