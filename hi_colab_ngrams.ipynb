{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "hi_ngrams.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CBypkkcrcB2",
        "colab_type": "text"
      },
      "source": [
        "## Языковое моделирование"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo7tm8KgrcB4",
        "colab_type": "text"
      },
      "source": [
        "Языковое моделирование заключается в приписывании вероятности последовательности слов. Сейчас языковые модели используются практически во всех nlp задачах. Всякие Берты и Элмо - языковые модели. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYkwgolKrcB6",
        "colab_type": "text"
      },
      "source": [
        "Это достаточно сложная тема, поэтому будем разбирать постепенно. Сегодня разберём самые основы. Научимся приписывать вероятность последовательности слов и попробуем генерировать текст."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeOWfJjOrcB8",
        "colab_type": "text"
      },
      "source": [
        "Возьмем два текста: Анну Каренину и Бесов. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlNHe6uyrsPy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "64991cec-e431-462a-ec01-eb89ecdd7809"
      },
      "source": [
        "#https://colab.research.google.com/\\\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/anna_karenina.txt\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/besy_dostoevsky.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 3001k  100 3001k    0     0  4771k      0 --:--:-- --:--:-- --:--:-- 4771k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2279k  100 2279k    0     0  9659k      0 --:--:-- --:--:-- --:--:-- 9659k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8duVLUBrcB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dostoevsky = open('besy_dostoevsky.txt').read()\n",
        "tolstoy = open('anna_karenina.txt', encoding='utf-8').read()\n",
        "# print(dostoevsky[:20])\n",
        "# print(tolstoy[:20])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMGhXX6PrcCE",
        "colab_type": "text"
      },
      "source": [
        "Анна Каренина немного больше."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EpkuilprcCF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6d160c35-f56c-4547-b161-ccc83016eb2f"
      },
      "source": [
        "print(\"Длина Бесов Достоевского -\", len(dostoevsky))\n",
        "print(\"Длина Анны Карениной Толстого - \", len(tolstoy))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Длина Бесов Достоевского - 1293557\n",
            "Длина Анны Карениной Толстого -  1710408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJcqDFFvrcCL",
        "colab_type": "text"
      },
      "source": [
        "Напишем простую функцию для нормализации. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McH3cYO1rcCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from string import punctuation\n",
        "import numpy as np\n",
        "\n",
        "def normalize(text):\n",
        "    normalized_text = [word.strip(punctuation) for word \\\n",
        "                                                            in text.lower().split()]\n",
        "    normalized_text = [word for word in normalized_text if word]\n",
        "    return normalized_text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FO1baixrcCQ",
        "colab_type": "text"
      },
      "source": [
        "Сравним тексты по словам"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-6_JKN7rcCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "norm_dostoevsky = normalize(dostoevsky)\n",
        "norm_tolstoy = normalize(tolstoy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvMvbhKHrcCU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8de071c0-b9e0-469a-e3c2-0f6714bc66a7"
      },
      "source": [
        "print(\"Длина Бесов Достоевского в токенах -\", len(norm_dostoevsky))\n",
        "print(\"Длина Анны Карениной Толстого в токенах - \", len(norm_tolstoy))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Длина Бесов Достоевского в токенах - 208453\n",
            "Длина Анны Карениной Толстого в токенах -  281201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F34U1pgzrcCX",
        "colab_type": "text"
      },
      "source": [
        "Бесы короче, но уникальных слов там больше!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRfv2mS1rcCY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5ce04cfd-5f96-4b9f-8c79-a43de0d6eb9d"
      },
      "source": [
        "print(\"Уникальных лемм в Бесах -\", len(set(norm_dostoevsky)))\n",
        "print(\"Уникальный лемм в Анне Карениной - \", len(set(norm_tolstoy)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Уникальных лемм в Бесах - 32547\n",
            "Уникальный лемм в Анне Карениной -  34820\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V4fvE0QrcCb",
        "colab_type": "text"
      },
      "source": [
        "Посчитаем, сколько раз встречаются слова и выведем самые частотные."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdLOPTkFrcCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdxBDHSXrcCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_dostoevsky = Counter(norm_dostoevsky)\n",
        "vocab_tolstoy = Counter(norm_tolstoy)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiKHyVSGrcCh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "93fdeb2b-6fa8-427c-c639-12ced8731bf7"
      },
      "source": [
        "vocab_dostoevsky.most_common(10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('и', 8599),\n",
              " ('—', 7227),\n",
              " ('в', 4734),\n",
              " ('не', 4707),\n",
              " ('что', 3547),\n",
              " ('я', 3377),\n",
              " ('он', 2489),\n",
              " ('с', 2419),\n",
              " ('на', 2359),\n",
              " ('но', 1833)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NNTIDjYrcCk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "1656aaa0-2d47-4fe8-ffae-6eced09d8179"
      },
      "source": [
        "vocab_tolstoy.most_common(10)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('и', 12885),\n",
              " ('–', 11490),\n",
              " ('не', 6517),\n",
              " ('что', 5721),\n",
              " ('в', 5717),\n",
              " ('он', 5531),\n",
              " ('на', 3594),\n",
              " ('она', 3418),\n",
              " ('с', 3324),\n",
              " ('я', 3147)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9vC7LFFuxbD",
        "colab_type": "text"
      },
      "source": [
        "-------------------------------------------------------------\n",
        "*Небольшая вставка - что такое Python Counter:\n",
        "\n",
        "A Counter is a container that keeps track of how many times equivalent values are added. It can be used to implement the same algorithms for which bag or multiset data structures are commonly used in other languages.\n",
        "\n",
        "Initializing\n",
        "Counter supports three forms of initialization. Its constructor can be called with a sequence of items, a dictionary containing keys and counts, or using keyword arguments mapping string names to counts.\n",
        "\n",
        "\n",
        "https://pymotw.com/2/collections/counter.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW19TpRovQAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8a08bda7-56e4-4630-bf86-77e7cd0bd094"
      },
      "source": [
        "import collections\n",
        "\n",
        "print(collections.Counter(['a', 'b', 'c', 'a', 'b', 'b']))\n",
        "print(collections.Counter({'a':2, 'b':3, 'c':1}))\n",
        "print(collections.Counter(a=2, b=3, c=1))\n",
        "#The results of all three forms of initialization are the same."
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'b': 3, 'a': 2, 'c': 1})\n",
            "Counter({'b': 3, 'a': 2, 'c': 1})\n",
            "Counter({'b': 3, 'a': 2, 'c': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SRxlGx0vteo",
        "colab_type": "text"
      },
      "source": [
        "Сравнивать употребимость конкретных слов в разных текстах в абсолютных числах неудобно. Нормализуем счётчики на размеры текстов. Так у нас получается вероятность слова."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Hdr3haMrcCo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "020d8cd3-65f2-40de-8594-b04ba14a0ce1"
      },
      "source": [
        "probas_dosoevsky = Counter({word:c/len(norm_dostoevsky) for word, c in vocab_dostoevsky.items()})\n",
        "probas_dosoevsky.most_common(20)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('и', 0.0412515051354502),\n",
              " ('—', 0.03466968573251524),\n",
              " ('в', 0.02271015528680326),\n",
              " ('не', 0.022580629686308185),\n",
              " ('что', 0.017015826109482712),\n",
              " ('я', 0.016200294550810016),\n",
              " ('он', 0.011940341467860861),\n",
              " ('с', 0.01160453435546622),\n",
              " ('на', 0.011316699687699385),\n",
              " ('но', 0.008793349100276801),\n",
              " ('вы', 0.008419164032179917),\n",
              " ('а', 0.008160112831189765),\n",
              " ('как', 0.00770917185168839),\n",
              " ('это', 0.006720939492355591),\n",
              " ('же', 0.0060541225120290905),\n",
              " ('его', 0.006025339045252407),\n",
              " ('так', 0.005440075220793176),\n",
              " ('к', 0.005392102776165371),\n",
              " ('всё', 0.004653327128897162),\n",
              " ('она', 0.004566976728567111)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Akz7gM4rcCr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "fb1b8ff8-d6c1-4e48-8508-69d526e64de3"
      },
      "source": [
        "probas_tolstoy = Counter({word:c/len(norm_tolstoy) for word, c in vocab_tolstoy.items()})\n",
        "probas_tolstoy.most_common(20)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('и', 0.04582131642490603),\n",
              " ('–', 0.04086045213210479),\n",
              " ('не', 0.023175593258914443),\n",
              " ('что', 0.020344877863165495),\n",
              " ('в', 0.020330653162684342),\n",
              " ('он', 0.019669204590310845),\n",
              " ('на', 0.012780893382313719),\n",
              " ('она', 0.012155006561143097),\n",
              " ('с', 0.01182072609983606),\n",
              " ('я', 0.011191283103545151),\n",
              " ('как', 0.009395414667799902),\n",
              " ('его', 0.009121589183537754),\n",
              " ('но', 0.009057578031372577),\n",
              " ('это', 0.007848478490474785),\n",
              " ('к', 0.007044782913289782),\n",
              " ('ее', 0.006390446691156859),\n",
              " ('все', 0.005889025999196305),\n",
              " ('было', 0.005871245123594866),\n",
              " ('сказал', 0.005007094569364974),\n",
              " ('так', 0.004985757518643248)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6CIfQckrcCu",
        "colab_type": "text"
      },
      "source": [
        "Эти вероятности уже можно использовать, чтобы ответить на вопрос - кто из авторов сказал бы такую фразу?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TVTaX2KrcCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "phrase = 'Все смешалось в доме облонских'\n",
        "\n",
        "prob = Counter({'tolstoy':0, 'dostoevsky':0})\n",
        "\n",
        "for word in normalize(phrase):\n",
        "    prob['dostoevsky'] += np.log(probas_dosoevsky.get(word, 0.00001))\n",
        "    prob['tolstoy'] += np.log(probas_tolstoy.get(word, 0.00001))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgmNJ5YDwASd",
        "colab_type": "text"
      },
      "source": [
        "-------\n",
        "небольшая вставка про numpy.log\n",
        "\n",
        "The natural logarithm log is the inverse of the exponential function, so that log(exp(x)) = x. The natural logarithm is logarithm in base e.\n",
        "\n",
        "Logarithm is a multivalued function: for each x there is an infinite number of z such that exp(z) = x. The convention is to return the z whose imaginary part lies in [-pi, pi].\n",
        "\n",
        "For real-valued input data types, log always returns real output. For each value that cannot be expressed as a real number or infinity, it yields nan and sets the invalid floating point error flag.\n",
        "\n",
        "For complex-valued input, log is a complex analytical function that has a branch cut [-inf, 0] and is continuous from above on it. log handles the floating-point negative zero as an infinitesimal negative number, conforming to the C99 standard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-arhk_vnrcCy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da4ad6a7-40a9-4d93-e71d-6d97ef9f7f69"
      },
      "source": [
        "prob.most_common()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tolstoy', -39.40673502639174), ('dostoevsky', -40.89446321918831)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV5rSufCrcC1",
        "colab_type": "text"
      },
      "source": [
        "Результаты получаются не очень точные. Возможно это из-за того, что мы считаем слова незовисымыми друг от друга. А это очевидно не так"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUdwdseGrcC2",
        "colab_type": "text"
      },
      "source": [
        "По-хорошему вероятность последовательности нужно расчитывать по формуле полной вероятности. Но у нас не очень большие тексты и мы не можем получить вероятности для длинных фраз (их просто может не быть в текстах). Поэтому мы воспользуемся предположением Маркова и будем учитывать только предыдущее слово."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Msfd21-nrcC3",
        "colab_type": "text"
      },
      "source": [
        "Чтобы расчитать вероятность с таким предположением, нам достаточно найти количество вхождений для каждого биграмма."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekjKyn84rcC4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "38893df4-9d38-434d-b409-b08243a07651"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "def ngrammer(tokens, n=2):\n",
        "    ngrams = []\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams.append(' '.join(tokens[i:i+n]))\n",
        "    return ngrams"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPBVMrnjrcC7",
        "colab_type": "text"
      },
      "source": [
        "Для того, чтобы у нас получились честные вероятности и можно было посчитать вероятность первого слова, нам нужно добавить тэг маркирующий начало предложений \\< start \\>\n",
        "\n",
        "Дальше мы попробуем сгенерировать текст, используя эти вероятности, и нам нужно будет когда-то остановится. Для этого добавим тэг окончания \\< end \\>\n",
        "\n",
        "Ну и поделим все на предложения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn_QWIsbrcC7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f46c211b-ee64-4bca-f680-6bbe37c03b41"
      },
      "source": [
        "sentences_dostoevsky = [['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dostoevsky)]\n",
        "sentences_tolstoy = [['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(tolstoy)]\n",
        "print(len(sentences_dostoevsky))\n",
        "print(len(sentences_tolstoy))\n",
        "sentences_dostoevsky = sentences_dostoevsky[:3000]\n",
        "sentences_tolstoy = sentences_tolstoy[:3000]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15291\n",
            "20178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7ZTVbhzrcDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigrams_dostoevsky = Counter()\n",
        "bigrams_dostoevsky = Counter()\n",
        "\n",
        "for sentence in sentences_dostoevsky:\n",
        "    unigrams_dostoevsky.update(sentence)\n",
        "    bigrams_dostoevsky.update(ngrammer(sentence))\n",
        "\n",
        "\n",
        "unigrams_tolstoy = Counter()\n",
        "bigrams_tolstoy = Counter()\n",
        "\n",
        "for sentence in sentences_tolstoy:\n",
        "    unigrams_tolstoy.update(sentence)\n",
        "    bigrams_tolstoy.update(ngrammer(sentence))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSm04buqrcDH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70c0b470-f510-48f7-ea63-9a76f73d3000"
      },
      "source": [
        "len(unigrams_tolstoy)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9321"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EFiSsLsrcDK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcffd955-f441-4041-ce59-9eb2a75f9bdd"
      },
      "source": [
        "len(unigrams_dostoevsky)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11298"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTkJyicarcDM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "a068a413-24fc-4d36-f45b-5b394e6df7a1"
      },
      "source": [
        "bigrams_tolstoy.most_common(10)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<start> –', 1060),\n",
              " ('– сказал', 165),\n",
              " ('<start> она', 137),\n",
              " ('<start> и', 117),\n",
              " ('степан аркадьич', 115),\n",
              " ('<start> он', 114),\n",
              " ('– сказала', 98),\n",
              " ('– я', 95),\n",
              " ('<start> но', 91),\n",
              " ('что он', 77)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L7PKs9crcDP",
        "colab_type": "text"
      },
      "source": [
        "Чтобы посчитать условную вероятность мы можем поделить количество вхождений на количество вхождений первого слова."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w76PP7-srcDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "phrase = 'Нужно быть действительно великим человеком, чтобы суметь устоять даже против здравого смысла.'\n",
        "# phrase = 'Все смешалось в доме облонских'\n",
        "prob = Counter()\n",
        "for ngram in ngrammer(['<start>'] + normalize(phrase) + ['<end>']):\n",
        "    word1, word2 = ngram.split()\n",
        "    if word1 in unigrams_dostoevsky and ngram in bigrams_dostoevsky:\n",
        "        prob['dostoevsky'] += np.log(bigrams_dostoevsky[ngram]/unigrams_dostoevsky[word1])\n",
        "    else:\n",
        "        prob['dostoevsky'] += -10\n",
        "    if word1 in unigrams_tolstoy and ngram in bigrams_tolstoy:\n",
        "        prob['tolstoy'] += np.log(bigrams_tolstoy[ngram]/unigrams_tolstoy[word1])\n",
        "    else:\n",
        "        prob['tolstoy'] += -10\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1X_byTSrcDU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bdf5353f-f226-48f4-e055-7a9ad6915902"
      },
      "source": [
        "prob.most_common()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('dostoevsky', -114.04305126783456), ('tolstoy', -130)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvof_6BKrcDW",
        "colab_type": "text"
      },
      "source": [
        "Работает получше. Мы воспользовались небольшим хаком - для слов или биграммов, которых не было у нас в словаре, прибавляли низкую вероятность. Исправить это по-нормальному - сложно, придется подробнее разбираться с вероятностями, сглаживаниями и заменой неизвестных слов. Если интрересно - в книге Журафского про это есть."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAllH823rcDX",
        "colab_type": "text"
      },
      "source": [
        "Проблем с неизвестными словами у нас не будет, если мы будем пытаться сгенерировать новый текст. Давайте попробуем это сделать."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzGfIeaBrcDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix_dostoevsky = np.zeros((len(unigrams_dostoevsky), \n",
        "                   len(unigrams_dostoevsky)))\n",
        "id2word_dostoevsky = list(unigrams_dostoevsky)\n",
        "word2id_dostoevsky = {word:i for i, word in enumerate(id2word_dostoevsky)}\n",
        "\n",
        "\n",
        "for ngram in bigrams_dostoevsky:\n",
        "    word1, word2 = ngram.split()\n",
        "    matrix_dostoevsky[word2id_dostoevsky[word1]][word2id_dostoevsky[word2]] =  (bigrams_dostoevsky[ngram]/\n",
        "                                                                     unigrams_dostoevsky[word1])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuhMAMMJrcDb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# создадим матрицу вероятностей перейти из 1 слов в другое\n",
        "matrix_tolstoy = np.zeros((len(unigrams_tolstoy), \n",
        "                   len(unigrams_tolstoy)))\n",
        "\n",
        "id2word_tolstoy = list(unigrams_tolstoy)\n",
        "word2id_tolstoy = {word:i for i, word in enumerate(id2word_tolstoy)}\n",
        "\n",
        "\n",
        "# вероятность расчитываем точно также\n",
        "for ngram in bigrams_tolstoy:\n",
        "    word1, word2 = ngram.split()\n",
        "    matrix_tolstoy[word2id_tolstoy[word1]][word2id_tolstoy[word2]] =  (bigrams_tolstoy[ngram]/\n",
        "                                                                     unigrams_tolstoy[word1])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2T62YIgrcDd",
        "colab_type": "text"
      },
      "source": [
        "Для генерации нам понадобится функция np.random.choice , которая выбирает случайный объект из заданных. Ещё в неё можно подать вероятность каждого объекта и она будет доставать по ним (не только максимальный по вероятности)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nDi-h6mrcDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def generate(matrix, id2word, word2id, n=100, start='<start>'):\n",
        "    text = []\n",
        "    current_idx = word2id[start]\n",
        "    \n",
        "    for i in range(n):\n",
        "        \n",
        "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx])\n",
        "        text.append(id2word[chosen])\n",
        "        \n",
        "        if id2word[chosen] == '<end>':\n",
        "            chosen = word2id['<start>']\n",
        "        current_idx = chosen\n",
        "    \n",
        "    return ' '.join(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhPCe0atrcDh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "51742cce-d563-4adc-a3e6-69d4ce4accee"
      },
      "source": [
        "print(generate(matrix_dostoevsky, id2word_dostoevsky, word2id_dostoevsky).replace('<end>', '\\n'))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "мнение о вчерашнем а то что он — в одну рашель \n",
            " самое утро после этих поступков так много \n",
            " настасья \n",
            " — улыбалась лиза \n",
            " меня догоняют без сомнения известно тоже в петербург в доме о теперь — непременно хотелось бы вам говорю как собою смерть и в каких закоулках опустился в день она \n",
            " он был и законных наслаждений желаю и даже глупое молчание не говорите \n",
            " ваша идея с обществом кла-сси-чес… значит способны при всякой прислуги \n",
            " — враки \n",
            " вон у него с модной картинки если вы не было подумать что она села опять — да\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt62G0kDrcDm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "361d12a2-8b2b-4725-c37f-2b26bdd80bdb"
      },
      "source": [
        "print(generate(matrix_tolstoy, id2word_tolstoy, word2id_tolstoy).replace('<end>', '\\n'))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "– сказала вдруг как он считал своим шуточным тоном – проговорил он жадно всматривался \n",
            " vii приехав с тою любовью – я не мог простить \n",
            " она беспрестанно как краснеют мальчики – стало быть заряженные пистолеты \n",
            " – сказала она любила этого дня на тело подошел с развратным отцом и хотят вывести из тех влюблений которые все запиской в дверях двинулась было в то в то что и ступай – писала ей несколько лет с облонским стали такие мускулы да – он собирался сделать – сказал степан аркадьич улыбаясь на него все записываю \n",
            " вронский и вронский выехал из моих\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cASg3mc5rcDq",
        "colab_type": "text"
      },
      "source": [
        "## Коллокации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aJyRLRyrcDs",
        "colab_type": "text"
      },
      "source": [
        "Коллокации - это устойчивые выражения, состоящие из двух и более слов. Устойчивые - значит, что они часто используются вместе. Также часто значения коллокации не могут быть выведены лишь из значений, входящих в них слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggxFwPBErcDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "morph = MorphAnalyzer()\n",
        "\n",
        "def normalize(text):\n",
        "    normalized_text = [morph.parse(word.strip(punctuation))[0].normal_form for word \\\n",
        "                                                            in text.lower().split()]\n",
        "    normalized_text = [word for word in normalized_text if word]\n",
        "    return normalized_text\n",
        "\n",
        "\n",
        "def ngrammer(tokens, n=2):\n",
        "    ngrams = []\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams.append(' '.join(tokens[i:i+n]))\n",
        "    return ngrams\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHdit3LqrcDw",
        "colab_type": "text"
      },
      "source": [
        "Предобработаем почти также, только теперь нам не нужны тэги начала и конца."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "GbkL5yPXrcDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_dostoevsky =  [normalize(text) for text in sent_tokenize(dostoevsky)]\n",
        "sentences_tolstoy =  [normalize(text) for text in sent_tokenize(tolstoy)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUyVJd6UrcD0",
        "colab_type": "text"
      },
      "source": [
        "В списке много всяких чисел, однобуквеных слов и стоп-слов. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJik6-kfrcD1",
        "colab_type": "text"
      },
      "source": [
        "Добавим какие-нибудь ограничения к коду выше, чтобы биграммы получались почище."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MErGwbzDrcD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cyYWpA-rcD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stops = set(stopwords.words('russian') + ['–'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn7Jqix3rcD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ngrammer(tokens, stops, n=2):\n",
        "    ngrams = []\n",
        "    tokens = [token for token in tokens if token not in stops]\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams.append('_'.join(tokens[i:i+n]))\n",
        "    return ngrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDiZzddZrcED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_counter = Counter()\n",
        "\n",
        "for text in sentences_tolstoy:\n",
        "    word_counter.update(ngrammer(text, n=2, stops=stops))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZBwe49TrcEH",
        "colab_type": "code",
        "colab": {},
        "outputId": "6abafcb1-993c-4665-bb81-b5df2857150c"
      },
      "source": [
        "word_counter.most_common(15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('алексей_александр', 577),\n",
              " ('степан_аркадьй', 549),\n",
              " ('сергей_иван', 294),\n",
              " ('дарья_александр', 209),\n",
              " ('весь_это', 171),\n",
              " ('сказать_левин', 155),\n",
              " ('сказать_степан', 114),\n",
              " ('лидий_иван', 104),\n",
              " ('сказать_вронский', 88),\n",
              " ('сказать_анна', 88),\n",
              " ('знать_это', 87),\n",
              " ('говорить_это', 86),\n",
              " ('агафья_михайло', 76),\n",
              " ('графиня_лидий', 74),\n",
              " ('сказать_кить', 62)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H89wMkrzrcEO",
        "colab_type": "text"
      },
      "source": [
        "В списке есть коллокации, которые попали в список из-за того, что одно слово очень частотное и вообще встречается много в каких контекстах. Нас скорее интересуют случаи, когда слова в большинстве случаев встречаются вместе. Для этого мы можем придумать какие-нибудь формулы, учитывающие частоты слов по отдельности и общую частоту."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bwm5uFJrcEP",
        "colab_type": "text"
      },
      "source": [
        "Самый простой способ - взять количество упоминаний биграма и поделить на сумму количеств упоминаний слов по отдельности. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfvuhpQ0rcEQ",
        "colab_type": "text"
      },
      "source": [
        "Такая формула называется PMI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_TRIx5lrcEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scorer_simple(word_count_a, word_count_b, bigram_count, *args):\n",
        "    try:\n",
        "        score = bigram_count/((word_count_a+word_count_b))\n",
        "    \n",
        "    except ZeroDivisionError:\n",
        "        return 0\n",
        "    \n",
        "    return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrqwlYaPrcEU",
        "colab_type": "text"
      },
      "source": [
        "Сделаем функцию, которая будет делать счетчики для слов и биграммов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnAesZrUrcEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_stats(texts, stops):\n",
        "    ## соберем статистики для отдельных слов\n",
        "    ## и биграммов\n",
        "    \n",
        "    unigrams = Counter()\n",
        "    bigrams = Counter()\n",
        "    \n",
        "    for text in texts:\n",
        "        unigrams.update(text)\n",
        "        bigrams.update(ngrammer(text, stops, 2))\n",
        "    \n",
        "    return unigrams, bigrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX6FbxHNrcEf",
        "colab_type": "text"
      },
      "source": [
        "И функцию, которая пройдет по всем биграммам и вычислит для них нашу метрику."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeY0xEKxrcEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def score_bigrams(unigrams, bigrams, scorer, threshold=-100000, min_count=5):\n",
        "    ## посчитаем метрику для каждого нграмма\n",
        "    bigram2score = Counter()\n",
        "    len_vocab = len(unigrams)\n",
        "    for bigram in bigrams:\n",
        "        score = scorer(unigrams[bigram[0]], unigrams[bigram[1]], \n",
        "                       bigrams[bigram], len_vocab, min_count)\n",
        "        \n",
        "        ## если метрика выше порога, добавляем в словарик\n",
        "        if score > threshold:\n",
        "            bigram2score[bigram] = score\n",
        "    \n",
        "    return bigram2score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_eHejBFrcEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigrams, bigrams = collect_stats(sentences_tolstoy, stops)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt_r0G9_rcEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigram2score = score_bigrams(unigrams, bigrams, scorer_simple)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL2sP40wrcE1",
        "colab_type": "text"
      },
      "source": [
        "Проблема с таким подходом в том, что на самом верху окажутся слова, которые встречают по одному разу."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwmsqsTWrcE2",
        "colab_type": "code",
        "colab": {},
        "outputId": "342f4776-32ce-41d9-8e44-0a57f1b4cb0a"
      },
      "source": [
        "bigram2score.most_common(15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('человек_который', 61.0),\n",
              " ('знать_это', 43.5),\n",
              " ('первое_время', 32.0),\n",
              " ('это_мочь', 30.5),\n",
              " ('это_весь', 29.0),\n",
              " ('это_дело', 28.5),\n",
              " ('это_время', 26.0),\n",
              " ('графиня_лидий', 24.666666666666668),\n",
              " ('левин_чувствовать', 24.0),\n",
              " ('несмотря_весь', 23.0),\n",
              " ('дело_который', 22.0),\n",
              " ('это_самый', 22.0),\n",
              " ('левин_видеть', 22.0),\n",
              " ('левин_мочь', 19.0),\n",
              " ('левин_понять', 19.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uwnP3LVrcFT",
        "colab_type": "text"
      },
      "source": [
        "Поэтому можно немного переделать оценивающую функцию, добавив минимальное число вхождений для биграмма."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUfmP0oyrcFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scorer(word_count_a, word_count_b, bigram_count, len_vocab, min_count):\n",
        "    try:\n",
        "        score = ((bigram_count - min_count) / ((word_count_a + word_count_b)))\n",
        "    except ZeroDivisionError:\n",
        "        return 0\n",
        "    \n",
        "    return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WTtBOw8rcFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigram2score = score_bigrams(unigrams, bigrams, scorer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCtylA7_rcFp",
        "colab_type": "code",
        "colab": {},
        "outputId": "80d9a421-7bc4-44ce-e359-a7184a58fe8b"
      },
      "source": [
        "bigram2score.most_common(15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('человек_который', 56.0),\n",
              " ('знать_это', 41.0),\n",
              " ('это_мочь', 28.0),\n",
              " ('первое_время', 27.0),\n",
              " ('это_весь', 26.5),\n",
              " ('это_дело', 26.0),\n",
              " ('это_время', 23.5),\n",
              " ('графиня_лидий', 23.0),\n",
              " ('это_самый', 19.5),\n",
              " ('левин_чувствовать', 19.0),\n",
              " ('несмотря_весь', 18.0),\n",
              " ('дело_который', 17.0),\n",
              " ('левин_видеть', 17.0),\n",
              " ('это_сказать', 16.0),\n",
              " ('друг_друг', 15.333333333333334)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX-rRYPBrcFt",
        "colab_type": "text"
      },
      "source": [
        "В статье про Word2Vec для создания нграммов использовалась такая функция:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLdOLofFrcFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scorer_w2v(word_count_a, word_count_b, bigram_count, len_vocab, min_count=10):\n",
        "\n",
        "    try:\n",
        "        score = ((bigram_count - min_count) / (word_count_a * word_count_b)) * len_vocab\n",
        "    except ZeroDivisionError:\n",
        "        return 0\n",
        "    \n",
        "    return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayZN1NtErcFz",
        "colab_type": "text"
      },
      "source": [
        "Посмотрим, отличается ли она от нашей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjV1tc5MrcF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigram2score = score_bigrams(unigrams, bigrams, scorer_w2v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "F48mzg1yrcF4",
        "colab_type": "code",
        "colab": {},
        "outputId": "27cc9ce3-fb7e-4db9-fd9c-3f55aef1bf16"
      },
      "source": [
        "bigram2score.most_common(15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ребёнок_который', 34925.333333333336),\n",
              " ('решить_ехать', 14968.0),\n",
              " ('редко_бывать', 14968.0),\n",
              " ('решить_это', 9978.666666666666),\n",
              " ('решительно_знать', 9978.666666666666),\n",
              " ('ребёнок_мочь', 4989.333333333333),\n",
              " ('решительно_понимать', 4989.333333333333),\n",
              " ('mademoiselle_linon', 2494.6666666666665),\n",
              " ('железный_дорога', 2204.589147286822),\n",
              " ('женщина_который', 2204.589147286822),\n",
              " ('желать_это', 1624.4341085271317),\n",
              " ('сергей_иван', 1198.2692520775622),\n",
              " ('жена_посланник', 1160.3100775193798),\n",
              " ('брат_николай', 1116.0350877192982),\n",
              " ('женщина_это', 464.1240310077519)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUgBr8jorcF-",
        "colab_type": "text"
      },
      "source": [
        "Во всех случаях выше мы считали нграммами только слова, которые встречаются друг за другом. Но в нграммы часто можно ещё что-то вставить. Например, \"принять участие\" может превратиться в \"принять самое активное/непосредственное участие\". \n",
        "\n",
        "Чтобы отловить такие случаи можно считать нграммами слова, которые встречаются внутри какого-то окна. И считать по ним все те же метрики."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSjXAXXgrcF_",
        "colab_type": "text"
      },
      "source": [
        "Можно ещё посчитать стандартное отклонение расстояния между двумя словами. Если оно маленькое - слова обычно стоят на строгой позиции по отношению друг к другу."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3968MIercGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "def get_window_stats(texts, window=8):\n",
        "    \n",
        "    bigrams = defaultdict(list)\n",
        "    \n",
        "    # проходим окном по текстам \n",
        "    # берем первое слово и считаем его целевым\n",
        "    # проходим по остальным словам и их индексам\n",
        "    # добавляем в словарь пары (целевое слов, текущее слово)\n",
        "    # и добавляем индекс текущего в список этой пары\n",
        "    # так мы получаем (слово_1,слово_2):[1,2,1,1,3,2]\n",
        "    # порядок в этом случае учитывается - (слово_2, слово_1) - другая запись\n",
        "    for text in texts:\n",
        "        for i in range(len(text)-window):\n",
        "            words = list(enumerate(text[i:i+window]))\n",
        "            target = words[0][1]\n",
        "            for j, word in words[1:]:\n",
        "                bigrams[(target, word)].append(j)\n",
        "    \n",
        "    bigrams_stds = Counter()\n",
        "    for bigram in bigrams:\n",
        "        # выкидываем биграмы встретившиеся < 5 раз\n",
        "        if len(bigrams[bigram]) > 5:\n",
        "            bigrams_stds[bigram] = np.std(bigrams[bigram])\n",
        "    \n",
        "    return bigrams_stds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrFUFS72rcGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigram2std = get_window_stats(sentences_dostoevsky)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw5x8TNdrcGI",
        "colab_type": "code",
        "colab": {},
        "outputId": "08bc979c-c458-4cc0-f06c-a2c25a73dc52"
      },
      "source": [
        "bigram2std.most_common()[:-20:-1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('софья', 'матвей'), 0.0),\n",
              " (('большой', 'дорога'), 0.0),\n",
              " (('ваш', 'превосходительство'), 0.0),\n",
              " (('знаешь', 'ли'), 0.0),\n",
              " (('общий', 'дело'), 0.0),\n",
              " (('арин', 'прохор'), 0.0),\n",
              " (('вслед', 'за'), 0.0),\n",
              " (('семён', 'яков'), 0.0),\n",
              " (('воротиться', 'домой'), 0.0),\n",
              " (('из', 'сила'), 0.0),\n",
              " (('cher', 'он'), 0.0),\n",
              " (('алексей', 'егор'), 0.0),\n",
              " (('господин', 'кармазин'), 0.0),\n",
              " (('чуть', 'ли'), 0.0),\n",
              " (('артемий', 'павло'), 0.0),\n",
              " (('замечать', 'что'), 0.0),\n",
              " (('какой-то', 'особенный'), 0.0),\n",
              " (('с', 'постель'), 0.0),\n",
              " (('ради', 'бог'), 0.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE7zrbwircGP",
        "colab_type": "text"
      },
      "source": [
        "Можно применять расширить размер нграмма, а можно последовательно преобразовывать один и тот же текст, на каждом шагу собирая новые биграммы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgSSayMPrcGS",
        "colab_type": "text"
      },
      "source": [
        "Напишием такую функцию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkDEW7kMrcGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bigram_text(text, bigram2score):\n",
        "    new_text = []\n",
        "    i = 0\n",
        "    \n",
        "    while i < (len(text)-1):\n",
        "        bigram = '_'.join((text[i], text[i+1]))\n",
        "        if bigram in bigram2score:\n",
        "            new_text.append(bigram)\n",
        "            i += 2\n",
        "        else:\n",
        "            new_text.append(text[i])\n",
        "            i += 1\n",
        "    else:\n",
        "        if i == (len(text)-1):\n",
        "            new_text.append(text[i])\n",
        "    \n",
        "    return new_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55V6-eLkrcGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigrams, bigrams = collect_stats(sentences_dostoevsky, stops)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDlQjvGIrcGr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigram2score = score_bigrams(unigrams, bigrams, scorer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZxDBo8xrcGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_dostoevsky_2 = [bigram_text(sent, bigram2score) for sent in sentences_dostoevsky]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xANqlpDBrcG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigrams, bigrams = collect_stats(sentences_dostoevsky_2, stops)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raHIoFpdrcHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trigram2score = score_bigrams(unigrams, bigrams, scorer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gGG2B0crcHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_dostoevsky_3 = [bigram_text(sent, trigram2score) for sent in sentences_dostoevsky_2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et3f_5BrrcHI",
        "colab_type": "code",
        "colab": {},
        "outputId": "93a09133-c8ee-488c-b46c-c8ca4963f5b9"
      },
      "source": [
        "sentences_dostoevsky_3[4]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['домовый', 'ли', 'хоронить_ведьма_ль_замуж', 'выдавать']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-WSCb-CrcHY",
        "colab_type": "text"
      },
      "source": [
        "По этой ссылке можно прочитать про другие метрики.\n",
        "\n",
        "http://www.scielo.org.mx/scielo.php?script=sci_arttext&pid=S1405-55462016000300327#t1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SExdEzParcHZ",
        "colab_type": "text"
      },
      "source": [
        "### Все готовое"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1nzkGxIrcHa",
        "colab_type": "text"
      },
      "source": [
        "Писать все это самому конечно не обязательно."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in1AOryhrcHc",
        "colab_type": "text"
      },
      "source": [
        "Удобно пользоваться phraser из gensim'а. Он собирает статистику по корпусу, а затем склеивает слова в биграммы. Так как мы сделали выше. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTMb7u2ZrcHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfrqae51rcH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# собираем статистики\n",
        "ph = gensim.models.Phrases(texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFX-9pymrcH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# преобразовывать можно и через ph, но так быстрее \n",
        "p = gensim.models.phrases.Phraser(ph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfhD90FprcH9",
        "colab_type": "text"
      },
      "source": [
        "По умолчанию там используется метрики из статьи про ворд2век и ещё есть нормализованные pmi.\n",
        "Если не нравятся функции оценки, то ему можно подать любую другую функцию. Интерфейс у функции там почти точно такой же как и у наших."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsuJl4ZArcH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UFBtqTkrcIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# собираем статистики по уже забиграммленному тексту\n",
        "ph2 = gensim.models.Phrases(p[texts])\n",
        "p2 = gensim.models.phrases.Phraser(ph2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEcBHuwKrcIK",
        "colab_type": "code",
        "colab": {},
        "outputId": "c6590379-1c47-4b1b-9ec2-919a93c3f6f5"
      },
      "source": [
        "p2[p[texts[0]]][:20]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['многие',\n",
              " 'интересоваться',\n",
              " 'зачем',\n",
              " 'нужный',\n",
              " '«яблоку»',\n",
              " 'молодёжный',\n",
              " 'фракция',\n",
              " 'основной_задача',\n",
              " '«молодёжный',\n",
              " '«яблока»',\n",
              " 'являться',\n",
              " 'привлечение',\n",
              " 'молодая_человек',\n",
              " 'к',\n",
              " 'участие',\n",
              " 'в',\n",
              " 'выборы',\n",
              " 'и',\n",
              " 'деятельность',\n",
              " 'партия']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekiVJQthrcIZ",
        "colab_type": "text"
      },
      "source": [
        "Ну и наконец нграммы есть в нлтк. Тут больше метрик, но преборазователь слов в нграммы нужно написать самому."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNgKD8M9rcIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.collocations import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMDDgLAurcIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ivn1ITZrcIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "finder2 = BigramCollocationFinder.from_documents(texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbyBaASBrcI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "finder3 = TrigramCollocationFinder.from_documents(texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pPTUA4rErcI6",
        "colab_type": "code",
        "colab": {},
        "outputId": "558f555e-e460-43b7-90d3-e91338a85209"
      },
      "source": [
        "finder2.nbest(bigram_measures.likelihood_ratio, 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('один', 'из'),\n",
              " ('тот', 'что'),\n",
              " ('а', 'также'),\n",
              " ('при', 'это'),\n",
              " ('2017', 'год'),\n",
              " ('не', 'только'),\n",
              " ('точка', 'зрение'),\n",
              " ('то', 'есть'),\n",
              " ('сей', 'пора'),\n",
              " ('тот', 'число'),\n",
              " ('куб', 'метр'),\n",
              " ('владимир', 'путин'),\n",
              " ('2016', 'год'),\n",
              " ('тот', 'же'),\n",
              " ('потому', 'что'),\n",
              " ('миллиард', 'доллар'),\n",
              " ('о', 'тот'),\n",
              " ('прежде', 'всего'),\n",
              " ('кроме', 'тот'),\n",
              " ('до', 'сей')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "dS3U1-IJrcJI",
        "colab_type": "code",
        "colab": {},
        "outputId": "76f2cb9c-e227-4350-dd13-c2284da4ef49"
      },
      "source": [
        "finder3.nbest(trigram_measures.pmi, 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1947–2001»', 'monterey', 'ca'),\n",
              " ('50-летие', 'rolling', 'stones'),\n",
              " ('acs', 'nano', 'letters'),\n",
              " ('areva', 'edf', 'alstom'),\n",
              " ('armored', 'multi-purpose', 'vehicles'),\n",
              " ('atr', 'ленур', 'ислям'),\n",
              " ('bad', 'can', 'it'),\n",
              " ('bourgeois', '«эпатировать', 'буржуа»'),\n",
              " ('bundesanstalt', 'fuer', 'geowissenschaften'),\n",
              " ('can', 'it', 'be'),\n",
              " ('charge', 'ion', 'battery'),\n",
              " ('citizens', '1947–2001»', 'monterey'),\n",
              " ('commitment', 'competence', 'consensus'),\n",
              " ('corriere', 'della', 'sera'),\n",
              " ('della', 'sera', 'папа-на-покой'),\n",
              " ('diyanet', 'isleri', 'turk-islam'),\n",
              " ('dux', 'recording', 'producers'),\n",
              " ('edf', 'alstom', 'schneider'),\n",
              " ('egf', 'gazprom', 'monitor'),\n",
              " ('espanola', 'чть', 'прад')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaKbhQMurcJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
