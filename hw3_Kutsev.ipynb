{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitryKutsev/hse_compling_homework/blob/master/hw3_Kutsev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NSqtdQZuSyq",
        "colab_type": "code",
        "outputId": "bd956d76-5386-4cb2-b897-f7f77a4a36c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/correct_sents.txt"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  117k  100  117k    0     0  1122k      0 --:--:-- --:--:-- --:--:-- 1111k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIXeIstdugZg",
        "colab_type": "code",
        "outputId": "fdeb06ae-01a7-492d-d747-1b20e0ca2948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Fetch a single <1MB file using the raw GitHub URL.\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/sents_with_mistakes.txt"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  120k  100  120k    0     0  1179k      0 --:--:-- --:--:-- --:--:-- 1167k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPjf4t1oug7X",
        "colab_type": "code",
        "outputId": "33d26dcc-d487-4c94-c6ca-f4e1ea4f31ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os, re\n",
        "from string import punctuation\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import Counter\n",
        "from pprint import pprint\n",
        "import nltk\n",
        "from nltk import sent_tokenize, punkt\n",
        "punctuation += \"«»—…“”\"\n",
        "punct = set(punctuation)\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWq4BLdrujOP",
        "colab_type": "code",
        "outputId": "1f724c84-d1d4-4524-d715-fc41d892a29c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "!wget  https://github.com/yutkin/Lenta.Ru-News-Dataset\n",
        "!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-22 21:01:31--  https://github.com/yutkin/Lenta.Ru-News-Dataset\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘Lenta.Ru-News-Dataset.2’\n",
            "\n",
            "Lenta.Ru-News-Datas     [  <=>               ]  79.96K   217KB/s    in 0.4s    \n",
            "\n",
            "2019-11-22 21:01:32 (217 KB/s) - ‘Lenta.Ru-News-Dataset.2’ saved [81878]\n",
            "\n",
            "--2019-11-22 21:01:37--  https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/87156914/0b363e00-0126-11e9-9e3c-e8c235463bd6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191122%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191122T210138Z&X-Amz-Expires=300&X-Amz-Signature=cbd7bfacce47b4f2745f6ab3a6b70bc5edaa38417d59b6aabe81dd0b8bb39c71&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2019-11-22 21:01:38--  https://github-production-release-asset-2e65be.s3.amazonaws.com/87156914/0b363e00-0126-11e9-9e3c-e8c235463bd6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191122%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191122T210138Z&X-Amz-Expires=300&X-Amz-Signature=cbd7bfacce47b4f2745f6ab3a6b70bc5edaa38417d59b6aabe81dd0b8bb39c71&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.217.0.83\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.217.0.83|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 527373240 (503M) [application/octet-stream]\n",
            "Saving to: ‘lenta-ru-news.csv.gz.2’\n",
            "\n",
            "lenta-ru-news.csv.g 100%[===================>] 502.94M  16.3MB/s    in 32s     \n",
            "\n",
            "2019-11-22 21:02:11 (15.5 MB/s) - ‘lenta-ru-news.csv.gz.2’ saved [527373240/527373240]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiSIaDurulen",
        "colab_type": "code",
        "outputId": "9f5322fd-095f-4f5a-a91b-fb3975001ba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import gzip\n",
        "import csv\n",
        "os.listdir(os.getcwd())"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'lenta-ru-news.csv.gz.2',\n",
              " 'Lenta.Ru-News-Dataset',\n",
              " 'correct_sents.txt',\n",
              " 'lenta-ru-news.csv.gz.1',\n",
              " 'lenta-ru-news.csv.gz',\n",
              " 'Lenta.Ru-News-Dataset.2',\n",
              " 'corpus_5000.txt',\n",
              " 'Lenta.Ru-News-Dataset.1',\n",
              " 'sents_with_mistakes.txt',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm4GJ86NusQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = open('corpus_5000.txt', 'w')\n",
        "with gzip.open('lenta-ru-news.csv.gz', 'rt') as archive:\n",
        "    reader = csv.reader(archive, delimiter=',', quotechar='\"')\n",
        "    for i, line in enumerate(reader):\n",
        "        if i < 5000: \n",
        "            corpus.write(line[2].replace('\\xa0', ' ') + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCtCsVj-us1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(text):\n",
        "    normalized_text = [(word.strip(punctuation)) for word \\\n",
        "                       in text.lower().split()]\n",
        "    normalized_text = [word for word in normalized_text if word]\n",
        "    return normalized_text\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Wfr7z3yuwy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_sents_dict = []\n",
        "for text in open('corpus_5000.txt').read().splitlines():\n",
        "    sents = sent_tokenize(text)\n",
        "    norm_sents = [normalize(sent) for sent in sents]\n",
        "    corpus_sents_dict += norm_sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVVcnaVM2Tak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ngrammer(tokens, n=2):\n",
        "    ngrams = []\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams.append(' '.join(tokens[i:i+n]))\n",
        "    return ngrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq45YlPtsxVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigrams = Counter()\n",
        "for sentence in corpus_sents_dict:\n",
        "    unigrams.update(sentence)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apn9mL3DuzgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deletes(word):\n",
        "    letters    = 'йцукенгшщзхъфывапролджэячсмитьбюё'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes1    = [L + R[1:]               for L, R in splits if R]\n",
        "    splits2     = [(word[:i], word[i:])    for i in range(len(deletes1))]\n",
        "    deletes2    = [L + R[2:]               for L, R in splits2 if R]\n",
        "    # решил сделать два удаления, потому что это улучшило результат значительно, \n",
        "    # а скорость не очень сильно упала\n",
        "    deletes = deletes1 + deletes2 \n",
        "    return set(deletes)\n",
        "    #print(\"\\n\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B8BX5c3mOSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_dict = {}\n",
        "for i in unigrams.keys():\n",
        "  for j in deletes(i):\n",
        "    global_dict[j] = i\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tBYGKqk35Gl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_processing(input_word):\n",
        "  candidates_list = []\n",
        "  candidates_dict = {}\n",
        "  strongest_candidate = input_word\n",
        "  if input_word in unigrams:\n",
        "    return input_word\n",
        "  for i in deletes(input_word):\n",
        "    if i in global_dict:\n",
        "      candidates_dict[unigrams[global_dict[i]]] = global_dict[i]\n",
        "      strongest_candidate = max(candidates_dict.keys())\n",
        "      strongest_candidate = candidates_dict[strongest_candidate]\n",
        "  return strongest_candidate\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r939r9CM8pB7",
        "colab_type": "code",
        "outputId": "44a678fe-96e2-4bcc-9a6c-17da8efbaacf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input_processing(\"смртность\")"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'смертность'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu81aEeK-0Aa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bad = open('sents_with_mistakes.txt', encoding='utf8').read().lower().splitlines()\n",
        "true = open('correct_sents.txt', encoding='utf8').read().lower().splitlines()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVDvrIQs-5ZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def align_words(sent_1, sent_2):\n",
        "    tokens_1 = sent_1.lower().split()\n",
        "    tokens_2 = sent_2.lower().split()\n",
        "    \n",
        "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n",
        "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n",
        "    \n",
        "    return list(zip(tokens_1, tokens_2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0thsFu9B9E5O",
        "colab_type": "code",
        "outputId": "8e2d35b4-1c1e-45ed-ee0d-60cbf6eb67bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "%%time\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "total_mistaken = 0\n",
        "mistaken_fixed = 0\n",
        "\n",
        "total_correct = 0\n",
        "correct_broken = 0\n",
        "\n",
        "cashed = {}\n",
        "for i in range(len(true)):\n",
        "    word_pairs = align_words(true[i], bad[i])\n",
        "    for pair in word_pairs:\n",
        "        predicted = cashed.get(pair[1], input_processing(pair[1]))\n",
        "        cashed[pair[0]] = predicted\n",
        "        if predicted == pair[0]:\n",
        "            correct += 1\n",
        "        total += 1        \n",
        "        if pair[0] == pair[1]:\n",
        "            total_correct += 1\n",
        "            if pair[0] !=  predicted:\n",
        "                correct_broken += 1\n",
        "        else:\n",
        "            total_mistaken += 1\n",
        "            if pair[0] == predicted:\n",
        "                mistaken_fixed += 1\n",
        "        \n",
        "    if not i % 100:\n",
        "      print(i)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "CPU times: user 135 ms, sys: 0 ns, total: 135 ms\n",
            "Wall time: 136 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcZ5Kno--jl8",
        "colab_type": "code",
        "outputId": "5dc2feea-0544-47e7-e9e0-a61eec9b4583",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(correct/total)\n",
        "print(mistaken_fixed/total_mistaken)\n",
        "print(correct_broken/total_correct)"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6595404595404596\n",
            "0.3645433614735226\n",
            "0.2963133111289767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsUwr5BKFhY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Symspell+Trigram model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv4IzCP0EPOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_trig_sents = [['<start>', '<start>'] + sent + ['<end>'] for sent in corpus_sents_dict]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHGJCmN5qdTQ",
        "colab_type": "code",
        "outputId": "9b6e3e64-5fc5-4394-9af4-a6f3515d1fd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "unigrams = Counter()\n",
        "bigrams = Counter()\n",
        "trigrams = Counter()\n",
        "\n",
        "for sentence in corpus_trig_sents:\n",
        "    unigrams.update(ngrammer(sentence, n=1))\n",
        "    bigrams.update(ngrammer(sentence))\n",
        "    trigrams.update(ngrammer(sentence, n=3))\n",
        "bigrams.most_common(2)\n",
        "unigrams.most_common(4)"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<start>', 118256), ('<end>', 59128), ('в', 41912), ('и', 20341)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3Ei9eYaq71F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tri_input_processing(input_words):\n",
        "  candidates_list = []\n",
        "  candidates_dict = {}\n",
        "  input_word = str(input_words.split()[-1])\n",
        "  strongest_candidate = input_word\n",
        "  if input_word in unigrams:\n",
        "    return input_word\n",
        "  trigram_counter = 0\n",
        "  for i in deletes(input_word):\n",
        "    if i in global_dict:\n",
        "      bigram = \" \".join(input_words.split()[:-1]) \n",
        "      if unigrams[input_words.split()[0]]:\n",
        "        trigram = bigram + \" \" + global_dict[i]\n",
        "      if bigrams[bigram]:\n",
        "        trigram_prob = trigrams[trigram]/bigrams[bigram]\n",
        "        if trigram_prob:\n",
        "          trigram_counter += 1\n",
        "\n",
        "  #print(trigram_counter)\n",
        "  for i in deletes(input_word):\n",
        "    if i in global_dict:\n",
        "      one_word_prob = unigrams[global_dict[i]]/len(unigrams)\n",
        "      bigram = \" \".join(input_words.split()[:-1]) \n",
        "      if unigrams[input_words.split()[0]]:\n",
        "        trigram = bigram + \" \" + global_dict[i]\n",
        "      if bigrams[bigram]:\n",
        "        trigram_prob = trigrams[trigram]/bigrams[bigram]\n",
        "      if trigram_counter > 0:\n",
        "        prob_with_context = trigram_prob\n",
        "      else:\n",
        "        prob_with_context = one_word_prob\n",
        "      candidates_dict[prob_with_context] = global_dict[i]\n",
        "      strongest_candidate = max(candidates_dict.keys())\n",
        "      strongest_candidate = candidates_dict[strongest_candidate]\n",
        "  return strongest_candidate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBdcq8TrxeRN",
        "colab_type": "code",
        "outputId": "7cd8f851-1d80-4669-ea24-690bfca82cd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tri_input_processing(\"вице-премьер по сциальным\")"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'социальным'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJVN2VnyxwYc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "407e75d7-322c-4536-fdfd-091e6253dfee"
      },
      "source": [
        "%%time\n",
        "orrect = 0\n",
        "total = 0\n",
        "\n",
        "total_mistaken = 0\n",
        "mistaken_fixed = 0\n",
        "\n",
        "total_correct = 0\n",
        "correct_broken = 0\n",
        "\n",
        "cashed = {}\n",
        "for i in range(len(true)):\n",
        "  for j in range(len(true[i].split())):\n",
        "    #print(true[i][j])\n",
        "    word_pairs = align_words(true[i], bad[i])\n",
        "    if j == 0:\n",
        "      res_bigram = \"<start> <start> \"\n",
        "    else:\n",
        "      res_bigram = true[i].split()[j-1]+ \" \" + true[i].split()[j] + \" \"\n",
        "    for pair in word_pairs:\n",
        "      predicted = cashed.get(pair[1], tri_input_processing(res_bigram + pair[1]))\n",
        "      cashed[pair[0]] = predicted\n",
        "      if predicted == pair[0]:\n",
        "          correct += 1\n",
        "      total += 1\n",
        "      \n",
        "      if pair[0] == pair[1]:\n",
        "          total_correct += 1\n",
        "          if pair[0] !=  predicted:\n",
        "              correct_broken += 1\n",
        "      else:\n",
        "          total_mistaken += 1\n",
        "          if pair[0] == predicted:\n",
        "              mistaken_fixed += 1\n",
        "    #print(res_bigram)\n",
        "    # if not i % 100:\n",
        "    #   print(i)\n",
        "    #   print(correct/total)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2.34 s, sys: 2.63 ms, total: 2.35 s\n",
            "Wall time: 2.35 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ_TViMQ0miu",
        "colab_type": "code",
        "outputId": "de57d4c8-a6fb-4efc-c999-eda5a751fcaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(correct/total)\n",
        "print(mistaken_fixed/total_mistaken)\n",
        "print(correct_broken/total_correct)"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7130921801763631\n",
            "0.34257810054466914\n",
            "0.2951235943410893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keUtS4oGBRAE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f43d9096-790e-4e44-a99e-e3fee57a2099"
      },
      "source": [
        "#В питоне есть такая библиотека(и наверное не одна?)\n",
        "#Сделал для сравнения, и решил оставить.\n",
        "#Надеюсь, у меня нет ужасных принципиальных ошибок, и на сравнение можно смотреть\n",
        "#SymmSpell\n",
        "!pip install symspellpy\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "import pkg_resources"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: symspellpy in /usr/local/lib/python3.6/dist-packages (6.5.2)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.17.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ag86emMCG75",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "472d8268-e891-45de-d500-1be1fcddbb30"
      },
      "source": [
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "corpus_path = 'corpus_5000.txt'\n",
        "dictionary_path = sym_spell.create_dictionary(corpus_path)\n",
        "def Sym_spell_func(input_term):\n",
        "  sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "  suggestions = sym_spell.lookup(input_term, Verbosity.CLOSEST,\n",
        "                               max_edit_distance=2, include_unknown=True)\n",
        "\n",
        "  return str(suggestions[0]).split(',')[0]\n",
        "Sym_spell_func('впревые')"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'впервые'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHTEf6ZNCKKP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "63699833-17f5-4421-d95c-af3d5e27388c"
      },
      "source": [
        "%%time\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "total_mistaken = 0\n",
        "mistaken_fixed = 0\n",
        "\n",
        "total_correct = 0\n",
        "correct_broken = 0\n",
        "\n",
        "cashed = {}\n",
        "for i in range(len(true)):\n",
        "    word_pairs = align_words(true[i], bad[i])\n",
        "    for pair in word_pairs:\n",
        "        predicted = cashed.get(pair[1], Sym_spell_func(pair[1]))\n",
        "        cashed[pair[0]] = predicted\n",
        "        if predicted == pair[0]:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        \n",
        "        if pair[0] == pair[1]:\n",
        "            total_correct += 1\n",
        "            if pair[0] !=  predicted:\n",
        "                correct_broken += 1\n",
        "        else:\n",
        "            total_mistaken += 1\n",
        "            if pair[0] == predicted:\n",
        "                mistaken_fixed += 1\n",
        "        \n",
        "    if not i % 100:\n",
        "      print(i)"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "CPU times: user 1.38 s, sys: 11.8 ms, total: 1.4 s\n",
            "Wall time: 1.39 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6MFx-csDJJK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "aff507ce-636a-46d6-af33-2e09c4c7a895"
      },
      "source": [
        "print(correct/total)\n",
        "print(mistaken_fixed/total_mistaken)\n",
        "print(correct_broken/total_correct)"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7020979020979021\n",
            "0.5003837298541827\n",
            "0.26771563110141267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-JUFs1wDMTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#В итоге у меня теряется 15%, но я так и не успел найти, где именно"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}