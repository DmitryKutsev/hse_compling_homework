{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Задание 1. (5 баллов) \n",
    "# # В тетрадке реализована биграмная языковая модель (при генерации учитывается информация только о 1\n",
    "# предыдущем слове). Реализуйте триграмную модель и сгенерируйте несколько текстов. Сравните их с текстами,\n",
    "# сгенерированными биграмной моделью. \n",
    "# # Можно использовать те же тексты, что в семинаре, или взять какой-то другой (на английском или русском языке)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dostoevsky = open('besy_dostoevsky.txt', encoding='cp1251').read()\n",
    "tolstoy = open('anna_karenina.txt', encoding='utf-8').read()\n",
    "dostoevsky = dostoevsky[:10000]\n",
    "tolstoy = tolstoy[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import numpy as np\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = [word.strip(punctuation) for word in text.lower().split()]\n",
    "    normalized_text = [word for word in normalized_text if word]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dostoevsky = normalize(dostoevsky)\n",
    "norm_tolstoy = normalize(tolstoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['спасибо', 'что', 'скачали', 'книгу', 'в', 'бесплатной', 'электронной', 'библиотеке', 'royallib.ru', 'http://royallib.ru']\n",
      "1507\n",
      "['annotation', '«анна', 'каренина»', '–', 'это', 'сложное', 'психологически', 'утонченное', 'остропроблемное', 'произведение']\n",
      "1587\n"
     ]
    }
   ],
   "source": [
    "print(norm_dostoevsky[:10])\n",
    "print(len(norm_dostoevsky))\n",
    "print(norm_tolstoy[:10])\n",
    "print(len(norm_tolstoy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уникальных лемм в срезе Бесов - 836\n",
      "Уникальный лемм в срезе Анны Карениной -  817\n"
     ]
    }
   ],
   "source": [
    "print(\"Уникальных лемм в срезе Бесов -\", len(set(norm_dostoevsky)))\n",
    "print(\"Уникальный лемм в срезе Анны Карениной - \", len(set(norm_tolstoy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dostoevsky = Counter(norm_dostoevsky)\n",
    "vocab_tolstoy = Counter(norm_tolstoy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 83),\n",
       " ('в', 57),\n",
       " ('что', 30),\n",
       " ('не', 28),\n",
       " ('его', 19),\n",
       " ('но', 19),\n",
       " ('он', 16),\n",
       " ('же', 15),\n",
       " ('с', 15),\n",
       " ('а', 11)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dostoevsky.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 70),\n",
       " ('–', 52),\n",
       " ('в', 47),\n",
       " ('не', 36),\n",
       " ('он', 36),\n",
       " ('что', 27),\n",
       " ('на', 25),\n",
       " ('с', 25),\n",
       " ('как', 22),\n",
       " ('матвей', 13)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_tolstoy.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 0.055076310550763105),\n",
       " ('в', 0.0378234903782349),\n",
       " ('что', 0.019907100199071003),\n",
       " ('не', 0.0185799601857996),\n",
       " ('его', 0.012607830126078301),\n",
       " ('но', 0.012607830126078301),\n",
       " ('он', 0.010617120106171201),\n",
       " ('же', 0.009953550099535502),\n",
       " ('с', 0.009953550099535502),\n",
       " ('а', 0.0072992700729927005)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas_dosoevsky = Counter({word:c/len(norm_dostoevsky) for word, c in vocab_dostoevsky.items()})\n",
    "probas_dosoevsky.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 0.04410838059231254),\n",
       " ('–', 0.03276622558286074),\n",
       " ('в', 0.029615626969124134),\n",
       " ('не', 0.022684310018903593),\n",
       " ('он', 0.022684310018903593),\n",
       " ('что', 0.017013232514177693),\n",
       " ('на', 0.01575299306868305),\n",
       " ('с', 0.01575299306868305),\n",
       " ('как', 0.013862633900441084),\n",
       " ('матвей', 0.008191556395715185)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas_tolstoy = Counter({word:c/len(norm_tolstoy) for word, c in vocab_tolstoy.items()})\n",
    "probas_tolstoy.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tolstoy', -28.392685814933895), ('dostoevsky', -43.52203961189503)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = 'Все смешалось в доме облонских'\n",
    "\n",
    "prob = Counter({'tolstoy':0, 'dostoevsky':0})\n",
    "\n",
    "for word in normalize(phrase):\n",
    "    prob['dostoevsky'] += np.log(probas_dosoevsky.get(word, 0.00001))\n",
    "    prob['tolstoy'] += np.log(probas_tolstoy.get(word, 0.00001))\n",
    "\n",
    "prob.most_common()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dostoevsky = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dostoevsky)]\n",
    "sentences_tolstoy = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(tolstoy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_dostoevsky = Counter()\n",
    "bigrams_dostoevsky = Counter()\n",
    "trigrams_dostoevsky = Counter()\n",
    "\n",
    "for sentence in sentences_dostoevsky:\n",
    "    unigrams_dostoevsky.update(sentence)\n",
    "    bigrams_dostoevsky.update(ngrammer(sentence))\n",
    "for sentence in sentences_dostoevsky:\n",
    "    trigrams_dostoevsky.update(ngrammer(sentence, n=3))\n",
    "\n",
    "\n",
    "\n",
    "unigrams_tolstoy = Counter()\n",
    "bigrams_tolstoy = Counter()\n",
    "trigrams_tolstoy = Counter()\n",
    "\n",
    "for sentence in sentences_tolstoy:\n",
    "    unigrams_tolstoy.update(sentence)\n",
    "    bigrams_tolstoy.update(ngrammer(sentence))\n",
    "for sentence in sentences_tolstoy:\n",
    "    trigrams_tolstoy.update(ngrammer(sentence, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_matrix_dostoevsky = np.zeros((len(unigrams_dostoevsky), \n",
    "                   len(unigrams_dostoevsky)))\n",
    "id2word_dostoevsky = list(unigrams_dostoevsky)\n",
    "word2id_dostoevsky = {word:i for i, word in enumerate(id2word_dostoevsky)}\n",
    "\n",
    "\n",
    "for ngram in bigrams_dostoevsky:\n",
    "    word1, word2 = ngram.split()\n",
    "    bi_matrix_dostoevsky[word2id_dostoevsky[word1]][word2id_dostoevsky[word2]] =  (bigrams_dostoevsky[ngram]/\n",
    "                                                                     unigrams_dostoevsky[word1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_matrix_dostoevsky = np.zeros((len(trigrams_dostoevsky), \n",
    "                   len(trigrams_dostoevsky)))\n",
    "bi_id2word_dostoevsky = list(bigrams_dostoevsky)\n",
    "bi_word2id_dostoevsky = {word:i for i, word in enumerate(bi_id2word_dostoevsky)}\n",
    "\n",
    "\n",
    "for ngram in trigrams_dostoevsky:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    tri_matrix_dostoevsky[bi_word2id_dostoevsky[word1 + ' ' + word2]][word2id_dostoevsky[word3]] \\\n",
    "        = (trigrams_dostoevsky[ngram]/ bigrams_dostoevsky[word1 + ' ' + word2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_matrix_tolstoy = np.zeros((len(unigrams_tolstoy), \n",
    "                   len(unigrams_tolstoy)))\n",
    "id2word_tolstoy = list(unigrams_tolstoy)\n",
    "word2id_tolstoy = {word:i for i, word in enumerate(id2word_tolstoy)}\n",
    "\n",
    "\n",
    "for ngram in bigrams_tolstoy:\n",
    "    word1, word2 = ngram.split()\n",
    "    bi_matrix_tolstoy[word2id_tolstoy[word1]][word2id_tolstoy[word2]] =  (bigrams_tolstoy[ngram]/\n",
    "                                                                     unigrams_tolstoy[word1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_matrix_tolstoy = np.zeros((len(trigrams_tolstoy), \n",
    "                   len(trigrams_tolstoy)))\n",
    "bi_id2word_tolstoy = list(bigrams_tolstoy)\n",
    "bi_word2id_tolstoy = {word:i for i, word in enumerate(bi_id2word_tolstoy)}\n",
    "\n",
    "\n",
    "for ngram in trigrams_tolstoy:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    tri_matrix_tolstoy[bi_word2id_tolstoy[word1 + ' ' + word2]][word2id_tolstoy[word3]] = \\\n",
    "    (trigrams_tolstoy[ngram]/bigrams_tolstoy[word1 + ' ' + word2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word, word2id, n=100, start='<start>'):\n",
    "    text = []\n",
    "    current_idx = word2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx])\n",
    "        text.append(id2word[chosen])\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            chosen = word2id['<start>']\n",
    "        current_idx = chosen\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tri_generate(matrix, id2word, uni_id2word, word2id, n=100):\n",
    "    text = []\n",
    "    for i in range(n):\n",
    "        chosen = np.random.choice(matrix.shape[1],\n",
    "                                p=matrix[word2id['<start> <start>']])       \n",
    "        text.append(id2word[chosen] + \" \" + uni_id2word[chosen])\n",
    "        if '<end>' in id2word[chosen]:\n",
    "                    chosen = np.random.choice(matrix.shape[1],\n",
    "                                p=matrix[word2id['<start> <start>']])\n",
    "\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "выдают \n",
      " и глава viii пусть двадцать лет сцена выдают \n",
      " и минуту как говорили выдают \n",
      " и выдают \n",
      " и сдаче губернии вдруг не раздавил какова сколько их домового уме и я http://royallib.ru все все по деревням глава двадцать лет сцена   спасибо спасибо по деревням глава бес нас по в городе евангелие положение «гонимого» привычка даже и прекратил выдают \n",
      " и стадо свиней он случившееся побежали видевшие он например но да рядом юноша уме и я не раздавил какова тем и мнение можно так одним степана трофимовича скажу чтения \n",
      " хоть   домового тут минуту как говорили не отличавшемся — сдаче губернии вдруг стадо свиней он лет к это   домового тут минуту как говорили привычка привела уверь глава viii пусть убей следа не бес нас по он например но книгу в в одно время диссертация стадо свиней он между тем вообще книгу в в   домового тут стадо свиней он книгу в в глава viii пусть свиней и бесы и что очень стадо свиней он глава viii пусть стадо свиней он но уже успел бес нас по сдаче губернии вдруг книгу в в стадо свиней он тем и мнение http://royallib.ru/author/dostoevskiy_fedor.html эта эта одно время диссертация и что очень   домового тут гражданскую роль потом выдают \n",
      " и он например но убей следа не чтения \n",
      " хоть стадо свиней он книгу в в он например но чрезвычайно любил чего если можно ну книгу в в лет к это можно так одним книгу в в убей следа не сколько их домового убей следа не чрезвычайно любил чего он например но он например но он например но между тем вообще   спасибо спасибо минуту как говорили уме и я стадо свиней он лет к это можно так одним чтения \n",
      " хоть свиней и бесы сдаче губернии вдруг не раздавил какова\n"
     ]
    }
   ],
   "source": [
    "print(tri_generate(tri_matrix_dostoevsky, bi_id2word_dostoevsky, id2word_dostoevsky, bi_word2id_dostoevsky).replace\n",
    "      ('<end>', '\\n').replace('<start>', ' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "не далее как принято было поспешно запрещено и одно время в ежемесячном и ганзеатическом значении немецкого городка ганау в ежемесячном и столь странных событий происшедших в совершенной невинности ему между 1413 и даже науки хотя впрочем в озеро и рассказали им \n",
      "     какова же начинает новую жизнь с оттенком высшего юмора \n",
      "   за напечатанную первую половину \n",
      " пусть эти хоры поют о совершенной невинности ему вовсе не видно да рядом случается \n",
      "     и даровитейший человек в прошлом году предлагал степану трофимовичу ее напечатать здесь — его а если разговаривают то обладатель\n"
     ]
    }
   ],
   "source": [
    "print(generate(bi_matrix_dostoevsky, id2word_dostoevsky, word2id_dostoevsky).\n",
    "      replace('<end>', '\\n').replace('<start> <start>', ' ').replace('<start>', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "и кучер да неожиданно уличены вслед – да «ах над ним и из театра ужасно каренина» – – – это это – да «ах бегали по он каренина» – – случается с матвей – да «ах черная кухарка глаза каренина» – – но там она сморщил лоб «всему каренина» – – бегали по он что это «там кухарка и «да тут он увидав каренина» – – к себе пускай взгляд на лев каждая несчастливая может виной эта наверху каренина» – – каренина» – – кухарка и «да взгляд на лев все счастливые с каренина» – – вчера со но что это «там долли неподвижно ответ но там она каренина» – – в этом-то ясно виной эта наверху театра веселый ай но там она удивлению не нехорошо каренина» – – ко дню вместо но там она несчастлива по-своему положение каренина» – – каренина» – – каренина» – – продолжалось уже дети кухарка и «да взгляд на лев каренина» – – продолжалось уже дети но там она а в много – да «ах света пробившуюся эту каренина» – – каренина» – – каждая несчастливая может бегали по он каренина» – – каренина» – – каренина» – –   л.н л.н каренина» – – взгляд на лев вчера со но – да «ах руке и забыться вчера со но часть восьмая все каренина» – – уклада жизни в   толстой на каренина» – – не выходила есть   л.н л.н театра веселый ай каренина» – – над ним и а в много отмщение и жена каренина» – – удивлению не нехорошо   работая взгляд театра веселый ай каренина» – –   работая взгляд аз воздам что неожиданно уличены вслед случается с матвей «ах ах «но tesoro и неприятнее сошедшиеся люди степан каренина» – – долли неподвижно ответ сморщил лоб «всему   толстой на\n"
     ]
    }
   ],
   "source": [
    "print(tri_generate(tri_matrix_tolstoy, bi_id2word_tolstoy, id2word_tolstoy, bi_word2id_tolstoy).replace\n",
    "      ('<end>', '\\n').replace('<start>', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " – подумал степан аркадьич помолчал \n",
      " степан аркадьич не виноват \n",
      " что виной эта вечно озабоченная и себя впечатления из этой ссоры князь степан аркадьич помолчал \n",
      "  – образуется \n",
      "   он не позволял себе \n",
      "     «попробовать хотите» – слава богу – сказал он сделал \n",
      "   лев николаевич толстой часть пятая часть четвертая часть вторая часть четвертая часть седьмая часть восьмая librs.net \n",
      " он вспомнил вдруг вскочил сел на диван и домочадцы чувствовали что случается с припасами для жены если б ожидал что виной эта глупая улыбка» – все смешалось в\n"
     ]
    }
   ],
   "source": [
    "print(generate(bi_matrix_tolstoy, id2word_tolstoy, word2id_tolstoy).\n",
    "      replace('<end>', '\\n').replace('<start> <start>', ' ').replace('<start> ', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Задание 2. (5 баллов) \n",
    "# При помощи gensim.models.Phrases реализуйте byte-pair-encoding,\n",
    "# про который говорилось на первом семинаре \n",
    "# (https://github.com/mannefedov/compling_nlp_hse_course/blob/master/notebooks/Preprocessing.ipynb) \n",
    "# А именно 1) возьмите любой текст; разбейте его на предложения, а каждое предложение разбейте на \n",
    "# отдельные символы (не потеряйте пробелы) 2) обучите gensim.models.Phrases на полученных\n",
    "# символьных предложениях 3) примените полученный нграммер к этим символьным предложениям \n",
    "# 4) повторите 2 и 3 N количество раз, чтобы начали получаться целые слова\n",
    "# Параметры в gensim.models.Phrases влияют на количество получаемых нграммов после каждого \n",
    "# прохода, поэтому не забудьте их настроить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.summarization.textcleaner import split_sentences\n",
    "from gensim.models.phrases import Phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['С', 'п', 'а', 'с', 'и', 'б', 'о', ',', ' ', 'ч']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens  = sent_tokenize(dostoevsky, 'russian')[:10]\n",
    "symbol_tokens = [list(i) for i in sent_tokens]\n",
    "symbol_tokens[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = Phrases(symbol_tokens, scoring='npmi', min_count=1, threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Phrases(p[symbol_tokens], scoring='npmi', min_count=1, threshold=0.1 )\n",
    "p2 = Phrases(p1[p[symbol_tokens]], scoring='npmi', min_count=1, threshold=-1 )\n",
    "p3 = Phrases(p2[p1[p[symbol_tokens]]], scoring='npmi', min_count=1, threshold=-1 )\n",
    "p4 = Phrases(p3[p2[p1[p[symbol_tokens]]]], scoring='npmi', min_count=1, threshold=1 )\n",
    "p5 = Phrases(p4[p3[p2[p1[p[symbol_tokens]]]]], scoring='npmi', min_count=1, threshold=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['С_п', 'а_с', 'и_б', 'о_,', ' _ч', 'т_о', ' _с', 'к_а', 'ч_а', 'л_и', ' _к', 'н_и', 'г_у', ' _в', ' _б', 'е_с', 'п_л', 'а_т', 'н_о', 'й_ ', 'э_л', 'е_к', 'т_р', 'о_н', 'н_о', 'й_ ', 'б_и', 'б_л', 'и', 'о_т', 'е_к', 'е_ ', 'R_o', 'y_a', 'l_l', 'i_b', '._r', 'u_:', ' _h', 't_t', 'p_:', '/_/', 'r_o', 'y_a', 'l_l', 'i_b', '._r', 'u_\\n', '\\n_В', 'с', 'е_ ', 'к_н', 'и_г', 'и_ ', 'а_в', 'т_о', 'р_а', ':_ ', 'h_t', 't_p', ':_/', '/_r', 'o_y', 'a_l', 'l_i', 'b_.', 'r_u', '/_a', 'u_t', 'h_o', 'r_/', 'd_o', 's_t', 'o_e', 'v_s', 'k_i', 'y__', 'f_e', 'd_o', 'r_.', 'h_t', 'm_l', '\\n_\\n', 'Э_т', 'а_ ', 'ж_е', ' _к', 'н_и', 'г_а', ' _в', ' _д', 'р_у', 'г_и', 'х_ ', 'ф_о', 'р_м', 'а_т', 'а_х', ':_ ', 'h_t', 't_p', ':_/', '/_r', 'o_y', 'a_l', 'l_i', 'b_.', 'r_u', '/_b', 'o_o', 'k_/', 'd_o', 's_t', 'o_e', 'v_s', 'k_i', 'y__', 'f_e', 'd_o', 'r_/', 'b_e', 's_i', '._h', 't_m', 'l_\\n', '\\n_П', 'р_и', 'я_т', 'н_о', 'г_о', ' _ч', 'т_е', 'н_и', 'я_!']] \n",
      "\n",
      "\n",
      "\n",
      "[['С_п_а_с', 'и_б_о_,', ' _ч_т_о', ' _с_к_а', 'ч_а_л_и', ' _к_н_и', 'г_у_ _в', ' _б_е_с', 'п_л_а_т', 'н_о_й_ ', 'э_л_е_к', 'т_р_о_н', 'н_о_й_ ', 'б_и_б_л', 'и_о_т', 'е_к_е_ ', 'R_o_y_a', 'l_l_i_b', '._r_u_:', ' _h_t_t', 'p_:_/_/', 'r_o_y_a', 'l_l_i_b', '._r_u_\\n', '\\n_В_с', 'е_ _к_н', 'и_г_и_ ', 'а_в_т_о', 'р_а_:_ ', 'h_t_t_p', ':_/_/_r', 'o_y_a_l', 'l_i_b_.', 'r_u_/_a', 'u_t_h_o', 'r_/_d_o', 's_t_o_e', 'v_s_k_i', 'y___f_e', 'd_o_r_.', 'h_t_m_l', '\\n_\\n_Э_т', 'а_ _ж_е', ' _к_н_и', 'г_а_ _в', ' _д_р_у', 'г_и_х_ ', 'ф_о_р_м', 'а_т_а_х', ':_ _h_t', 't_p_:_/', '/_r_o_y', 'a_l_l_i', 'b_._r_u', '/_b_o_o', 'k_/_d_o', 's_t_o_e', 'v_s_k_i', 'y___f_e', 'd_o_r_/', 'b_e_s_i', '._h_t_m', 'l_\\n_\\n_П', 'р_и_я_т', 'н_о_г_о', ' _ч_т_е', 'н_и_я_!']] \n",
      "\n",
      "\n",
      "\n",
      "[['С_п_а_с_и_б_о_,_ _ч_т_о_ _с_к_а', 'ч_а_л_и_ _к_н_и_г_у_ _в_ _б_е_с', 'п_л_а_т_н_о_й_ _э_л_е_к_т_р_о_н', 'н_о_й_ _б_и_б_л_и_о_т_е_к_е_ ', 'R_o_y_a_l_l_i_b_._r_u_:_ _h_t_t', 'p_:_/_/_r_o_y_a_l_l_i_b_._r_u_\\n', '\\n_В_с_е_ _к_н_и_г_и_ _а_в_т_о', 'р_а_:_ _h_t_t_p_:_/_/_r_o_y_a_l', 'l_i_b_._r_u_/_a_u_t_h_o_r_/_d_o', 's_t_o_e_v_s_k_i_y___f_e_d_o_r_.', 'h_t_m_l_\\n_\\n_Э_т_а_ _ж_е_ _к_н_и', 'г_а_ _в_ _д_р_у_г_и_х_ _ф_о_р_м', 'а_т_а_х_:_ _h_t_t_p_:_/_/_r_o_y', 'a_l_l_i_b_._r_u_/_b_o_o_k_/_d_o', 's_t_o_e_v_s_k_i_y___f_e_d_o_r_/', 'b_e_s_i_._h_t_m_l_\\n_\\n_П_р_и_я_т', 'н_о_г_о_ _ч_т_е_н_и_я_!']] \n",
      "\n",
      "\n",
      "\n",
      "[['С_п_а_с_и_б_о_,_ _ч_т_о_ _с_к_а', 'ч_а_л_и_ _к_н_и_г_у_ _в_ _б_е_с', 'п_л_а_т_н_о_й_ _э_л_е_к_т_р_о_н', 'н_о_й_ _б_и_б_л_и_о_т_е_к_е_ ', 'R_o_y_a_l_l_i_b_._r_u_:_ _h_t_t', 'p_:_/_/_r_o_y_a_l_l_i_b_._r_u_\\n', '\\n_В_с_е_ _к_н_и_г_и_ _а_в_т_о', 'р_а_:_ _h_t_t_p_:_/_/_r_o_y_a_l', 'l_i_b_._r_u_/_a_u_t_h_o_r_/_d_o', 's_t_o_e_v_s_k_i_y___f_e_d_o_r_.', 'h_t_m_l_\\n_\\n_Э_т_а_ _ж_е_ _к_н_и', 'г_а_ _в_ _д_р_у_г_и_х_ _ф_о_р_м', 'а_т_а_х_:_ _h_t_t_p_:_/_/_r_o_y', 'a_l_l_i_b_._r_u_/_b_o_o_k_/_d_o', 's_t_o_e_v_s_k_i_y___f_e_d_o_r_/', 'b_e_s_i_._h_t_m_l_\\n_\\n_П_р_и_я_т', 'н_о_г_о_ _ч_т_е_н_и_я_!']] \n",
      "\n",
      "\n",
      "\n",
      "[['С_п_а_с_и_б_о_,_ _ч_т_о_ _с_к_а', 'ч_а_л_и_ _к_н_и_г_у_ _в_ _б_е_с', 'п_л_а_т_н_о_й_ _э_л_е_к_т_р_о_н', 'н_о_й_ _б_и_б_л_и_о_т_е_к_е_ ', 'R_o_y_a_l_l_i_b_._r_u_:_ _h_t_t', 'p_:_/_/_r_o_y_a_l_l_i_b_._r_u_\\n', '\\n_В_с_е_ _к_н_и_г_и_ _а_в_т_о', 'р_а_:_ _h_t_t_p_:_/_/_r_o_y_a_l', 'l_i_b_._r_u_/_a_u_t_h_o_r_/_d_o', 's_t_o_e_v_s_k_i_y___f_e_d_o_r_.', 'h_t_m_l_\\n_\\n_Э_т_а_ _ж_е_ _к_н_и', 'г_а_ _в_ _д_р_у_г_и_х_ _ф_о_р_м', 'а_т_а_х_:_ _h_t_t_p_:_/_/_r_o_y', 'a_l_l_i_b_._r_u_/_b_o_o_k_/_d_o', 's_t_o_e_v_s_k_i_y___f_e_d_o_r_/', 'b_e_s_i_._h_t_m_l_\\n_\\n_П_р_и_я_т', 'н_о_г_о_ _ч_т_е_н_и_я_!']] \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(list(p[symbol_tokens][:1]), '\\n\\n\\n')\n",
    "print(list(p1[p[symbol_tokens]][:1]), '\\n\\n\\n')\n",
    "print(list(p3[p2[p1[p[symbol_tokens]]]][:1]), '\\n\\n\\n')\n",
    "print(list(p3[p2[p1[p[symbol_tokens]]]][:1]), '\\n\\n\\n' )\n",
    "print(list(p4[p3[p2[p1[p[symbol_tokens]]]]][:1]),'\\n\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
