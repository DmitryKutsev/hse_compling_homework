{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Языковое моделирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Языковое моделирование заключается в приписывании вероятности последовательности слов. Сейчас языковые модели используются практически во всех nlp задачах. Всякие Берты и Элмо - языковые модели. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это достаточно сложная тема, поэтому будем разбирать постепенно. Сегодня разберём самые основы. Научимся приписывать вероятность последовательности слов и попробуем генерировать текст."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем два текста: Анну Каренину и Бесов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dostoevsky = open('besy_dostoevsky.txt', encoding='cp1251').read()\n",
    "tolstoy = open('anna_karenina.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анна Каренина немного больше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина Бесов Достоевского - 1293557\n",
      "Длина Анны Карениной Толстого -  1710408\n"
     ]
    }
   ],
   "source": [
    "print(\"Длина Бесов Достоевского -\", len(dostoevsky))\n",
    "print(\"Длина Анны Карениной Толстого - \", len(tolstoy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем простую функцию для нормализации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import numpy as np\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = [word.strip(punctuation) for word \\\n",
    "                                                            in text.lower().split()]\n",
    "    normalized_text = [word for word in normalized_text if word]\n",
    "    return normalized_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним тексты по словам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dostoevsky = normalize(dostoevsky)\n",
    "norm_tolstoy = normalize(tolstoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина Бесов Достоевского в токенах - 208453\n",
      "Длина Анны Карениной Толстого в токенах -  281201\n"
     ]
    }
   ],
   "source": [
    "print(\"Длина Бесов Достоевского в токенах -\", len(norm_dostoevsky))\n",
    "print(\"Длина Анны Карениной Толстого в токенах - \", len(norm_tolstoy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бесы короче, но уникальных слов там больше!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уникальных лемм в Бесах - 32547\n",
      "Уникальный лемм в Анне Карениной -  34820\n"
     ]
    }
   ],
   "source": [
    "print(\"Уникальных лемм в Бесах -\", len(set(norm_dostoevsky)))\n",
    "print(\"Уникальный лемм в Анне Карениной - \", len(set(norm_tolstoy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем, сколько раз встречаются слова и выведем самые частотные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dostoevsky = Counter(norm_dostoevsky)\n",
    "vocab_tolstoy = Counter(norm_tolstoy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 8599),\n",
       " ('—', 7227),\n",
       " ('в', 4734),\n",
       " ('не', 4707),\n",
       " ('что', 3547),\n",
       " ('я', 3377),\n",
       " ('он', 2489),\n",
       " ('с', 2419),\n",
       " ('на', 2359),\n",
       " ('но', 1833)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dostoevsky.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 12885),\n",
       " ('–', 11490),\n",
       " ('не', 6517),\n",
       " ('что', 5721),\n",
       " ('в', 5717),\n",
       " ('он', 5531),\n",
       " ('на', 3594),\n",
       " ('она', 3418),\n",
       " ('с', 3324),\n",
       " ('я', 3147)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_tolstoy.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнивать употребимость конкретных слов в разных текстах в абсолютных числах неудобно. Нормализуем счётчики на размеры текстов. Так у нас получается вероятность слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 0.0412515051354502),\n",
       " ('—', 0.03466968573251524),\n",
       " ('в', 0.02271015528680326),\n",
       " ('не', 0.022580629686308185),\n",
       " ('что', 0.017015826109482712),\n",
       " ('я', 0.016200294550810016),\n",
       " ('он', 0.011940341467860861),\n",
       " ('с', 0.01160453435546622),\n",
       " ('на', 0.011316699687699385),\n",
       " ('но', 0.008793349100276801),\n",
       " ('вы', 0.008419164032179917),\n",
       " ('а', 0.008160112831189765),\n",
       " ('как', 0.00770917185168839),\n",
       " ('это', 0.006720939492355591),\n",
       " ('же', 0.0060541225120290905),\n",
       " ('его', 0.006025339045252407),\n",
       " ('так', 0.005440075220793176),\n",
       " ('к', 0.005392102776165371),\n",
       " ('всё', 0.004653327128897162),\n",
       " ('она', 0.004566976728567111)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas_dosoevsky = Counter({word:c/len(norm_dostoevsky) for word, c in vocab_dostoevsky.items()})\n",
    "probas_dosoevsky.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 0.04582131642490603),\n",
       " ('–', 0.04086045213210479),\n",
       " ('не', 0.023175593258914443),\n",
       " ('что', 0.020344877863165495),\n",
       " ('в', 0.020330653162684342),\n",
       " ('он', 0.019669204590310845),\n",
       " ('на', 0.012780893382313719),\n",
       " ('она', 0.012155006561143097),\n",
       " ('с', 0.01182072609983606),\n",
       " ('я', 0.011191283103545151),\n",
       " ('как', 0.009395414667799902),\n",
       " ('его', 0.009121589183537754),\n",
       " ('но', 0.009057578031372577),\n",
       " ('это', 0.007848478490474785),\n",
       " ('к', 0.007044782913289782),\n",
       " ('ее', 0.006390446691156859),\n",
       " ('все', 0.005889025999196305),\n",
       " ('было', 0.005871245123594866),\n",
       " ('сказал', 0.005007094569364974),\n",
       " ('так', 0.004985757518643248)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas_tolstoy = Counter({word:c/len(norm_tolstoy) for word, c in vocab_tolstoy.items()})\n",
    "probas_tolstoy.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти вероятности уже можно использовать, чтобы ответить на вопрос - кто из авторов сказал бы такую фразу?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = 'Все смешалось в доме облонских'\n",
    "\n",
    "prob = Counter({'tolstoy':0, 'dostoevsky':0})\n",
    "\n",
    "for word in normalize(phrase):\n",
    "    prob['dostoevsky'] += np.log(probas_dosoevsky.get(word, 0.00001))\n",
    "    prob['tolstoy'] += np.log(probas_tolstoy.get(word, 0.00001))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tolstoy', -39.40673502639174), ('dostoevsky', -40.89446321918831)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты получаются не очень точные. Возможно это из-за того, что мы считаем слова незовисымыми друг от друга. А это очевидно не так"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По-хорошему вероятность последовательности нужно расчитывать по формуле полной вероятности. Но у нас не очень большие тексты и мы не можем получить вероятности для длинных фраз (их просто может не быть в текстах). Поэтому мы воспользуемся предположением Маркова и будем учитывать только предыдущее слово."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы расчитать вероятность с таким предположением, нам достаточно найти количество вхождений для каждого биграмма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы у нас получились честные вероятности и можно было посчитать вероятность первого слова, нам нужно добавить тэг маркирующий начало предложений \\< start \\>\n",
    "\n",
    "Дальше мы попробуем сгенерировать текст, используя эти вероятности, и нам нужно будет когда-то остановится. Для этого добавим тэг окончания \\< end \\>\n",
    "\n",
    "Ну и поделим все на предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dostoevsky = [['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dostoevsky)]\n",
    "sentences_tolstoy = [['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(tolstoy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_dostoevsky = Counter()\n",
    "bigrams_dostoevsky = Counter()\n",
    "\n",
    "for sentence in sentences_dostoevsky:\n",
    "    unigrams_dostoevsky.update(sentence)\n",
    "    bigrams_dostoevsky.update(ngrammer(sentence))\n",
    "\n",
    "\n",
    "unigrams_tolstoy = Counter()\n",
    "bigrams_tolstoy = Counter()\n",
    "\n",
    "for sentence in sentences_tolstoy:\n",
    "    unigrams_tolstoy.update(sentence)\n",
    "    bigrams_tolstoy.update(ngrammer(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32474"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigrams_dostoevsky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start> –', 6632),\n",
       " ('– сказал', 1049),\n",
       " ('<start> он', 999),\n",
       " ('<start> и', 768),\n",
       " ('<start> она', 698),\n",
       " ('<start> но', 649),\n",
       " ('– я', 642),\n",
       " ('что он', 638),\n",
       " ('– сказала', 626),\n",
       " ('<start> я', 531)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_tolstoy.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы посчитать условную вероятность мы можем поделить количество вхождений на количество вхождений первого слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = 'Нужно быть действительно великим человеком, чтобы суметь устоять даже против здравого смысла.'\n",
    "# phrase = 'Все смешалось в доме облонских'\n",
    "prob = Counter()\n",
    "for ngram in ngrammer(['<start>'] + normalize(phrase) + ['<end>']):\n",
    "    word1, word2 = ngram.split()\n",
    "    if word1 in unigrams_dostoevsky and ngram in bigrams_dostoevsky:\n",
    "        prob['dostoevsky'] += np.log(bigrams_dostoevsky[ngram]/unigrams_dostoevsky[word1])\n",
    "    else:\n",
    "        prob['dostoevsky'] += -10\n",
    "    if word1 in unigrams_tolstoy and ngram in bigrams_tolstoy:\n",
    "        prob['tolstoy'] += np.log(bigrams_tolstoy[ngram]/unigrams_tolstoy[word1])\n",
    "    else:\n",
    "        prob['tolstoy'] += -10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tolstoy', -21.907500409441923), ('dostoevsky', -50.33963928073034)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работает получше. Мы воспользовались небольшим хаком - для слов или биграммов, которых не было у нас в словаре, прибавляли низкую вероятность. Исправить это по-нормальному - сложно, придется подробнее разбираться с вероятностями, сглаживаниями и заменой неизвестных слов. Если интрересно - в книге Журафского про это есть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблем с неизвестными словами у нас не будет, если мы будем пытаться сгенерировать новый текст. Давайте попробуем это сделать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_dostoevsky = np.zeros((len(unigrams_dostoevsky), \n",
    "                   len(unigrams_dostoevsky)))\n",
    "id2word_dostoevsky = list(unigrams_dostoevsky)\n",
    "word2id_dostoevsky = {word:i for i, word in enumerate(id2word_dostoevsky)}\n",
    "\n",
    "\n",
    "for ngram in bigrams_dostoevsky:\n",
    "    word1, word2 = ngram.split()\n",
    "    matrix_dostoevsky[word2id_dostoevsky[word1]][word2id_dostoevsky[word2]] =  (bigrams_dostoevsky[ngram]/\n",
    "                                                                     unigrams_dostoevsky[word1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим матрицу вероятностей перейти из 1 слов в другое\n",
    "matrix_tolstoy = np.zeros((len(unigrams_tolstoy), \n",
    "                   len(unigrams_tolstoy)))\n",
    "\n",
    "id2word_tolstoy = list(unigrams_tolstoy)\n",
    "word2id_tolstoy = {word:i for i, word in enumerate(id2word_tolstoy)}\n",
    "\n",
    "\n",
    "# вероятность расчитываем точно также\n",
    "for ngram in bigrams_tolstoy:\n",
    "    word1, word2 = ngram.split()\n",
    "    matrix_tolstoy[word2id_tolstoy[word1]][word2id_tolstoy[word2]] =  (bigrams_tolstoy[ngram]/\n",
    "                                                                     unigrams_tolstoy[word1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для генерации нам понадобится функция np.random.choice , которая выбирает случайный объект из заданных. Ещё в неё можно подать вероятность каждого объекта и она будет доставать по ним (не только максимальный по вероятности)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(matrix, id2word, word2id, n=100, start='<start>'):\n",
    "    text = []\n",
    "    current_idx = word2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx])\n",
    "        text.append(id2word[chosen])\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            chosen = word2id['<start>']\n",
    "        current_idx = chosen\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ограничусь лишь только есть каждый фунтов \n",
      " она уже твердо проговорила варвара петровна измучившая себя \n",
      " пошли как вы считаете поступок в голове» но он подсюсюкивает и те минуты еще не посмотрел николай всеволодович громко истину \n",
      " вы \n",
      " да вам просьбу подам посажу и все эти люди говорят философы начало эстетическое препровождение времени определены \n",
      " вы отжившего века и вы… вы знаете… в книгах которые вы ее испугом \n",
      " дело которое окружало юлию михайловну ее быстрого пронзительного взгляда вдруг поразила одна до которых уцелели теперь я застал арину прохоровну \n",
      " в стену \n",
      " я помню ваше пожертвование если он\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dostoevsky, id2word_dostoevsky, word2id_dostoevsky).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "– только кое-где в детстве еще мясистый лист бумаги исписали я кити \n",
      " – нет таких отношениях как расчет что же я сама простила бы занятая укладываньем своих свиданиях с вод князь говорит что понемногу припоминала не нужно \n",
      " она первая бросилась под угрозой не хочу \n",
      " она подняла к расставленному карточному столу и не читая приписала внизу слышны были слова не ошибаешься \n",
      " – продолжала решительно страстно хотя и он англичанину шорнику и на слово «немножко» \n",
      " интересно \n",
      " что будет доказано что приближается объяснение она пробудет еще этого дня но зато в петербурге и должно было еще нет\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_tolstoy, id2word_tolstoy, word2id_tolstoy).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Коллокации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коллокации - это устойчивые выражения, состоящие из двух и более слов. Устойчивые - значит, что они часто используются вместе. Также часто значения коллокации не могут быть выведены лишь из значений, входящих в них слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = [morph.parse(word.strip(punctuation))[0].normal_form for word \\\n",
    "                                                            in text.lower().split()]\n",
    "    normalized_text = [word for word in normalized_text if word]\n",
    "    return normalized_text\n",
    "\n",
    "\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработаем почти также, только теперь нам не нужны тэги начала и конца."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences_dostoevsky =  [normalize(text) for text in sent_tokenize(dostoevsky)]\n",
    "sentences_tolstoy =  [normalize(text) for text in sent_tokenize(tolstoy)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В списке много всяких чисел, однобуквеных слов и стоп-слов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим какие-нибудь ограничения к коду выше, чтобы биграммы получались почище."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('russian') + ['–'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, stops, n=2):\n",
    "    ngrams = []\n",
    "    tokens = [token for token in tokens if token not in stops]\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append('_'.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter()\n",
    "\n",
    "for text in sentences_tolstoy:\n",
    "    word_counter.update(ngrammer(text, n=2, stops=stops))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('алексей_александр', 577),\n",
       " ('степан_аркадьй', 549),\n",
       " ('сергей_иван', 294),\n",
       " ('дарья_александр', 209),\n",
       " ('весь_это', 171),\n",
       " ('сказать_левин', 155),\n",
       " ('сказать_степан', 114),\n",
       " ('лидий_иван', 104),\n",
       " ('сказать_вронский', 88),\n",
       " ('сказать_анна', 88),\n",
       " ('знать_это', 87),\n",
       " ('говорить_это', 86),\n",
       " ('агафья_михайло', 76),\n",
       " ('графиня_лидий', 74),\n",
       " ('сказать_кить', 62)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В списке есть коллокации, которые попали в список из-за того, что одно слово очень частотное и вообще встречается много в каких контекстах. Нас скорее интересуют случаи, когда слова в большинстве случаев встречаются вместе. Для этого мы можем придумать какие-нибудь формулы, учитывающие частоты слов по отдельности и общую частоту."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой способ - взять количество упоминаний биграма и поделить на сумму количеств упоминаний слов по отдельности. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такая формула называется PMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_simple(word_count_a, word_count_b, bigram_count, *args):\n",
    "    try:\n",
    "        score = bigram_count/((word_count_a+word_count_b))\n",
    "    \n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем функцию, которая будет делать счетчики для слов и биграммов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_stats(texts, stops):\n",
    "    ## соберем статистики для отдельных слов\n",
    "    ## и биграммов\n",
    "    \n",
    "    unigrams = Counter()\n",
    "    bigrams = Counter()\n",
    "    \n",
    "    for text in texts:\n",
    "        unigrams.update(text)\n",
    "        bigrams.update(ngrammer(text, stops, 2))\n",
    "    \n",
    "    return unigrams, bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И функцию, которая пройдет по всем биграммам и вычислит для них нашу метрику."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_bigrams(unigrams, bigrams, scorer, threshold=-100000, min_count=5):\n",
    "    ## посчитаем метрику для каждого нграмма\n",
    "    bigram2score = Counter()\n",
    "    len_vocab = len(unigrams)\n",
    "    for bigram in bigrams:\n",
    "        score = scorer(unigrams[bigram[0]], unigrams[bigram[1]], \n",
    "                       bigrams[bigram], len_vocab, min_count)\n",
    "        \n",
    "        ## если метрика выше порога, добавляем в словарик\n",
    "        if score > threshold:\n",
    "            bigram2score[bigram] = score\n",
    "    \n",
    "    return bigram2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams, bigrams = collect_stats(sentences_tolstoy, stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram2score = score_bigrams(unigrams, bigrams, scorer_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема с таким подходом в том, что на самом верху окажутся слова, которые встречают по одному разу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('человек_который', 61.0),\n",
       " ('знать_это', 43.5),\n",
       " ('первое_время', 32.0),\n",
       " ('это_мочь', 30.5),\n",
       " ('это_весь', 29.0),\n",
       " ('это_дело', 28.5),\n",
       " ('это_время', 26.0),\n",
       " ('графиня_лидий', 24.666666666666668),\n",
       " ('левин_чувствовать', 24.0),\n",
       " ('несмотря_весь', 23.0),\n",
       " ('дело_который', 22.0),\n",
       " ('это_самый', 22.0),\n",
       " ('левин_видеть', 22.0),\n",
       " ('левин_мочь', 19.0),\n",
       " ('левин_понять', 19.0)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram2score.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому можно немного переделать оценивающую функцию, добавив минимальное число вхождений для биграмма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer(word_count_a, word_count_b, bigram_count, len_vocab, min_count):\n",
    "    try:\n",
    "        score = ((bigram_count - min_count) / ((word_count_a + word_count_b)))\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram2score = score_bigrams(unigrams, bigrams, scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('человек_который', 56.0),\n",
       " ('знать_это', 41.0),\n",
       " ('это_мочь', 28.0),\n",
       " ('первое_время', 27.0),\n",
       " ('это_весь', 26.5),\n",
       " ('это_дело', 26.0),\n",
       " ('это_время', 23.5),\n",
       " ('графиня_лидий', 23.0),\n",
       " ('это_самый', 19.5),\n",
       " ('левин_чувствовать', 19.0),\n",
       " ('несмотря_весь', 18.0),\n",
       " ('дело_который', 17.0),\n",
       " ('левин_видеть', 17.0),\n",
       " ('это_сказать', 16.0),\n",
       " ('друг_друг', 15.333333333333334)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram2score.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В статье про Word2Vec для создания нграммов использовалась такая функция:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_w2v(word_count_a, word_count_b, bigram_count, len_vocab, min_count=10):\n",
    "\n",
    "    try:\n",
    "        score = ((bigram_count - min_count) / (word_count_a * word_count_b)) * len_vocab\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, отличается ли она от нашей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram2score = score_bigrams(unigrams, bigrams, scorer_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ребёнок_который', 34925.333333333336),\n",
       " ('решить_ехать', 14968.0),\n",
       " ('редко_бывать', 14968.0),\n",
       " ('решить_это', 9978.666666666666),\n",
       " ('решительно_знать', 9978.666666666666),\n",
       " ('ребёнок_мочь', 4989.333333333333),\n",
       " ('решительно_понимать', 4989.333333333333),\n",
       " ('mademoiselle_linon', 2494.6666666666665),\n",
       " ('железный_дорога', 2204.589147286822),\n",
       " ('женщина_который', 2204.589147286822),\n",
       " ('желать_это', 1624.4341085271317),\n",
       " ('сергей_иван', 1198.2692520775622),\n",
       " ('жена_посланник', 1160.3100775193798),\n",
       " ('брат_николай', 1116.0350877192982),\n",
       " ('женщина_это', 464.1240310077519)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram2score.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во всех случаях выше мы считали нграммами только слова, которые встречаются друг за другом. Но в нграммы часто можно ещё что-то вставить. Например, \"принять участие\" может превратиться в \"принять самое активное/непосредственное участие\". \n",
    "\n",
    "Чтобы отловить такие случаи можно считать нграммами слова, которые встречаются внутри какого-то окна. И считать по ним все те же метрики."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно ещё посчитать стандартное отклонение расстояния между двумя словами. Если оно маленькое - слова обычно стоят на строгой позиции по отношению друг к другу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def get_window_stats(texts, window=8):\n",
    "    \n",
    "    bigrams = defaultdict(list)\n",
    "    \n",
    "    # проходим окном по текстам \n",
    "    # берем первое слово и считаем его целевым\n",
    "    # проходим по остальным словам и их индексам\n",
    "    # добавляем в словарь пары (целевое слов, текущее слово)\n",
    "    # и добавляем индекс текущего в список этой пары\n",
    "    # так мы получаем (слово_1,слово_2):[1,2,1,1,3,2]\n",
    "    # порядок в этом случае учитывается - (слово_2, слово_1) - другая запись\n",
    "    for text in texts:\n",
    "        for i in range(len(text)-window):\n",
    "            words = list(enumerate(text[i:i+window]))\n",
    "            target = words[0][1]\n",
    "            for j, word in words[1:]:\n",
    "                bigrams[(target, word)].append(j)\n",
    "    \n",
    "    bigrams_stds = Counter()\n",
    "    for bigram in bigrams:\n",
    "        # выкидываем биграмы встретившиеся < 5 раз\n",
    "        if len(bigrams[bigram]) > 5:\n",
    "            bigrams_stds[bigram] = np.std(bigrams[bigram])\n",
    "    \n",
    "    return bigrams_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram2std = get_window_stats(sentences_dostoevsky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('софья', 'матвей'), 0.0),\n",
       " (('большой', 'дорога'), 0.0),\n",
       " (('ваш', 'превосходительство'), 0.0),\n",
       " (('знаешь', 'ли'), 0.0),\n",
       " (('общий', 'дело'), 0.0),\n",
       " (('арин', 'прохор'), 0.0),\n",
       " (('вслед', 'за'), 0.0),\n",
       " (('семён', 'яков'), 0.0),\n",
       " (('воротиться', 'домой'), 0.0),\n",
       " (('из', 'сила'), 0.0),\n",
       " (('cher', 'он'), 0.0),\n",
       " (('алексей', 'егор'), 0.0),\n",
       " (('господин', 'кармазин'), 0.0),\n",
       " (('чуть', 'ли'), 0.0),\n",
       " (('артемий', 'павло'), 0.0),\n",
       " (('замечать', 'что'), 0.0),\n",
       " (('какой-то', 'особенный'), 0.0),\n",
       " (('с', 'постель'), 0.0),\n",
       " (('ради', 'бог'), 0.0)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram2std.most_common()[:-20:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно применять расширить размер нграмма, а можно последовательно преобразовывать один и тот же текст, на каждом шагу собирая новые биграммы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишием такую функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_text(text, bigram2score):\n",
    "    new_text = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < (len(text)-1):\n",
    "        bigram = '_'.join((text[i], text[i+1]))\n",
    "        if bigram in bigram2score:\n",
    "            new_text.append(bigram)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_text.append(text[i])\n",
    "            i += 1\n",
    "    else:\n",
    "        if i == (len(text)-1):\n",
    "            new_text.append(text[i])\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams, bigrams = collect_stats(sentences_dostoevsky, stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram2score = score_bigrams(unigrams, bigrams, scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dostoevsky_2 = [bigram_text(sent, bigram2score) for sent in sentences_dostoevsky]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams, bigrams = collect_stats(sentences_dostoevsky_2, stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram2score = score_bigrams(unigrams, bigrams, scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dostoevsky_3 = [bigram_text(sent, trigram2score) for sent in sentences_dostoevsky_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['домовый', 'ли', 'хоронить_ведьма_ль_замуж', 'выдавать']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_dostoevsky_3[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По этой ссылке можно прочитать про другие метрики.\n",
    "\n",
    "http://www.scielo.org.mx/scielo.php?script=sci_arttext&pid=S1405-55462016000300327#t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Все готовое"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Писать все это самому конечно не обязательно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удобно пользоваться phraser из gensim'а. Он собирает статистику по корпусу, а затем склеивает слова в биграммы. Так как мы сделали выше. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собираем статистики\n",
    "ph = gensim.models.Phrases(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразовывать можно и через ph, но так быстрее \n",
    "p = gensim.models.phrases.Phraser(ph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию там используется метрики из статьи про ворд2век и ещё есть нормализованные pmi.\n",
    "Если не нравятся функции оценки, то ему можно подать любую другую функцию. Интерфейс у функции там почти точно такой же как и у наших."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собираем статистики по уже забиграммленному тексту\n",
    "ph2 = gensim.models.Phrases(p[texts])\n",
    "p2 = gensim.models.phrases.Phraser(ph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['многие',\n",
       " 'интересоваться',\n",
       " 'зачем',\n",
       " 'нужный',\n",
       " '«яблоку»',\n",
       " 'молодёжный',\n",
       " 'фракция',\n",
       " 'основной_задача',\n",
       " '«молодёжный',\n",
       " '«яблока»',\n",
       " 'являться',\n",
       " 'привлечение',\n",
       " 'молодая_человек',\n",
       " 'к',\n",
       " 'участие',\n",
       " 'в',\n",
       " 'выборы',\n",
       " 'и',\n",
       " 'деятельность',\n",
       " 'партия']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2[p[texts[0]]][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и наконец нграммы есть в нлтк. Тут больше метрик, но преборазователь слов в нграммы нужно написать самому."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder2 = BigramCollocationFinder.from_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder3 = TrigramCollocationFinder.from_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('один', 'из'),\n",
       " ('тот', 'что'),\n",
       " ('а', 'также'),\n",
       " ('при', 'это'),\n",
       " ('2017', 'год'),\n",
       " ('не', 'только'),\n",
       " ('точка', 'зрение'),\n",
       " ('то', 'есть'),\n",
       " ('сей', 'пора'),\n",
       " ('тот', 'число'),\n",
       " ('куб', 'метр'),\n",
       " ('владимир', 'путин'),\n",
       " ('2016', 'год'),\n",
       " ('тот', 'же'),\n",
       " ('потому', 'что'),\n",
       " ('миллиард', 'доллар'),\n",
       " ('о', 'тот'),\n",
       " ('прежде', 'всего'),\n",
       " ('кроме', 'тот'),\n",
       " ('до', 'сей')]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder2.nbest(bigram_measures.likelihood_ratio, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1947–2001»', 'monterey', 'ca'),\n",
       " ('50-летие', 'rolling', 'stones'),\n",
       " ('acs', 'nano', 'letters'),\n",
       " ('areva', 'edf', 'alstom'),\n",
       " ('armored', 'multi-purpose', 'vehicles'),\n",
       " ('atr', 'ленур', 'ислям'),\n",
       " ('bad', 'can', 'it'),\n",
       " ('bourgeois', '«эпатировать', 'буржуа»'),\n",
       " ('bundesanstalt', 'fuer', 'geowissenschaften'),\n",
       " ('can', 'it', 'be'),\n",
       " ('charge', 'ion', 'battery'),\n",
       " ('citizens', '1947–2001»', 'monterey'),\n",
       " ('commitment', 'competence', 'consensus'),\n",
       " ('corriere', 'della', 'sera'),\n",
       " ('della', 'sera', 'папа-на-покой'),\n",
       " ('diyanet', 'isleri', 'turk-islam'),\n",
       " ('dux', 'recording', 'producers'),\n",
       " ('edf', 'alstom', 'schneider'),\n",
       " ('egf', 'gazprom', 'monitor'),\n",
       " ('espanola', 'чть', 'прад')]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder3.nbest(trigram_measures.pmi, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
