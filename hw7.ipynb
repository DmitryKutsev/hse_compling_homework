{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled23.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMjNOwYdJv9GgIXsl1mcPfJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitryKutsev/hse_compling_homework/blob/master/hw7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEfNUCBFBGjx",
        "colab_type": "code",
        "outputId": "926fc377-966e-41fa-bb9b-0bedfe808f98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.1/lenta-ru-news.csv.bz2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-06 07:47:03--  https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.1/lenta-ru-news.csv.bz2\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/87156914/619f9f00-1e96-11ea-946e-dac89df8aced?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200306%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200306T074708Z&X-Amz-Expires=300&X-Amz-Signature=1857425174bdfe63bb45a9e62012458f2419fe846b03d4b6b45f5f8b72b08568&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.bz2&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-03-06 07:47:08--  https://github-production-release-asset-2e65be.s3.amazonaws.com/87156914/619f9f00-1e96-11ea-946e-dac89df8aced?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200306%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200306T074708Z&X-Amz-Expires=300&X-Amz-Signature=1857425174bdfe63bb45a9e62012458f2419fe846b03d4b6b45f5f8b72b08568&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.bz2&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.184.187\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.184.187|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 346031300 (330M) [application/octet-stream]\n",
            "Saving to: ‘lenta-ru-news.csv.bz2’\n",
            "\n",
            "lenta-ru-news.csv.b 100%[===================>] 330.00M  48.4MB/s    in 6.6s    \n",
            "\n",
            "2020-03-06 07:47:15 (50.3 MB/s) - ‘lenta-ru-news.csv.bz2’ saved [346031300/346031300]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjFg8bpAssV7",
        "colab_type": "code",
        "outputId": "3a9b8ea6-480a-4b47-c312-4e106ab9417a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!wget https://rusvectores.org/static/models/rusvectores2/news_mystem_skipgram_1000_20_2015.bin.gz\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-06 07:47:16--  https://rusvectores.org/static/models/rusvectores2/news_mystem_skipgram_1000_20_2015.bin.gz\n",
            "Resolving rusvectores.org (rusvectores.org)... 116.203.104.23\n",
            "Connecting to rusvectores.org (rusvectores.org)|116.203.104.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 549952184 (524M) [application/x-gzip]\n",
            "Saving to: ‘news_mystem_skipgram_1000_20_2015.bin.gz’\n",
            "\n",
            "news_mystem_skipgra 100%[===================>] 524.47M  28.7MB/s    in 19s     \n",
            "\n",
            "2020-03-06 07:47:37 (27.1 MB/s) - ‘news_mystem_skipgram_1000_20_2015.bin.gz’ saved [549952184/549952184]\n",
            "\n",
            "lenta-ru-news.csv.bz2  news_mystem_skipgram_1000_20_2015.bin.gz  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WPvPEgeb98S",
        "colab_type": "code",
        "outputId": "648e9f2e-1b73-4205-8f59-0eb9b3bb955b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "#!rm model.bin model.txt \n",
        "# !wget http://vectors.nlpl.eu/repository/20/186.zip\n",
        "!wget http://vectors.nlpl.eu/repository/20/185.zip\n",
        "!unzip 185.zip\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-06 07:47:39--  http://vectors.nlpl.eu/repository/20/185.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 639268566 (610M) [application/zip]\n",
            "Saving to: ‘185.zip’\n",
            "\n",
            "185.zip             100%[===================>] 609.65M  23.1MB/s    in 30s     \n",
            "\n",
            "2020-03-06 07:48:10 (20.1 MB/s) - ‘185.zip’ saved [639268566/639268566]\n",
            "\n",
            "Archive:  185.zip\n",
            "  inflating: meta.json               \n",
            "  inflating: model.bin               \n",
            "  inflating: model.txt               \n",
            "  inflating: README                  \n",
            "185.zip\t\t       model.bin\t\t\t\t README\n",
            "lenta-ru-news.csv.bz2  model.txt\t\t\t\t sample_data\n",
            "meta.json\t       news_mystem_skipgram_1000_20_2015.bin.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrGWKppNE97u",
        "colab_type": "code",
        "outputId": "21de84c1-092e-4b26-b77f-6c566637b063",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "!wget https://github.com/DmitryKutsev/hse_compling_homework/blob/master/paraphraser_gold.zip?raw=true\n",
        "!wget https://github.com/DmitryKutsev/hse_compling_homework/blob/master/data_paraphraser_norm.csv?raw=true\n",
        "!mv paraphraser_gold.zip?raw=true paraphraser_gold.zip\n",
        "!mv data_paraphraser_norm.csv?raw=true data_paraphraser_norm.csv\n",
        "!unzip paraphraser_gold.zip\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-06 07:48:26--  https://github.com/DmitryKutsev/hse_compling_homework/blob/master/paraphraser_gold.zip?raw=true\n",
            "Resolving github.com (github.com)... 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/DmitryKutsev/hse_compling_homework/raw/master/paraphraser_gold.zip [following]\n",
            "--2020-03-06 07:48:26--  https://github.com/DmitryKutsev/hse_compling_homework/raw/master/paraphraser_gold.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/DmitryKutsev/hse_compling_homework/master/paraphraser_gold.zip [following]\n",
            "--2020-03-06 07:48:27--  https://raw.githubusercontent.com/DmitryKutsev/hse_compling_homework/master/paraphraser_gold.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 118720 (116K) [application/zip]\n",
            "Saving to: ‘paraphraser_gold.zip?raw=true’\n",
            "\n",
            "\r          paraphras   0%[                    ]       0  --.-KB/s               \rparaphraser_gold.zi 100%[===================>] 115.94K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-03-06 07:48:27 (3.06 MB/s) - ‘paraphraser_gold.zip?raw=true’ saved [118720/118720]\n",
            "\n",
            "--2020-03-06 07:48:28--  https://github.com/DmitryKutsev/hse_compling_homework/blob/master/data_paraphraser_norm.csv?raw=true\n",
            "Resolving github.com (github.com)... 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/DmitryKutsev/hse_compling_homework/raw/master/data_paraphraser_norm.csv [following]\n",
            "--2020-03-06 07:48:28--  https://github.com/DmitryKutsev/hse_compling_homework/raw/master/data_paraphraser_norm.csv\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/DmitryKutsev/hse_compling_homework/master/data_paraphraser_norm.csv [following]\n",
            "--2020-03-06 07:48:28--  https://raw.githubusercontent.com/DmitryKutsev/hse_compling_homework/master/data_paraphraser_norm.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3575140 (3.4M) [text/plain]\n",
            "Saving to: ‘data_paraphraser_norm.csv?raw=true’\n",
            "\n",
            "data_paraphraser_no 100%[===================>]   3.41M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-03-06 07:48:28 (32.4 MB/s) - ‘data_paraphraser_norm.csv?raw=true’ saved [3575140/3575140]\n",
            "\n",
            "Archive:  paraphraser_gold.zip\n",
            "  inflating: paraphrases_gold.xml    \n",
            "185.zip\t\t\t   news_mystem_skipgram_1000_20_2015.bin.gz\n",
            "data_paraphraser_norm.csv  paraphraser_gold.zip\n",
            "lenta-ru-news.csv.bz2\t   paraphrases_gold.xml\n",
            "meta.json\t\t   README\n",
            "model.bin\t\t   sample_data\n",
            "model.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMeuIrD5if_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!bzip2 -d lenta-ru-news.csv.bz2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWZRDAhDimQk",
        "colab_type": "code",
        "outputId": "6269891b-28b8-49db-cb39-6ec021dbf5d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!ls\n",
        "!pip install pymorphy2[fast]\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "185.zip\t\t\t   news_mystem_skipgram_1000_20_2015.bin.gz\n",
            "corpus.txt\t\t   out.pkl\n",
            "data_paraphraser_norm.csv  paraphraser_gold.zip\n",
            "lenta-ru-news.csv\t   paraphrases_gold.xml\n",
            "meta.json\t\t   README\n",
            "model.bin\t\t   sample_data\n",
            "model.txt\n",
            "Requirement already satisfied: pymorphy2[fast] in /usr/local/lib/python3.6/dist-packages (0.8)\n",
            "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from pymorphy2[fast]) (2.4.393442.3710985)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2[fast]) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7 in /usr/local/lib/python3.6/dist-packages (from pymorphy2[fast]) (0.7.2)\n",
            "Requirement already satisfied: DAWG>=0.7.3; extra == \"fast\" in /usr/local/lib/python3.6/dist-packages (from pymorphy2[fast]) (0.8.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zX7M6MAkoQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from collections import Counter,defaultdict\n",
        "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
        "from string import punctuation\n",
        "import os\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from lxml import html\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import gensim\n",
        "import numpy as np\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "morph = MorphAnalyzer()\n",
        "punct = punctuation+'«»—…“”*№–,'\n",
        "stops = set(stopwords.words('russian'))\n",
        "\n",
        "def normalize(text):\n",
        "    \n",
        "    words = [word.strip(punct) for word in text.lower().split()]\n",
        "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "def tokenize(text):\n",
        "    \n",
        "    words = [word.strip(punct) for word in text.lower().split()]\n",
        "\n",
        "    return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHRVBKC9niKF",
        "colab_type": "code",
        "outputId": "d143d0cd-47fd-48db-9192-50b3da31b377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_handler = open('lenta-ru-news.csv', 'r')\n",
        "data = data_handler.read()\n",
        "print(len(data))\n",
        "data = data[:6000000]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1172327461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWmUFssTRnoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_norm = [normalize(text) for text in data.split('.')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5J3zqnP8Rnmh",
        "colab_type": "code",
        "outputId": "d325fe4f-8e16-46c0-e739-73223ec7fa35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "data_norm = [text for text in data_norm if text]\n",
        "print(len(data_norm))\n",
        "data_norm[:6]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['url,title,text,topic,tags,date https://lenta',\n",
              " 'ru/news/1914/09/16/hungarnn/,1914',\n",
              " 'русский войско вступить предел венгрия бой сопоцкина друскеник закончиться отступление германец',\n",
              " 'неприятель приблизиться север осовца начать артиллерийский борьба крепость',\n",
              " 'артиллерийский бой принимать участие тяжёлый калибр',\n",
              " 'ранний утро 14 сентябрь огонь достигнуть значительный напряжение']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA4i38sSZ_zP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import adagram\n",
        "from lxml import html\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from string import punctuation\n",
        "import json, os\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "morph = MorphAnalyzer()\n",
        "punct = punctuation+'«»—…“”*№–'\n",
        "stops = set(stopwords.words('russian'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQrQbXxJZ_12",
        "colab_type": "code",
        "outputId": "18fd9222-53a8-4814-a81d-7aed40f50e12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "!pip install Cython numpy\n",
        "!pip install git+https://github.com/lopuhin/python-adagram.git"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.5)\n",
            "Collecting git+https://github.com/lopuhin/python-adagram.git\n",
            "  Cloning https://github.com/lopuhin/python-adagram.git to /tmp/pip-req-build-nvd7jpme\n",
            "  Running command git clone -q https://github.com/lopuhin/python-adagram.git /tmp/pip-req-build-nvd7jpme\n",
            "Requirement already satisfied (use --upgrade to upgrade): adagram==0.0.1 from git+https://github.com/lopuhin/python-adagram.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (0.29.15)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (1.12.0)\n",
            "Building wheels for collected packages: adagram\n",
            "  Building wheel for adagram (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adagram: filename=adagram-0.0.1-cp36-cp36m-linux_x86_64.whl size=464650 sha256=d4c1e0edb0d671e156cba5ba4a40753554507bc917e6f9519828e45843ef7fee\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hqojdqgm/wheels/11/0f/46/f5df96670df8f7973b4c2311ffc9b02e435a7bd3207f992c4d\n",
            "Successfully built adagram\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EGOq8lufBAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(text):\n",
        "    \n",
        "    words = [word.strip(punct) for word in text.lower().split() if word and word not in stops]\n",
        "    words = [word for word in words if word]\n",
        "\n",
        "    return words\n",
        "\n",
        "def normalize(text):\n",
        "    \n",
        "    words = tokenize(text)\n",
        "    words = [morph.parse(word)[0].normal_form for word in words if word]\n",
        "\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F05KF0jjaBgN",
        "colab_type": "code",
        "outputId": "2a994f6c-efd9-4d8b-f842-c8d01e75fac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_handler = open('lenta-ru-news.csv', 'r')\n",
        "data = data_handler.read()\n",
        "print(len(data))\n",
        "corpus = data[:6000000]\n",
        "#corpus = open('corpus_ng.txt').read()\n",
        "corpus = normalize(corpus[2:])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1172327461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9kk5rT1aBik",
        "colab_type": "code",
        "outputId": "fbdd0384-1a59-4c76-8e21-ee8cc00fb5fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "corpus[:100]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['l,title,text,topic,tags,date',\n",
              " 'https://lenta.ru/news/1914/09/16/hungarnn/,1914',\n",
              " 'русский',\n",
              " 'войско',\n",
              " 'вступить',\n",
              " 'предел',\n",
              " 'венгрия',\n",
              " 'бой',\n",
              " 'сопоцкина',\n",
              " 'друскеник',\n",
              " 'закончиться',\n",
              " 'отступление',\n",
              " 'германец',\n",
              " 'неприятель',\n",
              " 'приблизиться',\n",
              " 'север',\n",
              " 'осовца',\n",
              " 'начать',\n",
              " 'артиллерийский',\n",
              " 'борьба',\n",
              " 'крепость',\n",
              " 'артиллерийский',\n",
              " 'бой',\n",
              " 'принимать',\n",
              " 'участие',\n",
              " 'тяжёлый',\n",
              " 'калибр',\n",
              " 'ранний',\n",
              " 'утро',\n",
              " '14',\n",
              " 'сентябрь',\n",
              " 'огонь',\n",
              " 'достигнуть',\n",
              " 'значительный',\n",
              " 'напряжение',\n",
              " 'попытка',\n",
              " 'германский',\n",
              " 'пехота',\n",
              " 'пробиться',\n",
              " 'близкий',\n",
              " 'крепость',\n",
              " 'отразить',\n",
              " 'галиция',\n",
              " 'занять',\n",
              " 'дембица',\n",
              " 'большой',\n",
              " 'колонна',\n",
              " 'отступать',\n",
              " 'шоссе',\n",
              " 'перемышль',\n",
              " 'санок',\n",
              " 'обстреливаться',\n",
              " 'высота',\n",
              " 'наш',\n",
              " 'батарея',\n",
              " 'бежать',\n",
              " 'бросить',\n",
              " 'парка',\n",
              " 'обоз',\n",
              " 'автомобиль',\n",
              " 'вылазка',\n",
              " 'гарнизон',\n",
              " 'перемышль',\n",
              " 'оставаться',\n",
              " 'безуспешный',\n",
              " 'продолжаться',\n",
              " 'отступление',\n",
              " 'австриец',\n",
              " 'обнаруживаться',\n",
              " 'полный',\n",
              " 'перемешивание',\n",
              " 'часть',\n",
              " 'захватываться',\n",
              " 'новое',\n",
              " 'партия',\n",
              " 'пленный',\n",
              " 'орудие',\n",
              " 'прочий',\n",
              " 'материальный',\n",
              " 'часть',\n",
              " 'перевал',\n",
              " 'ужок',\n",
              " 'разбить',\n",
              " 'неприятельский',\n",
              " 'отряд',\n",
              " 'взять',\n",
              " 'артиллерия',\n",
              " 'пленный',\n",
              " 'и',\n",
              " 'продолжать',\n",
              " 'преследовать',\n",
              " 'вступить',\n",
              " 'предел',\n",
              " 'венгрия',\n",
              " 'русский',\n",
              " 'инвалид',\n",
              " '16',\n",
              " 'сентябрь',\n",
              " '1914',\n",
              " 'года.\",библиотека,первать']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqdIeOX6aBk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open('corpus.txt', 'w')\n",
        "f.write(' '.join(corpus))\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCmHQYsMjGLx",
        "colab_type": "code",
        "outputId": "b156b01f-ffac-419b-993d-c36ea7d8e6be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!adagram-train corpus.txt out.pkl"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] 2020-03-06 09:27:12,015 Building dictionary...\n",
            "[INFO] 2020-03-06 09:27:19,786 Done! 6191 words.\n",
            "[INFO] 2020-03-06 09:27:34,061 13.21% -7.7208 0.0217 1.3/2.0 4.55 kwords/sec\n",
            "[INFO] 2020-03-06 09:27:40,474 26.41% -7.6413 0.0184 1.3/2.0 9.98 kwords/sec\n",
            "[INFO] 2020-03-06 09:27:46,916 39.62% -7.5668 0.0151 1.3/2.0 9.93 kwords/sec\n",
            "[INFO] 2020-03-06 09:27:53,314 52.83% -7.5011 0.0118 1.3/2.0 10.00 kwords/sec\n",
            "[INFO] 2020-03-06 09:27:59,718 66.03% -7.4434 0.0085 1.3/2.0 9.99 kwords/sec\n",
            "[INFO] 2020-03-06 09:28:06,079 79.24% -7.3931 0.0052 1.3/2.0 10.06 kwords/sec\n",
            "[INFO] 2020-03-06 09:28:12,336 92.45% -7.3501 0.0019 1.3/3.0 10.23 kwords/sec\n",
            "[INFO] 2020-03-06 09:28:15,880 100.00% -7.3289 0.0000 1.3/3.0 10.33 kwords/sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9raObW3jGOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vm = adagram.VectorModel.load(\"out.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JOm9cyLjGQ4",
        "colab_type": "code",
        "outputId": "ce92d950-50c3-4018-c1ab-8b48abc79e06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vm.word_sense_probs('бросить')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 0.9969787370642881), (1, 0.0027466025680557555)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGlfUu9YjGTh",
        "colab_type": "code",
        "outputId": "261916b6-13b8-4533-e471-af5938f632b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "vm.sense_neighbors('бросить', 0)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('гранат', 0, 0.76619744),\n",
              " ('филиппина', 0, 0.7201713),\n",
              " ('серб', 0, 0.70346206),\n",
              " ('столкнуться', 0, 0.69462293),\n",
              " ('окно', 0, 0.6935314),\n",
              " ('напротив', 0, 0.68303764),\n",
              " ('перекрытие', 0, 0.6785619),\n",
              " ('въехать', 0, 0.67713535),\n",
              " ('пистолет', 0, 0.6676827),\n",
              " ('разрушить', 0, 0.6670119)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTRD97kKjGWf",
        "colab_type": "code",
        "outputId": "2f1fd0f2-c44a-4d5e-d499-9a71cc8afc5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "vm.sense_neighbors('бросить', 1)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('новолакский', 0, 0.32016194),\n",
              " ('отпечаток', 0, 0.30431855),\n",
              " ('research', 0, 0.3006535),\n",
              " ('антимонопольный', 0, 0.30006462),\n",
              " ('сталин', 0, 0.28216752),\n",
              " ('авторитет', 0, 0.2814581),\n",
              " ('дели', 0, 0.26697457),\n",
              " ('заведомо', 0, 0.26637763),\n",
              " ('предыдущий', 0, 0.26423785),\n",
              " ('швейцария', 0, 0.26070237)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-DaP9x3cNSe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "cb71dcbd-9427-42ac-9b02-403a163731fa"
      },
      "source": [
        "vm.sense_vector('бросить', 0)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.12728064, -0.24252154,  0.19736363, -0.21332999,  0.00376238,\n",
              "        0.04505604,  0.00939871, -0.20406424, -0.03589703, -0.13567336,\n",
              "        0.03697933,  0.07658108, -0.11366298,  0.06683453, -0.04584728,\n",
              "       -0.05854072, -0.05402713,  0.3099658 ,  0.07953191,  0.04681319,\n",
              "        0.18225683, -0.01545188, -0.1948851 ,  0.14111505, -0.12767914,\n",
              "        0.02797218, -0.01596583, -0.073487  ,  0.0946255 ,  0.0613209 ,\n",
              "        0.01916189,  0.05813255, -0.06144684, -0.04836461,  0.18351555,\n",
              "       -0.0672642 ,  0.23717968, -0.05271492,  0.14234565,  0.02536315,\n",
              "        0.04920638,  0.04662352, -0.02188607, -0.12425616, -0.10724043,\n",
              "       -0.17466864, -0.04593752, -0.13489857,  0.01909587,  0.29966295,\n",
              "        0.06455539, -0.11408238,  0.11029915, -0.19302393,  0.19117126,\n",
              "       -0.03762378,  0.08525593,  0.14489605, -0.02354068,  0.09468605,\n",
              "       -0.03597291, -0.28984064, -0.02315626, -0.21253538,  0.10444414,\n",
              "        0.08671758, -0.26631257, -0.12820728,  0.18002582, -0.01879579,\n",
              "       -0.24836442, -0.01852936, -0.0820653 , -0.23898788, -0.07541154,\n",
              "       -0.0560839 ,  0.19263916,  0.03558422, -0.00794351, -0.11479044,\n",
              "        0.02245609, -0.31416965,  0.10018369, -0.06921475, -0.07993121,\n",
              "        0.07467982, -0.15608126, -0.0969466 ,  0.037654  ,  0.08960255,\n",
              "        0.2206405 ,  0.03544011, -0.09847306,  0.1372221 ,  0.1470776 ,\n",
              "        0.06359417,  0.04548925,  0.01111445,  0.01050593, -0.1906653 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy256X86jGZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ambiguous = []\n",
        "for word in vm.dictionary.id2word:\n",
        "    probs = vm.word_sense_probs(word)\n",
        "    if len(probs) > 1 and probs[0][1] < 0.8: # второе условие нужно, чтоб выкинуть слова с не очень сильным вторым значением\n",
        "        ambiguous.append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZN8KU0Lhw2B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26e50475-7dde-4504-8e8e-7c9be1a6b84f"
      },
      "source": [
        "ambiguous"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['группа']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOia7sHkaAAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "corpus_xml = html.fromstring(open('data_paraphraser_norm.csv', 'rb').read())\n",
        "texts_1 = []\n",
        "texts_2 = []\n",
        "classes = []\n",
        "\n",
        "for p in corpus_xml.xpath('//paraphrase'):\n",
        "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
        "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
        "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
        "    \n",
        "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7mOua0t1_pB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
        "data['text_2_norm'] = data['text_2'].apply(normalize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ca1nBid1_yH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = [0,1,2,3,4,5,6,7,8,9]\n",
        "\n",
        "def get_words_in_context(words, window=3):\n",
        "  main_list = []\n",
        "  for word in words:\n",
        "    local_list = []\n",
        "    if word < window:\n",
        "      local_list = [word, words[:window+word+1]]\n",
        "      local_list[1].remove(word)\n",
        "      main_list.append(local_list)\n",
        "    else:           \n",
        "      local_list = [word, words[word - window:word + window+1]]\n",
        "      local_list[1].remove(word)\n",
        "      main_list.append(local_list)\n",
        "\n",
        "  return main_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IcOxaLu1_4N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c7ad5f70-f462-4f2d-81c7-51fb981ba253"
      },
      "source": [
        "get_words_in_context(words, window=3)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, [1, 2, 3]],\n",
              " [1, [0, 2, 3, 4]],\n",
              " [2, [0, 1, 3, 4, 5]],\n",
              " [3, [0, 1, 2, 4, 5, 6]],\n",
              " [4, [1, 2, 3, 5, 6, 7]],\n",
              " [5, [2, 3, 4, 6, 7, 8]],\n",
              " [6, [3, 4, 5, 7, 8, 9]],\n",
              " [7, [4, 5, 6, 8, 9]],\n",
              " [8, [5, 6, 7, 9]],\n",
              " [9, [6, 7, 8]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YulwrrDR1_64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embedding_adagram(text, model, window, dim):\n",
        "    \n",
        "    word2context = get_words_in_context(text, window)\n",
        "    \n",
        "    \n",
        "    vectors = np.zeros((len(word2context), dim))\n",
        "    \n",
        "    for i, (word, context) in enumerate(word2context):\n",
        "        \n",
        "        try:\n",
        "            ### ваш код \n",
        "            vectors[i] = v\n",
        "        \n",
        "        except (KeyError, ValueError):\n",
        "            continue\n",
        "    \n",
        "    if vectors.any():\n",
        "        vector = np.average(vectors, axis=0)\n",
        "    else:\n",
        "        vector = np.zeros((dim))\n",
        "    \n",
        "    return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcsTPx071_9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB7vE3wB2AAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSPH17002ADH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA25QXuD2AGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqanL3ybRnjQ",
        "colab_type": "code",
        "outputId": "bdb22328-e39f-4869-9122-6863ce957ea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "my_w2v = gensim.models.Word2Vec([text.split() for text in data_norm], size=50, sg=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-94fcdc685d52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_norm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try an iterator.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m             self.train(\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m             trim_rule=trim_rule, **kwargs)\n\u001b[1;32m    942\u001b[0m         \u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'memory'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_retained_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_vocab_from_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mprepare_weights\u001b[0;34m(self, hs, negative, wv, update, vocabulary)\u001b[0m\n\u001b[1;32m   1820\u001b[0m         \u001b[0;31m# set initial input/projection and hidden weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mreset_weights\u001b[0;34m(self, hs, negative, wv)\u001b[0m\n\u001b[1;32m   1837\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m             \u001b[0;31m# construct deterministic seed from word AND seed argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseeded_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mseeded_vector\u001b[0;34m(self, seed_string, vector_size)\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0;34m\"\"\"Get a random vector (but deterministic by seed_string).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m         \u001b[0;31m# Note: built-in hash() may vary by Python version or even (in Py3.x) per launch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1829\u001b[0;31m         \u001b[0monce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhashfxn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xffffffff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1830\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0monce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmt19937.pyx\u001b[0m in \u001b[0;36mnumpy.random.mt19937.MT19937.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNTHoPBj4OD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_w2v.most_similar('чечня')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UjOP9Mq4b54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dim = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nepsHLlxfyW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rusv_v2w = gensim.models.KeyedVectors.load_word2vec_format('model.bin', binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K80JkCtYfy0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rusv_v2w.most_similar('путин_NOUN')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-7ficwpVPDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_df_embedding(text, model, dim):\n",
        "    text = text.split()\n",
        "    words = Counter(text)\n",
        "    total = len(text)\n",
        "    vectors = np.zeros((len(words), dim))   \n",
        "    for i, word in enumerate(words):\n",
        "        try:\n",
        "            v = model[word]\n",
        "        except (KeyError, ValueError) as errs:\n",
        "            #print(errs)\n",
        "            continue\n",
        "        vectors[i] = v*(words[word]/total)\n",
        "\n",
        "    if vectors.any():\n",
        "        vector = np.average(vectors, axis=0)\n",
        "    else:\n",
        "        vector = np.zeros((dim))\n",
        "    \n",
        "    return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwMRV4iJXrxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_xml = html.fromstring(open('paraphrases_gold.xml', 'rb').read())\n",
        "texts_1 = []\n",
        "texts_2 = []\n",
        "classes = []\n",
        "\n",
        "for p in corpus_xml.xpath('//paraphrase'):\n",
        "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
        "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
        "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
        "    \n",
        "df_data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xemqbs8R6R0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_data['text_1_norm'] = df_data['text_1'].apply(normalize)\n",
        "df_data['text_2_norm'] = df_data['text_2'].apply(normalize)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foeHzuCyYbeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "dim = 50\n",
        "df_data['text_1_notnorm'] = df_data['text_1'].apply(tokenize)\n",
        "df_data['text_2_notnorm'] = df_data['text_2'].apply(tokenize)\n",
        "\n",
        "X_text_1_ft = np.zeros((len(df_data['text_1_notnorm']), dim))\n",
        "X_text_2_ft = np.zeros((len(df_data['text_2_notnorm']), dim))\n",
        "\n",
        "for i, text in enumerate(df_data['text_1_notnorm'].values):\n",
        "    X_text_1_ft[i] = get_df_embedding(text, my_w2v, dim)\n",
        "    \n",
        "for i, text in enumerate(df_data['text_2_notnorm'].values):\n",
        "    X_text_2_ft[i] = get_df_embedding(text, my_w2v, dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rb8a07_TK9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_data.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf1GHgTxi4_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_text_ft = np.concatenate([X_text_1_ft, X_text_2_ft], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ar1wE4jjJT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Обученная модель w2v\n",
        "y = df_data['label'].values\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(X_text_ft, y,random_state=1)\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\n",
        "                             class_weight='balanced')\n",
        "clf.fit(train_X, train_y)\n",
        "preds = clf.predict(valid_X)\n",
        "print(classification_report(valid_y, preds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2zSaWSFv_TU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(clf, X_text_ft, y, cv=5, scoring = 'f1_micro')\n",
        "scores.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXr_4FbN2fDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X, valid_X, train_y, valid_y = train_test_split(X_text_ft, y,random_state=1)\n",
        "clf = LogisticRegression(C=1000)\n",
        "clf.fit(train_X, train_y)\n",
        "preds = clf.predict(valid_X)\n",
        "print(classification_report(valid_y, preds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgqVsdwZr1Kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = cross_val_score(clf, X_text_ft, y, cv=5, scoring = 'f1_micro')\n",
        "scores.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1SnTUYfNlcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_csv = pd.read_csv('data_paraphraser_norm.csv', error_bad_lines=False)\n",
        "#модель с русвекторес w2v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my1_c2cNtWDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "dim = 50\n",
        "\n",
        "X_text_1_ft = np.zeros((len(corpus_csv['text_1_norm']), 300))\n",
        "X_text_2_ft = np.zeros((len(corpus_csv['text_2_norm']), 300))\n",
        "\n",
        "for i, text in enumerate(corpus_csv['text_1_norm'].values):\n",
        "    X_text_1_ft[i] = get_df_embedding(text, rusv_v2w, 300)\n",
        "    \n",
        "for i, text in enumerate(corpus_csv['text_2_norm'].values):\n",
        "    X_text_2_ft[i] = get_df_embedding(text, rusv_v2w, 300)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL4XpW1UNIsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_text_ft = np.concatenate([X_text_1_ft, X_text_2_ft], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPQvZdG0NIuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = corpus_csv['label'].values\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(X_text_ft, y,random_state=1)\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\n",
        "                             class_weight='balanced')\n",
        "clf.fit(train_X, train_y)\n",
        "preds = clf.predict(valid_X)\n",
        "print(classification_report(valid_y, preds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s52-nFsouZSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "scores = cross_val_score(clf, X_text_ft, y, cv=5, scoring = 'f1_micro')\n",
        "scores.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd1sn0sFiU2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = corpus_csv['label'].values\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(X_text_ft, y,random_state=1)\n",
        "clf = RandomForestClassifier(n_estimators=200, max_depth=7, min_samples_leaf=15,\n",
        "                             class_weight='balanced')\n",
        "clf.fit(train_X, train_y)\n",
        "preds = clf.predict(valid_X)\n",
        "print(classification_report(valid_y, preds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxNJd005SPmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(clf, X_text_ft, y, cv=5, scoring = 'f1_micro')\n",
        "scores.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuM2A85CuRPW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors_df = corpus_csv\n",
        "\n",
        "vectors_df['text_1_nor'] = vectors_df['text_1'].apply(normalize)\n",
        "vectors_df['text_2_nor'] = vectors_df['text_2'].apply(normalize)\n",
        "vectors_df.head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pD5s6MsLx_Ho",
        "colab": {}
      },
      "source": [
        "data_lines_norm = [normalize(text) for text in data.splitlines()]\n",
        "# тексты для обучения w2v и fasttext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMtAzgWqzttY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf = TfidfVectorizer(min_df=3, max_df=0.4, max_features=1000)\n",
        "tfidf.fit(pd.concat([vectors_df['text_1_nor'], vectors_df['text_2_nor']]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVitMIL-obdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Подсчет косинусного расстояния для SVD и NMF\n",
        "svd = TruncatedSVD(200)\n",
        "\n",
        "X_text_1 = svd.fit_transform(tfidf.transform(vectors_df['text_1_nor']))\n",
        "X_text_2 = svd.fit_transform(tfidf.transform(vectors_df['text_2_nor']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twk5qgdPpInF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vect_dict = []\n",
        "len(distance)\n",
        "distance = np.zeros((len(vectors_df['text_1_nor'].values), dim))\n",
        "for i, text in enumerate(X_text_1):   \n",
        "    distance[i] = cosine_distances(X_text_2[[i]], X_text_1[[i]])[0][0]\n",
        "    vect_dict.append(distance[i][0])\n",
        "vectors_df['distance_svd'] = vect_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDhAxRG1OlYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nmf = NMF(50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9WN8D6lOlVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_text_1_nmf = nmf.fit_transform(tfidf.transform(vectors_df['text_1_nor'].values))\n",
        "X_text_2_nmf = nmf.fit_transform(tfidf.transform(vectors_df['text_2_nor'].values))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5dmmmsfOlSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vect_dict = []\n",
        "distance = np.zeros((len(vectors_df['text_1_nor'].values), dim))\n",
        "for i, text in enumerate(X_text_1):   \n",
        "    distance[i] = cosine_distances(X_text_2_nmf[[i]], X_text_1_nmf[[i]])[0][0]\n",
        "    vect_dict.append(distance[i][0])\n",
        "vectors_df['distance_nmf'] = vect_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKqt7BInQzZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors_df.head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHqd1GjopIhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fast_text = gensim.models.FastText([text.split() for text in data_lines_norm], size=50, \n",
        "                                   min_n=4, max_n=8) \n",
        "w2v = gensim.models.Word2Vec([text.split() for text in data_lines_norm], size=50, sg=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piKXPD_D_y6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Функция, возвращающая косинсное расстояние для оставшихся моделей\n",
        "# на вход подуются тексты 1 и 2 из таблички с перефразами,\n",
        "# модель, и датафрейм, куда пойдет результат\n",
        "\n",
        "def cosine_sim(model, text_1, text_2, res_df, dim=50):\n",
        "\n",
        "  X_text_1 = np.zeros((len(text_1), dim))\n",
        "  X_text_2 = np.zeros((len(text_2), dim))\n",
        "  vect_dict = []\n",
        "  distance = np.zeros((len(vectors_df['text_1_nor'].values), dim))\n",
        "  for i, text in enumerate(text_1.values):\n",
        "      X_text_1[i] = get_df_embedding(text, model, dim)\n",
        "      \n",
        "  for i, text in enumerate(text_2.values):\n",
        "      X_text_2[i] = get_df_embedding(text, model, dim)\n",
        "  for i, text in enumerate(X_text_1):\n",
        "      distance[i] = cosine_distances(X_text_2[[i]], X_text_1[[i]])[0][0]\n",
        "      try:\n",
        "        #print(distance)\n",
        "        vect_dict.append(distance[i][0])\n",
        "      except Exception as e:\n",
        "        pass\n",
        "  res_df['distance_' + str(model)] = vect_dict\n",
        "\n",
        "\n",
        "cosine_sim(my_w2v, vectors_df['text_1_nor'], vectors_df['text_2_nor'], vectors_df, dim=50)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0-ppnxiGTfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cosine_sim(fast_text, vectors_df['text_1_nor'], vectors_df['text_2_nor'], vectors_df, dim=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OutEfmKAIlgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cosine_sim(fast_text, vectors_df['text_1_nor'], vectors_df['text_2_nor'], vectors_df, dim=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1ixJUAwIlcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cosine_sim(rusv_v2w, vectors_df['text_1_norm'], vectors_df['text_2_norm'], vectors_df, dim=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V52eLskhieIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors_df.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxw0epZ6HbKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#удобный датафрейм\n",
        "vectors_only = vectors_df.drop(['text_1','text_2', 'text_1_norm', 'text_2_norm', 'text_1_nor', 'text_2_nor'], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EshTvYl1AeJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#переименование столбцов\n",
        "vectors_only.columns = ['label','my_w2v', 'fasttext', 'rusv_w2v', 'svd', 'nmf']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1qnthQiokXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors_only.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLkjjK5zVqEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = vectors_only['label'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLjPKelAeH9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "predict_list = []\n",
        "for i, lbl in enumerate(y):\n",
        "\n",
        "  pred_i = [vectors_only['nmf'][i], vectors_only['my_w2v'][i], vectors_only['fasttext'][i], vectors_only['rusv_w2v'][i], vectors_only['svd'][i]]\n",
        "  predict_list.append(pred_i)\n",
        "\n",
        "X = predict_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aocywywCfAaJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#классификатор и кросс-валидация\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\n",
        "                             class_weight='balanced')\n",
        "scores = cross_val_score(clf, X, y, cv=5, scoring = 'f1_micro')\n",
        "scores.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyS2Y1WZ54p3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}