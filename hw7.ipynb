{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Cython\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/54/9d66ee2180776dfe33c2b8cc2f2b67c343fd9d80de91ac0edc5bc346fb06/Cython-0.29.15-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting numpy\n",
      "  Using cached https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Installing collected packages: Cython, numpy\n",
      "Successfully installed Cython-0.29.15 numpy-1.18.1\n",
      "Collecting git+https://github.com/lopuhin/python-adagram.git\n",
      "  Cloning https://github.com/lopuhin/python-adagram.git to /tmp/pip-mf24uhyi-build\n",
      "Collecting cython (from adagram==0.0.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/54/9d66ee2180776dfe33c2b8cc2f2b67c343fd9d80de91ac0edc5bc346fb06/Cython-0.29.15-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting joblib (from adagram==0.0.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl\n",
      "Collecting numpy>=1.9 (from adagram==0.0.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting six (from adagram==0.0.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/65/eb/1f97cb97bfc2390a276969c6fae16075da282f5058082d4cb10c6c5c1dba/six-1.14.0-py2.py3-none-any.whl\n",
      "Installing collected packages: cython, joblib, numpy, six, adagram\n",
      "  Running setup.py install for adagram ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed adagram-0.0.1 cython-0.29.15 joblib-0.14.1 numpy-1.18.1 six-1.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install Cython numpy\n",
    "!pip3 install git+https://github.com/lopuhin/python-adagram.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dima/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adagram\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from string import punctuation\n",
    "import json, os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split() if word and word not in stops]\n",
    "    words = [word for word in words if word]\n",
    "\n",
    "    return words\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = tokenize(text)\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n"
     ]
    }
   ],
   "source": [
    "corpus = open('corpus_ng.txt').read()[:500000]\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = normalize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('corpus.txt', 'w')\n",
    "f.write(' '.join(corpus))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2020-03-16 01:11:59,873 Building dictionary...\n",
      "[INFO] 2020-03-16 01:11:59,950 Done! 1032 words.\n",
      "[INFO] 2020-03-16 01:12:02,793 100.00% -6.5942 0.0000 1.8/2.0 10.78 kwords/sec\n"
     ]
    }
   ],
   "source": [
    "!adagram-train corpus.txt out.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = adagram.VectorModel.load(\"out.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99851649, 0.00148351, 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm.disambiguate(\"мир\", ['чемпионат', \"россия\", \"игра\", \"футбол\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = vm.sense_vector(\"мир\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus_xml = html.fromstring(open('paraphraser/paraphrases.xml', 'rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "df_data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['text_1_norm'] = df_data['text_1'].apply(normalize)\n",
    "df_data['text_2_norm'] = df_data['text_2'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('my_text.txt', 'w')\n",
    "for i in df_data['text_1_norm']:\n",
    "    f.write(' '.join(i))\n",
    "for i in df_data['text_2_norm']:\n",
    "    f.write(' '.join(i))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2020-03-16 01:13:22,006 Building dictionary...\n",
      "[INFO] 2020-03-16 01:13:22,202 Done! 1358 words.\n",
      "[INFO] 2020-03-16 01:13:28,966 100.00% -6.6400 0.0000 1.7/3.0 10.26 kwords/sec\n"
     ]
    }
   ],
   "source": [
    "!adagram-train my_text.txt out.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = adagram.VectorModel.load(\"out.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [1,2,3,4,5,6,7,8,9]\n",
    "#Функция из Задания 1\n",
    "def get_words_in_context(words, window=3):\n",
    "  main_list = []\n",
    "  for i, word in enumerate(words):\n",
    "    local_list = []\n",
    "    if i < window:\n",
    "      local_list = [word, words[:window+i+1]]\n",
    "      local_list[1].remove(word)\n",
    "      main_list.append(local_list)\n",
    "    else:           \n",
    "      local_list = [word, words[i - window:i + window+1]]\n",
    "      local_list[1].remove(word)\n",
    "      main_list.append(local_list)\n",
    "\n",
    "  return main_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, [2, 3, 4]],\n",
       " [2, [1, 3, 4, 5]],\n",
       " [3, [1, 2, 4, 5, 6]],\n",
       " [4, [1, 2, 3, 5, 6, 7]],\n",
       " [5, [2, 3, 4, 6, 7, 8]],\n",
       " [6, [3, 4, 5, 7, 8, 9]],\n",
       " [7, [4, 5, 6, 8, 9]],\n",
       " [8, [5, 6, 7, 9]],\n",
       " [9, [6, 7, 8]]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_words_in_context(words, window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_embedding для адаграма\n",
    "def get_embedding_adagram(text, model, window, dim):   \n",
    "    \n",
    "    word2context = get_words_in_context(text, window)\n",
    "    \n",
    "    \n",
    "    vectors = np.zeros((len(word2context), dim))\n",
    "    \n",
    "    for i, (word, context) in enumerate(word2context):\n",
    "        \n",
    "        try:\n",
    "            v = vm.disambiguate(word, context).argmax()\n",
    "            vectors[i] = model.sense_vector(word, v)\n",
    "        \n",
    "        except (KeyError, ValueError) as e:\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_data['text_1_notnorm'] = df_data['text_1'].apply(tokenize)\n",
    "df_data['text_2_notnorm'] = df_data['text_2'].apply(tokenize)\n",
    "\n",
    "X_text_1_ft = np.zeros((len(df_data['text_1_norm']), 100))\n",
    "X_text_2_ft = np.zeros((len(df_data['text_2_norm']), 100))\n",
    "\n",
    "for i, text in enumerate(df_data['text_1_norm'].values):\n",
    "    X_text_1_ft[i] = get_embedding_adagram(text, vm, 3, 100)\n",
    "    \n",
    "for i, text in enumerate(df_data['text_2_norm'].values):\n",
    "    X_text_2_ft[i] = get_embedding_adagram(text, vm, 3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_ft = np.concatenate([X_text_1_ft, X_text_2_ft], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.59      0.53      0.56       629\n",
      "           0       0.51      0.21      0.29       737\n",
      "           1       0.33      0.70      0.45       441\n",
      "\n",
      "    accuracy                           0.44      1807\n",
      "   macro avg       0.47      0.48      0.43      1807\n",
      "weighted avg       0.49      0.44      0.42      1807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = df_data['label'].values\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X_text_ft, y,random_state=1)\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\n",
    "                             class_weight='balanced')\n",
    "clf.fit(train_X, train_y)\n",
    "preds = clf.predict(valid_X)\n",
    "print(classification_report(valid_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3915825544276778"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#оценка кросс-валидацией\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(clf, X_text_ft, y, cv=5, scoring = 'f1_micro')\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/dima/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117659"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(wn.all_synsets()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time', 'for', 'Earth', 'to', 'make', 'a', 'complete', 'rotation', 'on', 'its', 'axis']\n",
      "['some', 'point', 'or', 'period', 'in', 'time']\n",
      "['a', 'day', 'assigned', 'to', 'a', 'particular', 'purpose', 'or', 'observance']\n",
      "['the', 'time', 'after', 'sunrise', 'and', 'before', 'sunset', 'while', 'it', 'is', 'light', 'outside']\n",
      "['the', 'recurring', 'hours', 'when', 'you', 'are', 'not', 'sleeping', '(especially', 'those', 'when', 'you', 'are', 'working)']\n",
      "['an', 'era', 'of', 'existence', 'or', 'influence']\n",
      "['the', 'period', 'of', 'time', 'taken', 'by', 'a', 'particular', 'planet', '(e.g.', 'Mars)', 'to', 'make', 'a', 'complete', 'rotation', 'on', 'its', 'axis']\n",
      "['the', 'time', 'for', 'one', 'complete', 'rotation', 'of', 'the', 'earth', 'relative', 'to', 'a', 'particular', 'star,', 'about', '4', 'minutes', 'shorter', 'than', 'a', 'mean', 'solar', 'day']\n",
      "['a', 'period', 'of', 'opportunity']\n",
      "['United', 'States', 'writer', 'best', 'known', 'for', 'his', 'autobiographical', 'works', '(1874-1935)']\n"
     ]
    }
   ],
   "source": [
    "word = 'day'\n",
    "for synset in wn.synsets(word):\n",
    "    print(synset.definition().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"this is a good to die\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#алгоритм Леска\n",
    "def lesk( word, sentence_dict):\n",
    "    bestsense = 0\n",
    "    maxoverlap = 0\n",
    "    sentence_set = set(normalize(\" \".join(sentence_dict)))\n",
    "    for i, syns in enumerate(wn.synsets(word)):\n",
    "        syns = normalize(syns.definition())\n",
    "        if len(sentence_set & set(syns)) > maxoverlap:\n",
    "            maxoverlap = len(sentence_set & set(syns))\n",
    "            bestsense = i\n",
    "    return (wn.synsets(word)[bestsense])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a day assigned to a particular purpose or observance'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesk('day', sent.split()).definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#проверка на реальном корпусе\n",
    "corpus_wsd = []\n",
    "corpus = open('corpus_wsd_50k.txt').read().split('\\n\\n')[:1000]\n",
    "for sent in corpus:\n",
    "    corpus_wsd.append([s.split('\\t') for s in sent.split('\\n')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('long.a.01')"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma_from_key('long%3:00:02::').synset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list = []\n",
    "lebel_list = []\n",
    "for sent_list in corpus_wsd:\n",
    "    my_sentence = []\n",
    "    key_dict = Counter()\n",
    "    for word_list in sent_list:\n",
    "        if word_list[0]:\n",
    "            key_dict[word_list[2]] = word_list[0]\n",
    "        my_sentence.append(word_list[2])\n",
    "    for word in my_sentence:\n",
    "        if key_dict[word]:\n",
    "            try:\n",
    "                if lesk(word, my_sentence) == wn.lemma_from_key(key_dict[word]).synset():\n",
    "                    res_list.append(1)\n",
    "                else:\n",
    "                    res_list.append(0)\n",
    "            except IndexError:\n",
    "                res_list.append(0)\n",
    "            lebel_list.append(1)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3167509191176471"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(res_list, lebel_list, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
