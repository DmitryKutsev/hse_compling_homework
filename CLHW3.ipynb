{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitryKutsev/hse_compling_homework/blob/master/CLHW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEitWSBMoZxm",
        "colab_type": "code",
        "outputId": "5caa9273-0e8f-4a6b-946e-bfdc699188d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Fetch a single <1MB file using the raw GitHub URL.\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/correct_sents.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  117k  100  117k    0     0   572k      0 --:--:-- --:--:-- --:--:--  572k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrnsj3W5pLpQ",
        "colab_type": "code",
        "outputId": "3eeac022-decd-4e52-f469-ab2fc75dfe5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Fetch a single <1MB file using the raw GitHub URL.\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/sents_with_mistakes.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  120k  100  120k    0     0   447k      0 --:--:-- --:--:-- --:--:--  445k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p9_KKGLpS3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, re\n",
        "from string import punctuation\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import Counter\n",
        "from pprint import pprint\n",
        "from nltk import sent_tokenize\n",
        "punctuation += \"«»—…“”\"\n",
        "punct = set(punctuation)\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAXOYkhM0ikv",
        "colab_type": "code",
        "outputId": "40f215dd-a355-4162-f984-2e32716ea724",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "!wget  https://github.com/yutkin/Lenta.Ru-News-Dataset\n",
        "!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-15 18:17:50--  https://github.com/yutkin/Lenta.Ru-News-Dataset\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘Lenta.Ru-News-Dataset’\n",
            "\n",
            "\rLenta.Ru-News-Datas     [<=>                 ]       0  --.-KB/s               \rLenta.Ru-News-Datas     [ <=>                ]  79.89K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-11-15 18:17:51 (631 KB/s) - ‘Lenta.Ru-News-Dataset’ saved [81812]\n",
            "\n",
            "--2019-11-15 18:17:52--  https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/87156914/0b363e00-0126-11e9-9e3c-e8c235463bd6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191115%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191115T181753Z&X-Amz-Expires=300&X-Amz-Signature=46d857b2a37ea32228ed4934f770deaf504ef707a95d21bcddce734f4e856286&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2019-11-15 18:17:53--  https://github-production-release-asset-2e65be.s3.amazonaws.com/87156914/0b363e00-0126-11e9-9e3c-e8c235463bd6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191115%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191115T181753Z&X-Amz-Expires=300&X-Amz-Signature=46d857b2a37ea32228ed4934f770deaf504ef707a95d21bcddce734f4e856286&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.217.38.236\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.217.38.236|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 527373240 (503M) [application/octet-stream]\n",
            "Saving to: ‘lenta-ru-news.csv.gz’\n",
            "\n",
            "lenta-ru-news.csv.g 100%[===================>] 502.94M  45.5MB/s    in 11s     \n",
            "\n",
            "2019-11-15 18:18:05 (43.9 MB/s) - ‘lenta-ru-news.csv.gz’ saved [527373240/527373240]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hshsEbHs1kVX",
        "colab_type": "code",
        "outputId": "99db9841-8a67-4615-e006-d31ca08a07b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import gzip\n",
        "import csv\n",
        "os.listdir(os.getcwd())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'correct_sents.txt',\n",
              " 'Lenta.Ru-News-Dataset',\n",
              " 'lenta-ru-news.csv.gz',\n",
              " 'sents_with_mistakes.txt',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgQekL7i1uBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "corpus = open('corpus_5000.txt', 'w')\n",
        "with gzip.open('lenta-ru-news.csv.gz', 'rt') as archive:\n",
        "    reader = csv.reader(archive, delimiter=',', quotechar='\"')\n",
        "    for i, line in enumerate(reader):\n",
        "        if i < 5000: # увеличьте количество текстов тут\n",
        "            corpus.write(line[2].replace('\\xa0', ' ') + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXsj8rmo7YvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(text):\n",
        "    \n",
        "    normalized_text = [(word.strip(punctuation)) for word \\\n",
        "                                                            in text.lower().split()]\n",
        "    normalized_text = [word for word in normalized_text if word]\n",
        "    return normalized_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8A2kV5P3HEC",
        "colab_type": "code",
        "outputId": "5f4e89b4-7145-4c82-e7ec-77a5e9f5f0d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "corpus = []\n",
        "for text in open('corpus_5000.txt').read().splitlines():\n",
        "    sents = sent_tokenize(text)\n",
        "    norm_sents = [normalize(sent) for sent in sents]\n",
        "    corpus += norm_sents"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7cAqeCJ7CyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []\n",
        "for text in open('corpus_5000.txt').read().splitlines():\n",
        "    sents = sent_tokenize(text)\n",
        "    norm_sents = [normalize(sent) for sent in sents]\n",
        "    corpus += norm_sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy6yYf2J7hON",
        "colab_type": "code",
        "outputId": "3065f590-96da-4256-960d-98a359e06112",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "corpus[4]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['в',\n",
              " 'начале',\n",
              " 'года',\n",
              " 'стало',\n",
              " 'известно',\n",
              " 'что',\n",
              " 'смертность',\n",
              " 'от',\n",
              " 'онкологических',\n",
              " 'заболеваний',\n",
              " 'среди',\n",
              " 'россиян',\n",
              " 'снизилась',\n",
              " 'впервые',\n",
              " 'за',\n",
              " 'три',\n",
              " 'года']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaY-L-ShDYqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORDS = Counter()\n",
        "for sent in corpus:\n",
        "    WORDS.update(sent)\n",
        "\n",
        "N = sum(WORDS.values())\n",
        "def P(word, N=N): \n",
        "    \"Вычисляем вероятность слова\"\n",
        "    return WORDS[word] / N"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2X5RbM971vN",
        "colab_type": "code",
        "outputId": "be3b575d-2ed8-432f-9db6-7b54a8ac3e55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!pip install textdistance\n",
        "import textdistance"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textdistance\n",
            "  Downloading https://files.pythonhosted.org/packages/3f/18/31397b687f50ffae65469175f07faa68f288e27fcd8716276004c42e5637/textdistance-4.1.5-py3-none-any.whl\n",
            "Installing collected packages: textdistance\n",
            "Successfully installed textdistance-4.1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kav1a15-DKDk",
        "colab_type": "code",
        "outputId": "ad850b91-b23c-4bfc-9d8c-be81c2374bcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#PETER NORVIG\n",
        "def correction(word): \n",
        "    \"Находим наиболее вероятное похожее слово\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Генерируем кандидатов на исправление\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"Выбираем слова, которые есть в корпусе\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"Создаем кандидатов, которые отличаются на одну букву\"\n",
        "    letters    = 'йцукенгшщзхъфывапролджэячсмитьбюё'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    # for i in splits:\n",
        "    #   print(i)\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    # for i in deletes:\n",
        "    #   print(i)\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    # for i in transposes:\n",
        "    #   print(i)\n",
        "    # print(\"\\n\")\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    # for i in replaces:\n",
        "    #   print(i)\n",
        "    # print(\"\\n\")\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    # for i in inserts:\n",
        "    #   print(i)\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "    print(\"\\n\")\n",
        "\n",
        "def edits2(word): \n",
        "    \"Создаем кандидатов, которые отличаются на две буквы\"\n",
        "    print((e2 for e1 in edits1(word) for e2 in edits1(e1)))\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "correction('онпологических')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'онкологических'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSCIxYDGCadK",
        "colab_type": "code",
        "outputId": "4d9b7a6c-e68c-4049-caf2-2e83a33a3f4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#SymmSpell\n",
        "!pip install symspellpy\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "import pkg_resources"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting symspellpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/0b/2daa14bf1ed649fff0d072b2e51ae98d8b45cae6cf8fdda41be01ce6c289/symspellpy-6.5.2-py3-none-any.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.17.4)\n",
            "Installing collected packages: symspellpy\n",
            "Successfully installed symspellpy-6.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbpyf2_cEQhI",
        "colab_type": "code",
        "outputId": "7b6adf6c-f77f-4775-e9fb-c05188a157a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#print(os.listdir(os.getcwd()))\n",
        "\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "corpus_path = 'corpus_5000.txt'\n",
        "dictionary_path = sym_spell.create_dictionary(corpus_path)\n",
        "def Sym_spell_func(input_term):\n",
        "  sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "  suggestions = sym_spell.lookup(input_term, Verbosity.CLOSEST,\n",
        "                               max_edit_distance=2, include_unknown=True)\n",
        "\n",
        "  return str(suggestions[0]).split(',')[0]\n",
        "Sym_spell_func('впревые')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'впервые'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydpqS6AkwfEq",
        "colab_type": "code",
        "outputId": "440536c9-5f21-483f-8c07-2edee9204451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# напишем функцию, которая будет сопоставлять слова в правильном и ошибочном варианте\n",
        "# разобьем предложение по пробелам и удалим пунктуация на границах слов\n",
        "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
        "true = open('correct_sents.txt', encoding='utf8').read().splitlines()\n",
        "print(bad[2])\n",
        "print(true[2])\n",
        "def align_words(sent_1, sent_2):\n",
        "    tokens_1 = sent_1.lower().split()\n",
        "    tokens_2 = sent_2.lower().split()\n",
        "    \n",
        "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n",
        "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n",
        "    \n",
        "    return list(zip(tokens_1, tokens_2))\n",
        "pprint(align_words(true[2], bad[2]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Пояним эту мысль.\n",
            "Поясним эту мысль\n",
            "[('поясним', 'пояним'), ('эту', 'эту'), ('мысль', 'мысль')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SxcqRTBUwYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPTHuUoDGnUH",
        "colab_type": "code",
        "outputId": "3153d009-bef7-42de-8abe-ef77415354be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Для оценки используем будем использовать три метрики:\n",
        "# 1) процент правильных слов;\n",
        "# 2) процент исправленных ошибок\n",
        "# 3) процент ошибочно исправленных правильных слов\n",
        "%%time\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "total_mistaken = 0\n",
        "mistaken_fixed = 0\n",
        "\n",
        "total_correct = 0\n",
        "correct_broken = 0\n",
        "\n",
        "cashed = {}\n",
        "for i in range(len(true)):\n",
        "    word_pairs = align_words(true[i], bad[i])\n",
        "    for pair in word_pairs:\n",
        "        predicted = cashed.get(pair[1], Sym_spell_func(pair[1]))\n",
        "        cashed[pair[0]] = predicted\n",
        "        if predicted == pair[0]:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        \n",
        "        if pair[0] == pair[1]:\n",
        "            total_correct += 1\n",
        "            if pair[0] !=  predicted:\n",
        "                correct_broken += 1\n",
        "        else:\n",
        "            total_mistaken += 1\n",
        "            if pair[0] == predicted:\n",
        "                mistaken_fixed += 1\n",
        "        \n",
        "    if not i % 100:\n",
        "      print(i)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "CPU times: user 1.44 s, sys: 16.9 ms, total: 1.45 s\n",
            "Wall time: 1.46 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxJ3OYoDwhun",
        "colab_type": "code",
        "outputId": "7a84ec81-1cae-4054-a1ed-33ad95b32317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(correct/total)\n",
        "print(mistaken_fixed/total_mistaken)\n",
        "print(correct_broken/total_correct)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7020979020979021\n",
            "0.5003837298541827\n",
            "0.26771563110141267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xxz3lkV9BufU",
        "colab_type": "code",
        "outputId": "0be36a59-2b4d-456b-8b1f-5df685ab388f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Sym_spell_func('ооочень')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'очень'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi9S2VDxDjb2",
        "colab_type": "code",
        "outputId": "c1878886-28e2-4a6b-a2fe-8b5feb6748f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "#def precalculations(true_dict):\n",
        "corpus_path = 'corpus_5000.txt'\n",
        "true = open('correct_sents.txt', encoding='utf8').read().splitlines()\n",
        "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
        "corpus_sents = open('corpus_5000.txt', encoding='utf8').read().lower().split()\n",
        "\n",
        "\n",
        "\n",
        "def deletes(word):\n",
        "    letters    = 'йцукенгшщзхъфывапролджэячсмитьбюё'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes1    = [L + R[1:]               for L, R in splits if R]\n",
        "    splits2     = [(word[:i], word[i:])    for i in range(len(deletes1))]\n",
        "    deletes2    = [L + R[2:]               for L, R in splits2 if R]\n",
        "    deletes = deletes1 + deletes2\n",
        "    return set(deletes)\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "true_list = [re.sub('(^\\W+|\\W+$)', '', token) for token in corpus_sents if (set(token)-punct)]\n",
        "#print(true_list[1])\n",
        "true_dict = {}\n",
        "prob_dict = {}\n",
        "for i in true_list:\n",
        "  prob_dict[i] = prob_dict.get(i, 0) + 1\n",
        "  for j in deletes(i):\n",
        "    true_dict[j] =  i\n",
        "print(list(true_dict.keys())[:15])\n",
        "\n",
        "def prob(word):\n",
        "  p = prob_dict.get(word, 0)/len(set(true_list))\n",
        "  return(p)\n",
        "\n",
        "\n",
        "def input_processing(input_word):\n",
        "  candidates_list = []\n",
        "  local_probs = {}\n",
        "  if input_word in prob_dict:\n",
        "    return input_word\n",
        "  if input_word in true_dict:\n",
        "    candidates_list.append(true_dict[input_word])\n",
        "  for j in deletes(input_word):\n",
        "      #print(j)\n",
        "    if j in true_dict:\n",
        "        candidates_list.append(true_dict[j])\n",
        "        candidates_list = list(set(candidates_list))\n",
        "  if len(candidates_list) > 1:\n",
        "    for i in set(candidates_list):\n",
        "      local_probs[prob(i)] = i\n",
        "      sorted_probs = sorted(list(local_probs.keys()))\n",
        "      return local_probs[sorted_probs[0]]\n",
        "  else:\n",
        "    if len(candidates_list) > 0:  \n",
        "     return candidates_list[0]\n",
        "    else:\n",
        "      return input_word + \"\"\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['te', 'tex', 'tet', 'txt', 'tt', 'xt', 'ext', 'Вие-премьер', 'Вице-премье', 'Вицеремьер', 'Вице-премь', 'Вицепремьер', 'Вице-премр', 'Вице-преьер', 'Вице-ремьер']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-52m4nVhiyZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "93cb7cc4-5f3d-4935-c289-d887121838c5"
      },
      "source": [
        "print(input_processing('мсль'))\n",
        "print(input_processing('опофеозом'))\n",
        "print(input_processing('сонце'))\n",
        "print(input_processing('седня'))\n",
        "print(input_processing('оочень'))"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "лишь\n",
            "апофеозом\n",
            "сцены\n",
            "сегодня\n",
            "очень\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L5vQK1Lu-A-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "319bcda8-671b-4285-f817-ab296bc37f80"
      },
      "source": [
        "\n",
        "# Для оценки используем будем использовать три метрики:\n",
        "# 1) процент правильных слов;\n",
        "# 2) процент исправленных ошибок\n",
        "# 3) процент ошибочно исправленных правильных слов\n",
        "# %%time\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "total_mistaken = 0\n",
        "mistaken_fixed = 0\n",
        "\n",
        "total_correct = 0\n",
        "correct_broken = 0\n",
        "\n",
        "cashed = {}\n",
        "for i in range(len(true)):\n",
        "    word_pairs = align_words(true[i], bad[i])\n",
        "    for pair in word_pairs:\n",
        "        predicted = cashed.get(pair[1], input_processing(pair[1]))\n",
        "        cashed[pair[0]] = predicted\n",
        "        if predicted == pair[0]:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        \n",
        "        if pair[0] == pair[1]:\n",
        "            total_correct += 1\n",
        "            if pair[0] !=  predicted:\n",
        "                correct_broken += 1\n",
        "        else:\n",
        "            total_mistaken += 1\n",
        "            if pair[0] == predicted:\n",
        "                mistaken_fixed += 1\n",
        "        \n",
        "    if not i % 100:\n",
        "      print(i)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eWNUaKUBJPH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "abdd4e09-dec8-4921-80d3-dbdc6a8a4bd6"
      },
      "source": [
        "print(correct/total)\n",
        "print(mistaken_fixed/total_mistaken)\n",
        "print(correct_broken/total_correct)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6366633366633366\n",
            "0.27858787413660785\n",
            "0.309750775238314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zioakNuUEjA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#По сравнению с теми, что были на лекции, и с реализацией SymspellPy, результат, конечно, не очень."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VWajKsMFT96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#С триграммной моделью"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmsBIpzqgw2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ngrammer(tokens, n=2):\n",
        "    ngrams = []\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams.append(' '.join(tokens[i:i+n]))\n",
        "    return ngrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gvLEGa-f-Py",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []\n",
        "for text in open('corpus_5000.txt').read().splitlines():\n",
        "    sents = sent_tokenize(text)\n",
        "    norm_sents = [normalize(sent) for sent in sents]\n",
        "    corpus += norm_sents\n",
        "\n",
        "corpus_news = [['<start>', '<start>'] + sent + ['<end>'] for sent in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG9-WrWaf_9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigrams = Counter()\n",
        "bigrams = Counter()\n",
        "trigrams = Counter()\n",
        "\n",
        "for sentence in corpus_news:\n",
        "    unigrams.update(sentence)\n",
        "    bigrams.update(ngrammer(sentence))\n",
        "    trigrams.update(ngrammer(sentence, n=3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E2v7i2NfjEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "true = open('correct_sents.txt', encoding='utf8').read().splitlines()\n",
        "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
        "corpus_sents = open('corpus_5000.txt', encoding='utf8').read().lower().split()\n",
        "\n",
        "\n",
        "\n",
        "def deletes(word):\n",
        "    letters    = 'йцукенгшщзхъфывапролджэячсмитьбюё'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes1    = [L + R[1:]               for L, R in splits if R]\n",
        "    splits2     = [(word[:i], word[i:])    for i in range(len(deletes1))]\n",
        "    deletes2    = [L + R[2:]               for L, R in splits2 if R]\n",
        "    deletes = deletes1 + deletes2\n",
        "    return set(deletes)\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "dict_list = [re.sub('(^\\W+|\\W+$)', '', token) for token in corpus_sents if (set(token)-punct)]\n",
        "#print(true_list[1])\n",
        "candidats_dict = {}\n",
        "prob_dict = {}\n",
        "for i in range(len(dict_list)):\n",
        "  prob_dict[dict_list[i]] = prob_dict.get(dict_list[i], 0) + 1\n",
        "  for j in deletes(dict_list[i]):\n",
        "    candidats_dict[j] =  dict_list[i]\n",
        "print(list(true_dict.keys())[:15])\n",
        "\n",
        "def prob(word):\n",
        "  p = prob_dict.get(word, 0)/len(set(true_list))\n",
        "  return(p)\n",
        "\n",
        "\n",
        "def input_processing(input_word):\n",
        "  candidates_list = []\n",
        "  local_probs = {}\n",
        "  if input_word in prob_dict:\n",
        "    return input_word\n",
        "  if input_word in candidats_dict:\n",
        "    candidates_list.append(true_dict[input_word])\n",
        "  for j in deletes(input_word):\n",
        "      #print(j)\n",
        "    if j in marks_dict:\n",
        "        candidates_list.append(candidats_dict[j])\n",
        "        candidates_list = list(set(candidates_list))\n",
        "  if len(candidates_list) > 1:\n",
        "    for i in set(candidates_list):\n",
        "      local_probs[prob(i)] = i\n",
        "      sorted_probs = sorted(list(local_probs.keys()))\n",
        "      return local_probs[sorted_probs[0]]\n",
        "  else:\n",
        "    if len(candidates_list) > 0:  \n",
        "     return candidates_list[0]\n",
        "    else:\n",
        "      return input_word + \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xT6RQ3wfji1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCdm736NYS4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc2ctQU9ZYVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORDS = Counter()\n",
        "for sent in corpus:\n",
        "    WORDS.update(sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_6B6T1VYZmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_closest_match_vec(text, X, vec, topn=20):\n",
        "    # превращаем слово в вектор такой же размерности\n",
        "    v = vec.transform([text])\n",
        "    \n",
        "    # вся эффективноть берется из того, что мы сразу считаем близость \n",
        "    # 1 вектора ко всей матрице (словам в словаре)\n",
        "    # считать по отдельности циклом было бы дольше\n",
        "    # вместо одного вектора может даже целая матрица\n",
        "    # тогда считаться в итоге будет ещё быстрее\n",
        "    \n",
        "    similarities = cosine_distances(v, X)[0] #distance - чем больше, тем хуже, а similarity наоборот\n",
        "    topn = similarities.argsort()[:topn] \n",
        "    \n",
        "    return [(id2word[top], similarities[top]) for top in topn]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFczdl5sZBUI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "b74be3e3-bb20-46cd-89d3-7158de52f6f0"
      },
      "source": [
        "%time\n",
        "get_closest_hybrid_match('сонце', X, vec)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 8.58 µs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-199-dccc69b31d1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_closest_hybrid_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'сонце'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua1sRaK6Y2j-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_closest_hybrid_match(text, X, vec, topn=5, metric=textdistance.damerau_levenshtein):\n",
        "    # ваш код здесь\n",
        "    candidates = get_closest_match_vec(text, X, vec, topn*4)\n",
        "    sims = Counter()\n",
        "    lookup = [cand[0] for cand in candidates]\n",
        "    closest = get_closest_match_with_metric(text, lookup,topn, metric=metric)\n",
        "\n",
        "    \n",
        "    return closest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBF4XmfcLLOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "corpus = []\n",
        "for text in open('corpus_5000.txt').read().splitlines():\n",
        "    sents = sent_tokenize(text)\n",
        "    norm_sents = [normalize(sent) for sent in sents]\n",
        "    corpus += norm_sents\n",
        "\n",
        "corpus_news = [['<start>', '<start>'] + sent + ['<end>'] for sent in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NhvgKLILcAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigrams = Counter()\n",
        "bigrams = Counter()\n",
        "trigrams = Counter()\n",
        "\n",
        "for sentence in corpus_news:\n",
        "    unigrams.update(sentence)\n",
        "    bigrams.update(ngrammer(sentence))\n",
        "    trigrams.update(ngrammer(sentence, n=3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAcI3wTINeyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def ngrammer(tokens, n=2):\n",
        "    ngrams = []\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams.append(' '.join(tokens[i:i+n]))\n",
        "    return ngrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2Q-pqZBOezv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigrams = Counter()\n",
        "bigrams = Counter()\n",
        "trigrams = Counter()\n",
        "\n",
        "for sentence in corpus_news:\n",
        "    unigrams.update(sentence)\n",
        "    bigrams.update(ngrammer(sentence))\n",
        "    trigrams.update(ngrammer(sentence, n=3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vov-N6ntOlFe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "34952e8a-d7b3-4630-e835-44ed4fc9e927"
      },
      "source": [
        "#print(ngrams[:10])\n",
        "print(unigrams.most_common(10))\n",
        "print(bigrams.most_common(10))\n",
        "print(trigrams.most_common(10))"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('<start>', 118304), ('<end>', 59152), ('в', 41930), ('и', 20346), ('на', 17455), ('что', 11631), ('с', 9616), ('по', 8778), ('не', 7696), ('из', 4369)]\n",
            "[('<start> <start>', 59152), ('<start> в', 6892), ('об этом', 3074), ('<start> по', 2985), ('<start> об', 2984), ('<start> на', 1312), ('этом сообщает', 1280), ('<start> он', 1254), ('<start> ранее', 946), ('а также', 927)]\n",
            "[('<start> <start> в', 6892), ('<start> <start> по', 2985), ('<start> <start> об', 2984), ('<start> об этом', 2953), ('<start> <start> на', 1312), ('об этом сообщает', 1278), ('<start> <start> он', 1254), ('<start> <start> ранее', 946), ('<start> <start> как', 770), ('<start> по словам', 769)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg9YoV1wO6eE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "8efeec47-ddf8-4c1e-8694-2e7868871491"
      },
      "source": [
        "mistakes = []\n",
        "total_mistaken = 0\n",
        "mistaken_fixed = 0\n",
        "\n",
        "total_correct = 0\n",
        "correct_broken = 0\n",
        "\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "\n",
        "for i in range(len(true)):\n",
        "    word_pairs = align_words(true[i], bad[i])\n",
        "    \n",
        "    word_pairs = [('<start>', '<start>')] + word_pairs\n",
        "    pred_sent = []\n",
        "    for j in range(1, len(word_pairs)):\n",
        "        \n",
        "        pred = None\n",
        "        predicted = get_closest_hybrid_match(word_pairs[j][1], X, vec)\n",
        "        \n",
        "        \n",
        "        prev_word = word_pairs[j-1][1]\n",
        "        \n",
        "        \n",
        "        if prev_word not in unigrams:\n",
        "            pred = predicted[0][0]\n",
        "            \n",
        "        \n",
        "        else:\n",
        "            \n",
        "            lm_predicted = []\n",
        "            for word, m in predicted:\n",
        "                bigram = ' '.join([prev_word, word])\n",
        "                # домножаем полученную метрику для слова на вероятность биграма\n",
        "                # биграм - предыдущее слово + текущее слово кандидат\n",
        "                lm_predicted.append((word, (m)*(1+(bigrams[bigram]/unigrams[prev_word]))))\n",
        "            if lm_predicted:\n",
        "                \n",
        "                pred = sorted(lm_predicted, key=lambda x: -x[1])[0][0]\n",
        "            \n",
        "        \n",
        "        if pred is None:\n",
        "            pred = word_pairs[j][1]\n",
        "        \n",
        "\n",
        "        \n",
        "        if pred == word_pairs[j][0]:\n",
        "            correct += 1\n",
        "        else:\n",
        "            mistakes.append((word_pairs[j][0], word_pairs[j][1], pred))\n",
        "        total += 1\n",
        "            \n",
        "        if word_pairs[j][0] == word_pairs[j][1]:\n",
        "            total_correct += 1\n",
        "            if word_pairs[j][0] !=  pred:\n",
        "                correct_broken += 1\n",
        "        else:\n",
        "            total_mistaken += 1\n",
        "            if word_pairs[j][0] == pred:\n",
        "                mistaken_fixed += 1\n",
        "    \n",
        "    if not i % 50:\n",
        "        print(i)\n",
        "        print(correct/total)"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-198-d2697afc59b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_closest_hybrid_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "384EFuHHYn_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}