{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitryKutsev/hse_compling_homework/blob/master/CLHW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEitWSBMoZxm",
        "colab_type": "code",
        "outputId": "a31b913a-a9f9-410c-f10c-fa537cf902d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Fetch a single <1MB file using the raw GitHub URL.\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/correct_sents.txt"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  1  117k    1  1895    0     0   4737      0  0:00:25 --:--:--  0:00:25  4725\r100  117k  100  117k    0     0   279k      0 --:--:-- --:--:-- --:--:--  278k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrnsj3W5pLpQ",
        "colab_type": "code",
        "outputId": "fd7e9bda-0783-4654-86f7-61d26a929e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Fetch a single <1MB file using the raw GitHub URL.\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/sents_with_mistakes.txt"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  1  120k    1  1895    0     0  10586      0  0:00:11 --:--:--  0:00:11 10527\r100  120k  100  120k    0     0   601k      0 --:--:-- --:--:-- --:--:--  598k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p9_KKGLpS3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, re\n",
        "from string import punctuation\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import Counter\n",
        "from pprint import pprint\n",
        "from nltk import sent_tokenize\n",
        "punctuation += \"«»—…“”\"\n",
        "punct = set(punctuation)\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAXOYkhM0ikv",
        "colab_type": "code",
        "outputId": "0facf2a2-2d6a-4f68-984c-efdaaa54122f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "!wget  https://github.com/yutkin/Lenta.Ru-News-Dataset\n",
        "!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-16 01:01:23--  https://github.com/yutkin/Lenta.Ru-News-Dataset\n",
            "Resolving github.com (github.com)... 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘Lenta.Ru-News-Dataset.1’\n",
            "\n",
            "\rLenta.Ru-News-Datas     [<=>                 ]       0  --.-KB/s               \rLenta.Ru-News-Datas     [ <=>                ]  79.96K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-11-16 01:01:23 (638 KB/s) - ‘Lenta.Ru-News-Dataset.1’ saved [81877]\n",
            "\n",
            "--2019-11-16 01:01:24--  https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/87156914/0b363e00-0126-11e9-9e3c-e8c235463bd6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191116T010125Z&X-Amz-Expires=300&X-Amz-Signature=48a9635829f584e3d628fa63efe3125f374ddf7ec003f27c85418d228947e583&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2019-11-16 01:01:25--  https://github-production-release-asset-2e65be.s3.amazonaws.com/87156914/0b363e00-0126-11e9-9e3c-e8c235463bd6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191116T010125Z&X-Amz-Expires=300&X-Amz-Signature=48a9635829f584e3d628fa63efe3125f374ddf7ec003f27c85418d228947e583&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.230.211\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.230.211|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 527373240 (503M) [application/octet-stream]\n",
            "Saving to: ‘lenta-ru-news.csv.gz.1’\n",
            "\n",
            "lenta-ru-news.csv.g 100%[===================>] 502.94M  38.7MB/s    in 12s     \n",
            "\n",
            "2019-11-16 01:01:38 (40.4 MB/s) - ‘lenta-ru-news.csv.gz.1’ saved [527373240/527373240]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hshsEbHs1kVX",
        "colab_type": "code",
        "outputId": "cd95b777-f540-4f38-9cde-4489c538c418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import gzip\n",
        "import csv\n",
        "os.listdir(os.getcwd())"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'correct_sents.txt',\n",
              " 'Lenta.Ru-News-Dataset',\n",
              " 'lenta-ru-news.csv.gz',\n",
              " 'lenta-ru-news.csv.gz.1',\n",
              " 'sents_with_mistakes.txt',\n",
              " 'Lenta.Ru-News-Dataset.1',\n",
              " 'corpus_5000.txt',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgQekL7i1uBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "corpus = open('corpus_5000.txt', 'w')\n",
        "with gzip.open('lenta-ru-news.csv.gz', 'rt') as archive:\n",
        "    reader = csv.reader(archive, delimiter=',', quotechar='\"')\n",
        "    for i, line in enumerate(reader):\n",
        "        if i < 5000: # увеличьте количество текстов тут\n",
        "            corpus.write(line[2].replace('\\xa0', ' ') + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXsj8rmo7YvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(text):\n",
        "    \n",
        "    normalized_text = [(word.strip(punctuation)) for word \\\n",
        "                                                            in text.lower().split()]\n",
        "    normalized_text = [word for word in normalized_text if word]\n",
        "    return normalized_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8A2kV5P3HEC",
        "colab_type": "code",
        "outputId": "5e808aee-9d53-41e7-a0f3-ea4fe8cf8eff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "corpus = []\n",
        "for text in open('corpus_5000.txt').read().splitlines():\n",
        "    sents = sent_tokenize(text)\n",
        "    norm_sents = [normalize(sent) for sent in sents]\n",
        "    corpus += norm_sents"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7cAqeCJ7CyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []\n",
        "for text in open('corpus_5000.txt').read().splitlines():\n",
        "    sents = sent_tokenize(text)\n",
        "    norm_sents = [normalize(sent) for sent in sents]\n",
        "    corpus += norm_sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy6yYf2J7hON",
        "colab_type": "code",
        "outputId": "ce2c1abe-620d-411a-c671-bf70776389ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "corpus[4]"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['в',\n",
              " 'начале',\n",
              " 'года',\n",
              " 'стало',\n",
              " 'известно',\n",
              " 'что',\n",
              " 'смертность',\n",
              " 'от',\n",
              " 'онкологических',\n",
              " 'заболеваний',\n",
              " 'среди',\n",
              " 'россиян',\n",
              " 'снизилась',\n",
              " 'впервые',\n",
              " 'за',\n",
              " 'три',\n",
              " 'года']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaY-L-ShDYqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORDS = Counter()\n",
        "for sent in corpus:\n",
        "    WORDS.update(sent)\n",
        "\n",
        "N = sum(WORDS.values())\n",
        "def P(word, N=N): \n",
        "    \"Вычисляем вероятность слова\"\n",
        "    return WORDS[word] / N"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2X5RbM971vN",
        "colab_type": "code",
        "outputId": "a9d09581-9ed5-4bc8-d79d-d7bca6bfb055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install textdistance\n",
        "import textdistance"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textdistance in /usr/local/lib/python3.6/dist-packages (4.1.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kav1a15-DKDk",
        "colab_type": "code",
        "outputId": "95f4c8e5-65ff-4a8c-d091-264378892a97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#PETER NORVIG\n",
        "def correction(word): \n",
        "    \"Находим наиболее вероятное похожее слово\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Генерируем кандидатов на исправление\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"Выбираем слова, которые есть в корпусе\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"Создаем кандидатов, которые отличаются на одну букву\"\n",
        "    letters    = 'йцукенгшщзхъфывапролджэячсмитьбюё'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    # for i in splits:\n",
        "    #   print(i)\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    # for i in deletes:\n",
        "    #   print(i)\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    # for i in transposes:\n",
        "    #   print(i)\n",
        "    # print(\"\\n\")\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    # for i in replaces:\n",
        "    #   print(i)\n",
        "    # print(\"\\n\")\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    # for i in inserts:\n",
        "    #   print(i)\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "    print(\"\\n\")\n",
        "\n",
        "def edits2(word): \n",
        "    \"Создаем кандидатов, которые отличаются на две буквы\"\n",
        "    print((e2 for e1 in edits1(word) for e2 in edits1(e1)))\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "correction('онпологических')"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'онкологических'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSCIxYDGCadK",
        "colab_type": "code",
        "outputId": "2dfc8e88-aafd-4b70-f83d-9175c3ee9637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#SymmSpell\n",
        "!pip install symspellpy\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "import pkg_resources"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: symspellpy in /usr/local/lib/python3.6/dist-packages (6.5.2)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.17.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbpyf2_cEQhI",
        "colab_type": "code",
        "outputId": "8989dbfc-ca39-4fa2-b2ec-8c72c807f472",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#print(os.listdir(os.getcwd()))\n",
        "\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "corpus_path = 'corpus_5000.txt'\n",
        "dictionary_path = sym_spell.create_dictionary(corpus_path)\n",
        "def Sym_spell_func(input_term):\n",
        "  sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "  suggestions = sym_spell.lookup(input_term, Verbosity.CLOSEST,\n",
        "                               max_edit_distance=2, include_unknown=True)\n",
        "\n",
        "  return str(suggestions[0]).split(',')[0]\n",
        "Sym_spell_func('впревые')"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'впервые'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydpqS6AkwfEq",
        "colab_type": "code",
        "outputId": "6d73cf1f-59c9-4014-8e89-f64a5a3ea135",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# напишем функцию, которая будет сопоставлять слова в правильном и ошибочном варианте\n",
        "# разобьем предложение по пробелам и удалим пунктуация на границах слов\n",
        "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
        "true = open('correct_sents.txt', encoding='utf8').read().splitlines()\n",
        "print(bad[2])\n",
        "print(true[2])\n",
        "def align_words(sent_1, sent_2):\n",
        "    tokens_1 = sent_1.lower().split()\n",
        "    tokens_2 = sent_2.lower().split()\n",
        "    \n",
        "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n",
        "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n",
        "    \n",
        "    return list(zip(tokens_1, tokens_2))\n",
        "pprint(align_words(true[2], bad[2]))"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Пояним эту мысль.\n",
            "Поясним эту мысль\n",
            "[('поясним', 'пояним'), ('эту', 'эту'), ('мысль', 'мысль')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SxcqRTBUwYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPTHuUoDGnUH",
        "colab_type": "code",
        "outputId": "30bf9d00-fdf6-4f9d-c4fe-f124ace02cad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Для оценки используем будем использовать три метрики:\n",
        "# 1) процент правильных слов;\n",
        "# 2) процент исправленных ошибок\n",
        "# 3) процент ошибочно исправленных правильных слов\n",
        "%%time\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "total_mistaken = 0\n",
        "mistaken_fixed = 0\n",
        "\n",
        "total_correct = 0\n",
        "correct_broken = 0\n",
        "\n",
        "cashed = {}\n",
        "for i in range(len(true)):\n",
        "    word_pairs = align_words(true[i], bad[i])\n",
        "    for pair in word_pairs:\n",
        "        predicted = cashed.get(pair[1], Sym_spell_func(pair[1]))\n",
        "        cashed[pair[0]] = predicted\n",
        "        if predicted == pair[0]:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        \n",
        "        if pair[0] == pair[1]:\n",
        "            total_correct += 1\n",
        "            if pair[0] !=  predicted:\n",
        "                correct_broken += 1\n",
        "        else:\n",
        "            total_mistaken += 1\n",
        "            if pair[0] == predicted:\n",
        "                mistaken_fixed += 1\n",
        "        \n",
        "    if not i % 100:\n",
        "      print(i)"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "CPU times: user 1.46 s, sys: 13 ms, total: 1.47 s\n",
            "Wall time: 1.5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxJ3OYoDwhun",
        "colab_type": "code",
        "outputId": "6d8dc761-3df7-44a7-f295-bbf84bb4dbba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(correct/total)\n",
        "print(mistaken_fixed/total_mistaken)\n",
        "print(correct_broken/total_correct)"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7020979020979021\n",
            "0.5003837298541827\n",
            "0.26771563110141267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xxz3lkV9BufU",
        "colab_type": "code",
        "outputId": "b5dacae7-c37a-4026-e2ae-4342fd04420e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Sym_spell_func('ооочень')"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'очень'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi9S2VDxDjb2",
        "colab_type": "code",
        "outputId": "8994000f-8a9b-4464-e2cc-9f3f70e0a7ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "#def precalculations(true_dict):\n",
        "corpus_path = 'corpus_5000.txt'\n",
        "true = open('correct_sents.txt', encoding='utf8').read().splitlines()\n",
        "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
        "corpus_sents = open('corpus_5000.txt', encoding='utf8').read().lower().split()\n",
        "\n",
        "\n",
        "\n",
        "def deletes(word):\n",
        "    letters    = 'йцукенгшщзхъфывапролджэячсмитьбюё'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes1    = [L + R[1:]               for L, R in splits if R]\n",
        "    splits2     = [(word[:i], word[i:])    for i in range(len(deletes1))]\n",
        "    deletes2    = [L + R[2:]               for L, R in splits2 if R]\n",
        "    deletes = deletes1 + deletes2\n",
        "    return set(deletes)\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "true_list = [re.sub('(^\\W+|\\W+$)', '', token) for token in corpus_sents if (set(token)-punct)]\n",
        "#print(true_list[1])\n",
        "true_dict = {}\n",
        "prob_dict = {}\n",
        "for i in true_list:\n",
        "  prob_dict[i] = prob_dict.get(i, 0) + 1\n",
        "  for j in deletes(i):\n",
        "    true_dict[j] =  i\n",
        "print(list(true_dict.keys())[:15])\n",
        "\n",
        "def prob(word):\n",
        "  p = prob_dict.get(word, 0)/len(set(true_list))\n",
        "  return(p)\n",
        "\n",
        "\n",
        "def input_processing(input_word):\n",
        "  candidates_list = []\n",
        "  local_probs = {}\n",
        "  if input_word in prob_dict:\n",
        "    return input_word\n",
        "  if input_word in true_dict:\n",
        "    candidates_list.append(true_dict[input_word])\n",
        "  for j in deletes(input_word):\n",
        "      #print(j)\n",
        "    if j in true_dict:\n",
        "        candidates_list.append(true_dict[j])\n",
        "        candidates_list = list(set(candidates_list))\n",
        "  if len(candidates_list) > 1:\n",
        "    for i in set(candidates_list):\n",
        "      local_probs[prob(i)] = i\n",
        "      sorted_probs = sorted(list(local_probs.keys()))\n",
        "      return local_probs[sorted_probs[0]]\n",
        "  else:\n",
        "    if len(candidates_list) > 0:  \n",
        "     return candidates_list[0]\n",
        "    else:\n",
        "      return input_word + \"\"\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['te', 'tex', 'tet', 'txt', 'tt', 'xt', 'ext', 'вице-пмьер', 'виц-премьер', 'вице-прмьер', 'вице-прьер', 'вце-премьер', 'ве-премьер', 'вице-премь', 'вице-ремьер']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-52m4nVhiyZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f326e06f-ab60-47a3-f04e-a4f26585773f"
      },
      "source": [
        "print(input_processing('мсль'))\n",
        "print(input_processing('опофеозом'))\n",
        "print(input_processing('сонце'))\n",
        "print(input_processing('седня'))\n",
        "print(input_processing('оочень'))"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "лишь\n",
            "апофеозом\n",
            "сцены\n",
            "сегодня\n",
            "очень\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L5vQK1Lu-A-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "dfc5c256-e64c-4413-bd2a-5db97fbeab15"
      },
      "source": [
        "\n",
        "# Для оценки используем будем использовать три метрики:\n",
        "# 1) процент правильных слов;\n",
        "# 2) процент исправленных ошибок\n",
        "# 3) процент ошибочно исправленных правильных слов\n",
        "# %%time\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "total_mistaken = 0\n",
        "mistaken_fixed = 0\n",
        "\n",
        "total_correct = 0\n",
        "correct_broken = 0\n",
        "\n",
        "cashed = {}\n",
        "for i in range(len(true)):\n",
        "    word_pairs = align_words(true[i], bad[i])\n",
        "    for pair in word_pairs:\n",
        "        predicted = cashed.get(pair[1], input_processing(pair[1]))\n",
        "        cashed[pair[0]] = predicted\n",
        "        if predicted == pair[0]:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        \n",
        "        if pair[0] == pair[1]:\n",
        "            total_correct += 1\n",
        "            if pair[0] !=  predicted:\n",
        "                correct_broken += 1\n",
        "        else:\n",
        "            total_mistaken += 1\n",
        "            if pair[0] == predicted:\n",
        "                mistaken_fixed += 1\n",
        "        \n",
        "    if not i % 100:\n",
        "      print(i)"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eWNUaKUBJPH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "16115479-c7d9-47d3-c4d8-dfbc90ca879c"
      },
      "source": [
        "print(correct/total)\n",
        "print(mistaken_fixed/total_mistaken)\n",
        "print(correct_broken/total_correct)"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6464535464535465\n",
            "0.283960092095165\n",
            "0.299299414264385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zioakNuUEjA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#По сравнению с теми, что были на лекции, и с реализацией SymspellPy, результат, конечно, не очень."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VWajKsMFT96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#С триграммной моделью"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmsBIpzqgw2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ngrammer(tokens, n=2):\n",
        "    ngrams = []\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams.append(' '.join(tokens[i:i+n]))\n",
        "    return ngrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gvLEGa-f-Py",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []\n",
        "for text in open('corpus_5000.txt').read().splitlines():\n",
        "    sents = sent_tokenize(text)\n",
        "    norm_sents = [normalize(sent) for sent in sents]\n",
        "    corpus += norm_sents\n",
        "\n",
        "corpus_news = [['<start>', '<start>'] + sent + ['<end>'] for sent in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG9-WrWaf_9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigrams = Counter()\n",
        "bigrams = Counter()\n",
        "trigrams = Counter()\n",
        "\n",
        "for sentence in corpus_news:\n",
        "    unigrams.update(sentence)\n",
        "    bigrams.update(ngrammer(sentence))\n",
        "    trigrams.update(ngrammer(sentence, n=3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E2v7i2NfjEU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fb402da9-1435-4fd0-b05b-d3ff3a0e587c"
      },
      "source": [
        "\n",
        "true = open('correct_sents.txt', encoding='utf8').read().splitlines()\n",
        "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
        "corpus_sents = open('corpus_5000.txt', encoding='utf8').read().lower().split()\n",
        "\n",
        "\n",
        "\n",
        "def deletes(word):\n",
        "    letters    = 'йцукенгшщзхъфывапролджэячсмитьбюё'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes1    = [L + R[1:]               for L, R in splits if R]\n",
        "    splits2     = [(word[:i], word[i:])    for i in range(len(deletes1))]\n",
        "    deletes2    = [L + R[2:]               for L, R in splits2 if R]\n",
        "    deletes = deletes1 + deletes2\n",
        "    return set(deletes)\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "dict_list = [re.sub('(^\\W+|\\W+$)', '', token) for token in corpus_sents if (set(token)-punct)]\n",
        "#print(true_list[1])\n",
        "candidats_dict = {}\n",
        "prob_dict = {}\n",
        "bigrams_dict = {}\n",
        "trigrams_dict = {}\n",
        "pairs_dict = {}\n",
        "three_dict = {}\n",
        "for i in range(len(dict_list)):\n",
        "  if i == 0:\n",
        "    bigrams_dict[\"<start> \" + dict_list[i]] = bigrams_dict.get(\"<start> \" + dict_list[i], 0) + 1\n",
        "    trigrams_dict[\"<start> <start> \" + dict_list[i]] =  trigrams_dict.get(\"<start> <start> \" + dict_list[i], 0) + 1\n",
        "  elif i == 1:\n",
        "    trigrams_dict[\"<start> \"+ dict_list[i-1] + \" \" + dict_list[i]] =  trigrams_dict.get(\"<start> \"+ dict_list[i-1] + \" \" + dict_list[i], 0) + 1\n",
        "    bigrams_dict[dict_list[i-1] + \" \" + dict_list[i]] = bigrams_dict.get(dict_list[i-1] + \" \" + dict_list[i], 0) + 1\n",
        "  elif i == len(dict_list):\n",
        "    bigrams_dict[dict_list[i] + \" <end>\"] = bigrams_dict.get(dict_list[i] + \" <end>\", 0) + 1\n",
        "    trigrams_dict[dict_list[i-1]+ \" \" + dict_list[i] + \" <end>\"] =  \\\n",
        "      trigrams_dict.get(dict_list[i-1]+ \" \" + dict_list[i] + \" <end>\", 0) + 1\n",
        "  else:\n",
        "    bigrams_dict[dict_list[i-1] + \" \" + dict_list[i]] = bigrams_dict.get(dict_list[i-1] + \" \" + dict_list[i], 0) + 1\n",
        "    trigrams_dict[dict_list[i-2]+ \" \" + dict_list[i-1] + \" \" + dict_list[i]] =  \\\n",
        "      trigrams_dict.get(dict_list[i-2]+ \" \" + dict_list[i-1] + \" \" + dict_list[i], 0) + 1\n",
        "\n",
        "\n",
        "  prob_dict[dict_list[i]] = prob_dict.get(dict_list[i], 0) + 1\n",
        "  for j in deletes(dict_list[i]):\n",
        "    candidats_dict[j] =  dict_list[i]\n",
        "print(list(true_dict.keys())[:15])\n",
        "\n",
        "def prob(word):\n",
        "  p2=0\n",
        "  p3=0\n",
        "  p1 = prob_dict.get(word, 0)/len(set(true_list))\n",
        "  for i in bigrams_dict.keys():\n",
        "    #print(i.split())\n",
        "    if len(i.split())>1  and word == i.split()[1]:\n",
        "      p2 += bigrams_dict[i]/p1\n",
        "  for i in trigrams_dict.keys():\n",
        "    local3 = i.split()\n",
        "    if len(i.split()) > 2  and word == i.split()[1]:\n",
        "      p3 += trigrams_dict[i]/p1\n",
        "  p = p1 + p2 + p3\n",
        "  return(p)\n",
        "\n",
        "\n",
        "def input_processing(input_word):\n",
        "  candidates_list = []\n",
        "  local_probs = {}\n",
        "  if input_word in prob_dict:\n",
        "    return input_word\n",
        "  if input_word in candidats_dict:\n",
        "    candidates_list.append(true_dict[input_word])\n",
        "  for j in deletes(input_word):\n",
        "      #print(j)\n",
        "    if j in candidats_dict:\n",
        "        candidates_list.append(candidats_dict[j])\n",
        "        candidates_list = list(set(candidates_list))\n",
        "  if len(candidates_list) > 1:\n",
        "    for i in set(candidates_list):\n",
        "      local_probs[prob(i)] = i\n",
        "      sorted_probs = sorted(list(local_probs.keys()))\n",
        "      return local_probs[sorted_probs[0]]\n",
        "  else:\n",
        "    if len(candidates_list) > 0:  \n",
        "     return candidates_list[0]\n",
        "    else:\n",
        "      return input_word"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['te', 'tex', 'tet', 'txt', 'tt', 'xt', 'ext', 'вице-пмьер', 'виц-премьер', 'вице-прмьер', 'вице-прьер', 'вце-премьер', 'ве-премьер', 'вице-премь', 'вице-ремьер']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJVWEWep0ZS_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ae64deba-805e-41bb-98af-7f84ba967ca9"
      },
      "source": [
        "print(input_processing('мсль'))\n",
        "print(input_processing('опофеозом'))\n",
        "print(input_processing('сонце'))\n",
        "print(input_processing('седня'))\n",
        "print(input_processing('оочень'))\n"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "лишь\n",
            "апофеозом\n",
            "сцены\n",
            "сегодня\n",
            "очень\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT_KGr001v05",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8294dd53-7780-4175-c0b5-bf6879421b45"
      },
      "source": [
        "# Для оценки используем будем использовать три метрики:\n",
        "# 1) процент правильных слов;\n",
        "# 2) процент исправленных ошибок\n",
        "# 3) процент ошибочно исправленных правильных слов\n",
        "# %%time\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "total_mistaken = 0\n",
        "mistaken_fixed = 0\n",
        "\n",
        "total_correct = 0\n",
        "correct_broken = 0\n",
        "\n",
        "cashed = {}\n",
        "for i in range(len(true)):\n",
        "    word_pairs = align_words(true[i], bad[i])\n",
        "    for pair in word_pairs:\n",
        "        predicted = cashed.get(pair[1], input_processing(pair[1]))\n",
        "        cashed[pair[0]] = predicted\n",
        "        if predicted == pair[0]:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        \n",
        "        if pair[0] == pair[1]:\n",
        "            total_correct += 1\n",
        "            if pair[0] !=  predicted:\n",
        "                correct_broken += 1\n",
        "        else:\n",
        "            total_mistaken += 1\n",
        "            if pair[0] == predicted:\n",
        "                mistaken_fixed += 1\n",
        "        \n",
        "    if not i % 100:\n",
        "      print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCXcwqhC5sV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(correct/total)\n",
        "print(mistaken_fixed/total_mistaken)\n",
        "print(correct_broken/total_correct)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xT6RQ3wfji1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCdm736NYS4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc2ctQU9ZYVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORDS = Counter()\n",
        "for sent in corpus:\n",
        "    WORDS.update(sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_6B6T1VYZmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_closest_match_vec(text, X, vec, topn=20):\n",
        "    # превращаем слово в вектор такой же размерности\n",
        "    v = vec.transform([text])\n",
        "    \n",
        "    # вся эффективноть берется из того, что мы сразу считаем близость \n",
        "    # 1 вектора ко всей матрице (словам в словаре)\n",
        "    # считать по отдельности циклом было бы дольше\n",
        "    # вместо одного вектора может даже целая матрица\n",
        "    # тогда считаться в итоге будет ещё быстрее\n",
        "    \n",
        "    similarities = cosine_distances(v, X)[0] #distance - чем больше, тем хуже, а similarity наоборот\n",
        "    topn = similarities.argsort()[:topn] \n",
        "    \n",
        "    return [(id2word[top], similarities[top]) for top in topn]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFczdl5sZBUI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "b74be3e3-bb20-46cd-89d3-7158de52f6f0"
      },
      "source": [
        "%time\n",
        "get_closest_hybrid_match('сонце', X, vec)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 8.58 µs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-199-dccc69b31d1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_closest_hybrid_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'сонце'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua1sRaK6Y2j-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_closest_hybrid_match(text, X, vec, topn=5, metric=textdistance.damerau_levenshtein):\n",
        "    # ваш код здесь\n",
        "    candidates = get_closest_match_vec(text, X, vec, topn*4)\n",
        "    sims = Counter()\n",
        "    lookup = [cand[0] for cand in candidates]\n",
        "    closest = get_closest_match_with_metric(text, lookup,topn, metric=metric)\n",
        "\n",
        "    \n",
        "    return closest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBF4XmfcLLOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "corpus = []\n",
        "for text in open('corpus_5000.txt').read().splitlines():\n",
        "    sents = sent_tokenize(text)\n",
        "    norm_sents = [normalize(sent) for sent in sents]\n",
        "    corpus += norm_sents\n",
        "\n",
        "corpus_news = [['<start>', '<start>'] + sent + ['<end>'] for sent in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NhvgKLILcAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigrams = Counter()\n",
        "bigrams = Counter()\n",
        "trigrams = Counter()\n",
        "\n",
        "for sentence in corpus_news:\n",
        "    unigrams.update(sentence)\n",
        "    bigrams.update(ngrammer(sentence))\n",
        "    trigrams.update(ngrammer(sentence, n=3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAcI3wTINeyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def ngrammer(tokens, n=2):\n",
        "    ngrams = []\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams.append(' '.join(tokens[i:i+n]))\n",
        "    return ngrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2Q-pqZBOezv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigrams = Counter()\n",
        "bigrams = Counter()\n",
        "trigrams = Counter()\n",
        "\n",
        "for sentence in corpus_news:\n",
        "    unigrams.update(sentence)\n",
        "    bigrams.update(ngrammer(sentence))\n",
        "    trigrams.update(ngrammer(sentence, n=3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vov-N6ntOlFe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "34952e8a-d7b3-4630-e835-44ed4fc9e927"
      },
      "source": [
        "#print(ngrams[:10])\n",
        "print(unigrams.most_common(10))\n",
        "print(bigrams.most_common(10))\n",
        "print(trigrams.most_common(10))"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('<start>', 118304), ('<end>', 59152), ('в', 41930), ('и', 20346), ('на', 17455), ('что', 11631), ('с', 9616), ('по', 8778), ('не', 7696), ('из', 4369)]\n",
            "[('<start> <start>', 59152), ('<start> в', 6892), ('об этом', 3074), ('<start> по', 2985), ('<start> об', 2984), ('<start> на', 1312), ('этом сообщает', 1280), ('<start> он', 1254), ('<start> ранее', 946), ('а также', 927)]\n",
            "[('<start> <start> в', 6892), ('<start> <start> по', 2985), ('<start> <start> об', 2984), ('<start> об этом', 2953), ('<start> <start> на', 1312), ('об этом сообщает', 1278), ('<start> <start> он', 1254), ('<start> <start> ранее', 946), ('<start> <start> как', 770), ('<start> по словам', 769)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg9YoV1wO6eE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "8efeec47-ddf8-4c1e-8694-2e7868871491"
      },
      "source": [
        "mistakes = []\n",
        "total_mistaken = 0\n",
        "mistaken_fixed = 0\n",
        "\n",
        "total_correct = 0\n",
        "correct_broken = 0\n",
        "\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "\n",
        "for i in range(len(true)):\n",
        "    word_pairs = align_words(true[i], bad[i])\n",
        "    \n",
        "    word_pairs = [('<start>', '<start>')] + word_pairs\n",
        "    pred_sent = []\n",
        "    for j in range(1, len(word_pairs)):\n",
        "        \n",
        "        pred = None\n",
        "        predicted = get_closest_hybrid_match(word_pairs[j][1], X, vec)\n",
        "        \n",
        "        \n",
        "        prev_word = word_pairs[j-1][1]\n",
        "        \n",
        "        \n",
        "        if prev_word not in unigrams:\n",
        "            pred = predicted[0][0]\n",
        "            \n",
        "        \n",
        "        else:\n",
        "            \n",
        "            lm_predicted = []\n",
        "            for word, m in predicted:\n",
        "                bigram = ' '.join([prev_word, word])\n",
        "                # домножаем полученную метрику для слова на вероятность биграма\n",
        "                # биграм - предыдущее слово + текущее слово кандидат\n",
        "                lm_predicted.append((word, (m)*(1+(bigrams[bigram]/unigrams[prev_word]))))\n",
        "            if lm_predicted:\n",
        "                \n",
        "                pred = sorted(lm_predicted, key=lambda x: -x[1])[0][0]\n",
        "            \n",
        "        \n",
        "        if pred is None:\n",
        "            pred = word_pairs[j][1]\n",
        "        \n",
        "\n",
        "        \n",
        "        if pred == word_pairs[j][0]:\n",
        "            correct += 1\n",
        "        else:\n",
        "            mistakes.append((word_pairs[j][0], word_pairs[j][1], pred))\n",
        "        total += 1\n",
        "            \n",
        "        if word_pairs[j][0] == word_pairs[j][1]:\n",
        "            total_correct += 1\n",
        "            if word_pairs[j][0] !=  pred:\n",
        "                correct_broken += 1\n",
        "        else:\n",
        "            total_mistaken += 1\n",
        "            if word_pairs[j][0] == pred:\n",
        "                mistaken_fixed += 1\n",
        "    \n",
        "    if not i % 50:\n",
        "        print(i)\n",
        "        print(correct/total)"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-198-d2697afc59b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_closest_hybrid_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "384EFuHHYn_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}